{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bfba26f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building Multimodal AI Applications with Langchain & OpenAI API\"\n",
    "description: \"In this post, we will explore how we can create a simple bot to ask Questions from YouTube Videos.\"\n",
    "date: \"2024-01-07\"\n",
    "author: \n",
    "  - name: Chirag Sharma\n",
    "categories: [OpenAI API, Langchain, Whisper]\n",
    "toc: true\n",
    "code-fold: false\n",
    "image: posts\\Building_Multimodal_AI_Applications_with_LangChain_&_the_OpenAI_API\\Images\\image.png\n",
    "execute: \n",
    "  message: false\n",
    "  warning: false\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e513836",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n",
    "\n",
    "In this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.\n",
    "\n",
    "- Understanding the building blocks of working with Multimodal AI projects\n",
    "- Working with some of the fundamental concepts of LangChain  \n",
    "- How to use the Whisper API to transcribe audio to text \n",
    "- How to combine both LangChain and Whisper API to create ask questions of any YouTube video \n",
    "\n",
    "## Before you begin\n",
    "\n",
    "You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up.\n",
    "\n",
    "## Task 0: Setup\n",
    "\n",
    "The project requires several packages that need to be installed into Workspace.\n",
    "\n",
    "- `langchain` is a framework for developing generative AI applications.\n",
    "- `yt_dlp` lets you download YouTube videos.\n",
    "- `tiktoken` converts text into tokens.\n",
    "- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Run the following code to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fe258e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "# Install langchain\n",
    "!pip install langchain==0.0.228 yt_dlp==2023.7.6 tiktoken==0.5.1 docarray==0.38.0 chromadb==0.4.19 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d13d274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "## saving the OpenAI API key in an environment file\n",
    "OPENAI_API_KEY = \"PASTE_YOUR_OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf0c8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Importing libraries\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "## Loading the Secrets from the .env file\n",
    "print(load_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bc3c7",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "## Task 1: Import The Required Libraries \n",
    "\n",
    "For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. \n",
    "\n",
    "Import the following packages.\n",
    "\n",
    "- Import `os` \n",
    "- Import `openai`\n",
    "- Import `yt_dlp` with the alias `youtube_dl`\n",
    "- From the `yt_dlp` package, import `DowloadError`\n",
    "- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90211f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n",
    "\n",
    "# Import the os package \n",
    "import os\n",
    "# Import Glob package\n",
    "import glob\n",
    "# Import the openai package \n",
    "import openai\n",
    "# Import the yt_dlp package as youtube_dl\n",
    "import yt_dlp as youtube_dl\n",
    "# Import DownloadError from yt_dlp \n",
    "from yt_dlp import DownloadError\n",
    "# Import DocArray\n",
    "import docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5e0ca",
   "metadata": {},
   "source": [
    "We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe7d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ddfde5",
   "metadata": {},
   "source": [
    "## Task 2: Download the YouTube Video\n",
    "\n",
    "After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n",
    "\n",
    "We'll download a DataCamp tutorial about machine learning in Python.\n",
    "\n",
    "We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n",
    "\n",
    "The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n",
    "\n",
    "Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. \n",
    "\n",
    "Create the following: \n",
    "- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n",
    "- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n",
    "- Use the `ydl_config` that is provided to you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1085ce10",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out UTF-8 (No VT), error UTF-8 (No VT), screen UTF-8 (No VT)\n",
      "[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip) API\n",
      "[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set()}\n",
      "[debug] Python 3.8.8 (CPython AMD64 64bit) - Windows-10-10.0.19041-SP0 (OpenSSL 1.1.1k  25 Mar 2021)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Video from the url : https://www.youtube.com/watch?v=aqzxYofJ_ck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] exe versions: ffmpeg 4.2.2, ffprobe 4.2.2\n",
      "[debug] Optional libraries: Cryptodome-3.16.0, brotli-None, certifi-2023.05.07, mutagen-1.47.0, sqlite3-2.6.0, websockets-10.4\n",
      "[debug] Proxy map: {}\n",
      "[debug] Loaded 1855 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n",
      "[youtube] aqzxYofJ_ck: Downloading webpage\n",
      "[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n",
      "[youtube] aqzxYofJ_ck: Downloading android player API JSON\n",
      "[youtube] aqzxYofJ_ck: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr4---sn-gwpa-qxae7.googlevideo.com/videoplayback?expire=1702465583&ei=zzt5ZeuFHfnI9fwPxsga&ip=2405%3A204%3A1084%3Af6f9%3A880b%3A3c9d%3A3034%3Ad424&id=o-AKpHyYLtPNcrqGHmxYmtXWFheNhhFtT8bQK3iZamIu0W&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zw&mm=31%2C26&mn=sn-gwpa-qxae7%2Csn-cvh7kn6l&ms=au%2Conr&mv=m&mvi=4&pl=41&initcwndbps=586250&spc=UWF9f58kWoASEo1Oapo-5m9N4QmkVKo&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=10932652&dur=752.701&lmt=1654008313150389&mt=1702443665&fvip=4&keepalive=yes&fexp=24007246&c=ANDROID&txp=5318224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAOdDElAhZsHVY7UdSsCZoXj2FJcrT6pjkAciKt_hI5FQAiEAuBAwx5I3YXa3DTcS5eNgD69LPOlAXOTzdo1hw99iUT0%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AAO5W4owRAIgI1tmLq-v0-RNT53cftUz7SpfBdarWL-JjglCzkWRbacCICXoEBE__X8AXZxj91DHwYiyK5tsPCsupXOwN6Mk1o8G\"\n",
      "[debug] File locking is not supported. Proceeding without locking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\n",
      "[download] 100% of   10.43MiB in 00:00:07 at 1.45MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffprobe -show_streams \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\" -vn -acodec libmp3lame \"-b:a\" 192.0k -movflags \"+faststart\" \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# An example YouTube tutorial video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n",
    "# Directory to store the downloaded video\n",
    "output_dir = \"files/audio/\"\n",
    "\n",
    "# Config for youtube-dl\n",
    "ydl_config = {\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\",\n",
    "        }\n",
    "    ],\n",
    "    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "# Check if the output directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# Print a message indicating which video is being downloaded\n",
    "print(f\"Downloading Video from the url : {youtube_url}\")\n",
    "# Attempt to download the video using the specified configuration\n",
    "# If a DownloadError occurs, attempt to download the video again\n",
    "try:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "except DownloadError:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371052b",
   "metadata": {},
   "source": [
    "To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. \n",
    "\n",
    "Create the following: \n",
    "- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n",
    "- Select the first first file in the list and assign it to `audio_filename`\n",
    "- To verify the filename, print `audio_filename` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce2a68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"
     ]
    }
   ],
   "source": [
    "# Find the audio file in the output directory\n",
    "\n",
    "# Find all the audio files in the output directory\n",
    "audio_file = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n",
    "\n",
    "# Select the first audio file in the list\n",
    "audio_filename = audio_file[0]\n",
    "\n",
    "# Print the name of the selected audio file\n",
    "print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb830ce",
   "metadata": {},
   "source": [
    "## Task 3: Transcribe the Video using Whisper\n",
    "\n",
    "In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n",
    "\n",
    "Using these variables we will:\n",
    "- create a list to store the transcripts\n",
    "- Read the Audio File \n",
    "- Send the file to the Whisper Model using the OpenAI package \n",
    "\n",
    "To complete this step, create the following: \n",
    "- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n",
    "- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n",
    "- A variable named `model` that is assigned the value  `\"whisper-1\"`\n",
    "- An empty list called `transcripts`\n",
    "- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n",
    "- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n",
    "- Append the `response[\"text\"]`to the `transcripts` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205aefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Audio to Text.....\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define function parameters\n",
    "audio_file = audio_filename\n",
    "output_file = \"files/transcripts/transcript.txt\"\n",
    "model = \"whisper-1\"\n",
    "\n",
    "# Set the API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Transcribe the audio file to text using OpenAI API\n",
    "print(\"Converting Audio to Text.....\")\n",
    "with open(audio_file, \"rb\") as audio:\n",
    "    response = openai.Audio.transcribe(model, audio)\n",
    "\n",
    "# Extract the transcript from the response\n",
    "transcript = response['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ba006",
   "metadata": {},
   "source": [
    "To save the transcripts to text files we will use the below provided code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c4bf7a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating, because it's going to make your model appear to perform better than it actually does. So it's giving you a false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as PD. That's the standard alias for it. And then we actually, we purchased one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train test split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan applications. And we're going to use PD.read CSV because it is in a CSV file. And the file is called loan underscore data dot CSV. Let me just check and see if I got that correct. Loan underscore data dot CSV. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, this is a categorical column that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column. And then I'm just going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications, with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test. And then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default, the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And because we set the random state, this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice, and so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So if we're going to look at features train 2, and you see, even though it's random, we have exactly the same results. It's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful.\n"
     ]
    }
   ],
   "source": [
    "# If an output file is specified, save the transcript to a .txt file\n",
    "if output_file is not None:\n",
    "    # Create the directory for the output file if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    # Write the transcript to the output file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(transcript)\n",
    "\n",
    "# Print the transcript to the console to verify it worked \n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd9e58",
   "metadata": {},
   "source": [
    "## Task 4: Create a TextLoader using LangChain \n",
    "\n",
    "In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document.\n",
    "\n",
    "To complete this step, do the following: \n",
    "- Import `TextLoader` from `langchain.document_loaders`\n",
    "- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n",
    "- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacc2a91",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chirag Sharma\\anaconda3\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.11) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the TextLoader class from the langchain.document_loaders module\n",
    "from langchain.document_loaders import TextLoader \n",
    "\n",
    "# Create a new instance of the TextLoader class, specifying the directory containing the text files\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "# Load the documents from the specified directory using the TextLoader instance\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6242f321",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating, because it's going to make your model appear to perform better than it actually does. So it's giving you a false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as PD. That's the standard alias for it. And then we actually, we purchased one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train test split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan applications. And we're going to use PD.read CSV because it is in a CSV file. And the file is called loan underscore data dot CSV. Let me just check and see if I got that correct. Loan underscore data dot CSV. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, this is a categorical column that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column. And then I'm just going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications, with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test. And then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default, the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And because we set the random state, this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice, and so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So if we're going to look at features train 2, and you see, even though it's random, we have exactly the same results. It's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful.\", metadata={'source': './files/transcripts/transcript.txt'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first element of docs to verify it has been loaded \n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb59ed",
   "metadata": {},
   "source": [
    "## Task 4: Creating an In-Memory Vector Store \n",
    "\n",
    "Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n",
    "\n",
    "For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n",
    "\n",
    "We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. \n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `tiktoken` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3656d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tiktoken package\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a22ea",
   "metadata": {},
   "source": [
    "## Task 5: Create the Document Search \n",
    "\n",
    "We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n",
    "\n",
    "- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n",
    "- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n",
    "- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n",
    "- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n",
    "- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8eaa933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RetrievalQA class from the langchain.chains module\n",
    "from langchain.chains import RetrievalQA\n",
    "# Import the ChatOpenAI class from the langchain.chat_models module\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "# Import the OpenAIEmbeddings class from the langchain.embeddings module\n",
    "from langchain.embeddings import OpenAIEmbeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb034d",
   "metadata": {},
   "source": [
    "Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. \n",
    "\n",
    "To complete this step: \n",
    "- Create a variable called `db`\n",
    "- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n",
    "- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c057c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194c2d5",
   "metadata": {},
   "source": [
    "We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM.\n",
    "\n",
    "Create the following: \n",
    "- A variable called `retriever` that is assigned `db.as_retriever()`\n",
    "- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75b0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DocArrayInMemorySearch instance to a retriever\n",
    "retriever = db.as_retriever()\n",
    "# Create a new ChatOpenAI instance with a temperature of 0.0\n",
    "llm = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e896067",
   "metadata": {},
   "source": [
    "Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n",
    "- The `llm` we want to use\n",
    "- The `chain_type` which is how the model retrieves the data\n",
    "- The `retriever` that we have created \n",
    "- An option called `verbose` that allows use to see the seperate steps of the chain \n",
    "\n",
    "Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n",
    "\n",
    "Use the following settings inside this method: \n",
    "- `llm=llm`\n",
    "- `chain_type=\"stuff\"`\n",
    "- `retriever=retriever`\n",
    "- `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f41e70c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new RetrievalQA instance with the specified parameters\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "llm=llm,\n",
    "chain_type=\"stuff\",\n",
    "retriever=retriever,\n",
    "verbose=True,\n",
    ")\n",
    "    # The ChatOpenAI instance to use for generating responses\n",
    "    # The type of chain to use for the QA system\n",
    "    # The retriever to use for retrieving relevant documents\n",
    "    # Whether to print verbose output during retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd2169",
   "metadata": {},
   "source": [
    "## Task 5: Create the Queries \n",
    "\n",
    "Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. \n",
    "\n",
    "To create the questions to ask the model complete the following steps: \n",
    "- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n",
    "- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n",
    "- Show the `resposnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "354885b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This tutorial is about a data pre-processing technique for machine learning called splitting your data. It explains why it is important to split your data set into a training set and a testing set, and when in the machine learning workflow this should be done. The tutorial also provides code examples using pandas and scikit-learn to demonstrate how to split the data set.\n"
     ]
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is this tutorial about?\"\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "# Print the response to the console\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9943c",
   "metadata": {},
   "source": [
    "We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d056179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The training set is a subset of the data that is used to train a machine learning model. It is used to teach the model the patterns and relationships in the data so that it can make accurate predictions or classifications. The test set, on the other hand, is a subset of the data that is used to evaluate the performance of the trained model. It is used to assess how well the model generalizes to new, unseen data. The test set is independent of the training set and should not be used during the training process to avoid biasing the model's performance.\n"
     ]
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"What is the difference between a training set and test set?\"\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "# Print the response to the console\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e02a2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This lesson is suitable for anyone who is interested in data pre-processing techniques for machine learning, specifically the technique of splitting data into training and testing sets. It is especially relevant for individuals who are new to machine learning and want to understand the importance of splitting data and how to implement it in their workflow.\n"
     ]
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"Who should watch this lesson?\"\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "# Print the response to the console\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceca6c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know the answer to that question as it is subjective and can vary depending on personal opinions and criteria used to determine greatness.\n"
     ]
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query =\"Who is the greatest football team on earth?\"\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "# Print the response to the console\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc588362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-SWfi4b6dZN3Y7f3B7Kokq756 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know the answer to that question.\n"
     ]
    }
   ],
   "source": [
    "# Set the query to be used for the QA system\n",
    "query = \"How long is the circumference of the earth?\"\n",
    "# Run the query through the RetrievalQA instance and store the response\n",
    "response = qa_stuff.run(query)\n",
    "# Print the response to the console\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f250a6",
   "metadata": {},
   "source": [
    "## All done, congrats! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
