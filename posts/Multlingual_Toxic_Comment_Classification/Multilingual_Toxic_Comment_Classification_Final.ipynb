{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multilingual Toxic Comment Classification using Tensorflow and TPUs\"\n",
    "description: \"In this post, we'll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\"\n",
    "date: \"2023-01-09\"\n",
    "categories: [Web Scrapping]\n",
    "toc: true\n",
    "code-fold: false\n",
    "execute: \n",
    "  message: false\n",
    "  warning: false\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as `anything rude, disrespectful, or otherwise likely` to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet.     \n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "As part of the competition, competitors were provided several files, specifically:\n",
    "\n",
    "`jigsaw-toxic-comment-train.csv` - data from the [Jigsaw toxic comment classification competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). The dataset is made up of English comments from Wikipedia’s talk page edits.\n",
    "\n",
    "`jigsaw-unintended-bias-train.csv` - data from the [Jigsaw Unintended Bias in Toxicity Classification competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). This is an expanded version of the Civil Comments dataset with a range of additional labels.\n",
    "\n",
    "`sample_submission.csv` - a sample submission file.\n",
    "\n",
    "`test.csv` - comments from Wikipedia talk pages in different non-English languages.\n",
    "\n",
    "`validation.csv` - comments from Wikipedia talk pages in different non-English languages\n",
    "\n",
    "## Evaluation Metric\n",
    "Submissions were evaluated based on Area Under the ROC Curve between the predicted probability and the observed target.\n",
    "\n",
    "## Strategies to Tackle\n",
    "### `Monolingual Approach`\n",
    "Monolingual models are the type of language models which are trained on a single language.\n",
    "\n",
    "They are focused on understanding and generating text in a specific language. \n",
    "\n",
    "For example, a monolingual model trained on English language will be proficient in understanding and generating English text. \n",
    "\n",
    "These models are typically for tasks such as text classification, sentiment analysis and more within a specific language.\n",
    "\n",
    "Monolingual models can be beneficial to utilize when we have a specific language in our training, testing datasets and in the upcoming unseen data. \n",
    "\n",
    "### `Multilingual Approach`\n",
    "Multilingual models are, on the other hand, are trained on multiple different languages. \n",
    "\n",
    "They are designed to handle and process text in multiple languages, allowing them to perform across different languages. \n",
    "\n",
    "Multilingual models have the advantage of of being able to provide language-agnostic solutions, as they can handle a wide-range of languages. \n",
    "\n",
    "They can be used for zero-shot and few-shot learning, where the model can perform a task in a language it has not been seen specifically trained on by leveraging its knowledge of other languages.\n",
    "\n",
    "### `Which models to use for our problem?`\n",
    "As per the dataset given in the competition, we have only english data in our training dataset and very samples are given in the validation dataset containing languages `Spanish`, `Turkish` and `Italian` only and the Testing dataset contains languages `Turkish`, `Spanish`, `Italian`, `Russian`, `French` and `Portugese`.\n",
    "\n",
    "Since in our validation and test dataset contains non-english languages it would be better approach to build multilingual models rather than monolingual models. \n",
    "\n",
    "Now, if we had only one language (as stated above) building monolingual models would be a better choice. \n",
    "\n",
    "Let's discuss Multilingual models approach a bit more:\n",
    "\n",
    "How are multilingual models are trained?\n",
    "\n",
    "Multilingual models are pre-trained on a mix of different languages and they don't distinguish between the languages. \n",
    "\n",
    "The English BERT was pre-trained on English Wikipedia and BookCorpus dataset, while multilingual models like mBERT was pre-trained on 102 different languages from largest Wikipedia dataset and XLM-Roberta was pre-trained on CommonCrawl dataset from 100 different languages respectively.\n",
    "\n",
    "### `Cross Lingual Transfer`\n",
    "Cross-lingual transfer refers to transfer learning using data and models available for one language for which ample such resources are available (e.g., English) to solve tasks in another, commonly more low-resource, language.\n",
    "\n",
    "In our case, we are trying to create an application that can automatically detect whether a sentence or phrase is toxic or not. \n",
    "\n",
    "Models like XLM-Roberta provides us the ability to fine tune it on English dataset and predict to identify comments in any different language.\n",
    "\n",
    "XLM-R is able to take it's specific knowledge learnt in one language and apply it to a different langauge (languages), even though it never seen the language during fine-tuning. \n",
    "\n",
    "This concept is of transfer learning applied from one language to another language is referred to as `Cross-Lingual Transfer (AKA Zero-Shot Learning)` .\n",
    "\n",
    "Another reason to use Pre-Trained multi-lingual models for a task like this (as in our case) is that is the `Lack of languages by resources` i.e., different languages have different amounts of training data available to build models like BERT and its variants. \n",
    "\n",
    "Some languages like English, Chinese, Russian, Indonesian, Vietnamese etc. are the languages that have high resource languages, whereas languages like sundanese, assamese etc. are low resource languages. \n",
    "\n",
    "Training our own BERT like model on these low resources could be very expensive in terms of data collection and performance-wise  , therefore, We should leverage these multi-lingual models.\n",
    "\n",
    "### `What experiments did I perform ?`\n",
    "At the Overall level, I performed 9 experiments with the following ideas keeping in mind.\n",
    "\n",
    "Perform Pre-processing techniques like removal of stopwords, removing URLs, Contraction to Expansion of words, removing multiple characters from words and removal of punctuations.\n",
    "\n",
    "From model stand point we experimented with 2 models mBERT & XLM-Roberta.\n",
    "\n",
    "From Training dataset perspective we used 2 types of datasets: one case where we used the provided training datasets where we tried to balance the dataset by the target and the other case where we used the translated training dataset of languages provided in the test dataset along with the english language with class balancing.\n",
    "\n",
    "We always trained on validation dataset as well to further boost the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a model with the following ideas:\n",
    "> Training on original training & validation datasets, class balancing (undersampling), fine-tuning the model for 2 epochs on training as well as validation dataset, and will not perform any preprocessing dataset. We will be leveraging the TPUs offered by Kaggle.\n",
    "\n",
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T07:22:33.408790Z",
     "iopub.status.busy": "2023-05-08T07:22:33.408138Z",
     "iopub.status.idle": "2023-05-08T07:23:35.810995Z",
     "shell.execute_reply": "2023-05-08T07:23:35.809930Z",
     "shell.execute_reply.started": "2023-05-08T07:22:33.408758Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 KB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.5.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0508 07:23:28.285075581      14 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0508 07:23:28.285109420      14 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0508 07:23:28.285113692      14 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0508 07:23:28.285116753      14 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0508 07:23:28.285119508      14 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0508 07:23:28.285122651      14 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0508 07:23:28.285125779      14 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0508 07:23:28.285136675      14 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0508 07:23:28.285139712      14 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0508 07:23:28.285142515      14 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0508 07:23:28.285145320      14 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0508 07:23:28.285148256      14 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0508 07:23:28.285151131      14 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0508 07:23:28.285153928      14 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0508 07:23:28.285344519      14 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\n",
      "D0508 07:23:28.297613061      14 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0508 07:23:28.297635890      14 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0508 07:23:28.297978050      14 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0508 07:23:28.297989261      14 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0508 07:23:28.297993453      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0508 07:23:28.297997280      14 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0508 07:23:28.298001081      14 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0508 07:23:28.298004402      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0508 07:23:28.298011355      14 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0508 07:23:28.298027677      14 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0508 07:23:28.298054234      14 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0508 07:23:28.298068190      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0508 07:23:28.298072029      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0508 07:23:28.298076040      14 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0508 07:23:28.298082495      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0508 07:23:28.298086497      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0508 07:23:28.298090079      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0508 07:23:28.298093976      14 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0508 07:23:28.300184472      14 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0508 07:23:28.318782434     242 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0508 07:23:28.326237166     242 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-05-08T07:23:28.326222349+00:00\"}\n",
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers --quiet\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:23:35.813255Z",
     "iopub.status.busy": "2023-05-08T07:23:35.812749Z",
     "iopub.status.idle": "2023-05-08T07:23:35.818440Z",
     "shell.execute_reply": "2023-05-08T07:23:35.817610Z",
     "shell.execute_reply.started": "2023-05-08T07:23:35.813228Z"
    }
   },
   "outputs": [],
   "source": [
    "main_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "toxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\n",
    "unintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\n",
    "validation_csv_path = main_data_dir_path + \"validation.csv\"\n",
    "test_csv_path = main_data_dir_path + \"test.csv\"\n",
    "submission_csv_path = main_data_dir_path + \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Configurations\n",
    "\n",
    "Intializing the TPU configurations and other constants like `number of epochs, batch_size (16 * number of cores offered on TPUS), MAX_LEN (length of the sentence), we use xlm-roberta-large model, number of samples (for undersampling) = 150k, Learning_rate = 1e-5 etc.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T07:23:38.288496Z",
     "iopub.status.busy": "2023-05-08T07:23:38.288089Z",
     "iopub.status.idle": "2023-05-08T07:23:47.572679Z",
     "shell.execute_reply": "2023-05-08T07:23:47.571505Z",
     "shell.execute_reply.started": "2023-05-08T07:23:38.288467Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  \n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "#################### TPU Configurations ####################\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "MODEL = 'xlm-roberta-large'\n",
    "NUM_SAMPLES = 150000\n",
    "RANDOM_STATE = 42\n",
    "LEARNING_RATE = 1e-5 ######################### MAIN CHANGE ############################\n",
    "WEIGHT_DECAY = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & Balancing the data by Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:23:47.574865Z",
     "iopub.status.busy": "2023-05-08T07:23:47.574503Z",
     "iopub.status.idle": "2023-05-08T07:24:17.487288Z",
     "shell.execute_reply": "2023-05-08T07:24:17.485969Z",
     "shell.execute_reply.started": "2023-05-08T07:23:47.574836Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading csv files \n",
    "train1 = pd.read_csv(toxic_comment_train_csv_path)\n",
    "train2 = pd.read_csv(unintended_bias_train_csv_path)\n",
    "valid = pd.read_csv(validation_csv_path)\n",
    "test = pd.read_csv(test_csv_path)\n",
    "sub = pd.read_csv(submission_csv_path)\n",
    "\n",
    "## Converting floating points to integers ##\n",
    "train2.toxic = train2['toxic'].round().astype(int)\n",
    "\n",
    "##### BALANCING THE DATA ##### \n",
    "# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n",
    "# & sampling 150k observations randomly from non-toxic observation population.\n",
    "\n",
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n",
    "])\n",
    "\n",
    "## Dropping missing observations with respect to comment-text column \n",
    "train = train.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:24:19.248281Z",
     "iopub.status.busy": "2023-05-08T07:24:19.247827Z",
     "iopub.status.idle": "2023-05-08T07:24:19.255160Z",
     "shell.execute_reply": "2023-05-08T07:24:19.254028Z",
     "shell.execute_reply.started": "2023-05-08T07:24:19.248249Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Function takes a list of texts, tokenizer (object)\n",
    "    initialized from HuggingFace library, max_len (defines\n",
    "    of how long the sentence lengths should be).\n",
    "    \"\"\"       \n",
    "    tokens = tokenizer(texts, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='np')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding comment_text\n",
    "\n",
    "We first initialize the tokenizer from Hugging Face transformer library and encoding our training, validation and test dataset comment_texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T07:24:20.808760Z",
     "iopub.status.busy": "2023-05-08T07:24:20.807724Z",
     "iopub.status.idle": "2023-05-08T07:25:17.700276Z",
     "shell.execute_reply": "2023-05-08T07:25:17.698933Z",
     "shell.execute_reply.started": "2023-05-08T07:24:20.808723Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 616/616 [00:00<00:00, 138kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 41.6MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 57.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "## Intializing the tokenizer ##\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_inputs = encode(train['comment_text'].values.tolist(), \n",
    "                      tokenizer, max_len=MAX_LEN)\n",
    "validation_inputs = encode(valid['comment_text'].values.tolist(),\n",
    "                          tokenizer, max_len=MAX_LEN)\n",
    "test_inputs = encode(test['content'].values.tolist(),\n",
    "                    tokenizer, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data using tf.data.Data API\n",
    "\n",
    "Writing a function to create a tuple of inputs and outputs, where inputs have a dictionary datatype.\n",
    "\n",
    "We'll be leveraging tf.data.Data API to pass our inputs and outputs as tuple, i.e., (inputs, outputs), inputs are `{\"input_ids\": input_ids, \"attention_mask\": attention_mask} and outputs labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:25:20.108817Z",
     "iopub.status.busy": "2023-05-08T07:25:20.108341Z",
     "iopub.status.idle": "2023-05-08T07:25:20.114569Z",
     "shell.execute_reply": "2023-05-08T07:25:20.113537Z",
     "shell.execute_reply.started": "2023-05-08T07:25:20.108784Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_fn(input_ids, attention_mask, labels=None):\n",
    "    if labels is not None:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n",
    "    else:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:25:21.211292Z",
     "iopub.status.busy": "2023-05-08T07:25:21.210320Z",
     "iopub.status.idle": "2023-05-08T07:25:22.088206Z",
     "shell.execute_reply": "2023-05-08T07:25:22.087132Z",
     "shell.execute_reply.started": "2023-05-08T07:25:21.211257Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n",
    "                                                    train_inputs[\"attention_mask\"],\n",
    "                                                   train['toxic']))\n",
    "train_dataset = train_dataset.map(map_fn)\n",
    "train_dataset = train_dataset.repeat().shuffle(buffer_size=2048,seed=RANDOM_STATE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n",
    "                                                         validation_inputs['attention_mask'],\n",
    "                                                        valid['toxic']))\n",
    "validation_dataset = validation_dataset.map(map_fn)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n",
    "                                                  test_inputs['attention_mask']))\n",
    "test_dataset = test_dataset.map(map_fn)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:25:24.088859Z",
     "iopub.status.busy": "2023-05-08T07:25:24.087623Z",
     "iopub.status.idle": "2023-05-08T07:25:24.098396Z",
     "shell.execute_reply": "2023-05-08T07:25:24.097425Z",
     "shell.execute_reply.started": "2023-05-08T07:25:24.088820Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer_layer, max_len):\n",
    "    \"\"\"\n",
    "    Creating the model input layers, output layers,\n",
    "    model definition and compilation.\n",
    "        \n",
    "    Returns: model object after compiling. \n",
    "    \"\"\"\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                      dtype=tf.int32, \n",
    "                                      name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                       dtype=tf.int32, \n",
    "                                       name=\"attention_mask\")\n",
    "    output = transformer_layer.roberta(input_ids, \n",
    "                                 attention_mask=attention_mask)[1]\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(output)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n",
    "                             outputs=y)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n",
    "                                         weight_decay=WEIGHT_DECAY)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "    \n",
    "    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model on TPUs\n",
    "\n",
    "It is important to initialize & compile the model inside the `with strategy.scope()`.\n",
    "\n",
    "One thing I want to point out that for some reason I was getting different results even though I was setting the seed before initializing the model, but the results are always consistent even though the results differ very little every time we run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:25:26.128412Z",
     "iopub.status.busy": "2023-05-08T07:25:26.127843Z",
     "iopub.status.idle": "2023-05-08T07:27:20.756174Z",
     "shell.execute_reply": "2023-05-08T07:27:20.754801Z",
     "shell.execute_reply.started": "2023-05-08T07:25:26.128375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 2.24G/2.24G [00:46<00:00, 48.0MB/s]\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " roberta (TFXLMRobertaMainLayer  TFBaseModelOutputWi  559890432  ['input_ids[0][0]',              \n",
      " )                              thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 192,                                               \n",
      "                                 1024),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1049600     ['roberta[0][1]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 560,941,057\n",
      "Trainable params: 560,941,057\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    model = build_model(transformer_layer,\n",
    "                        max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Only English data for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T07:27:22.298147Z",
     "iopub.status.busy": "2023-05-08T07:27:22.297769Z",
     "iopub.status.idle": "2023-05-08T08:17:33.238761Z",
     "shell.execute_reply": "2023-05-08T08:17:33.237230Z",
     "shell.execute_reply.started": "2023-05-08T07:27:22.298116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2023-05-08 07:28:46.072958: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n",
      "2023-05-08 07:28:48.332211: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - ETA: 0s - loss: 0.0551 - auc: 0.9970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 07:53:43.374173: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n",
      "2023-05-08 07:53:43.934692: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - 1609s 378ms/step - loss: 0.0551 - auc: 0.9970 - val_loss: 0.4696 - val_auc: 0.8942\n",
      "Epoch 2/2\n",
      "3795/3795 [==============================] - 1399s 369ms/step - loss: 0.0450 - auc: 0.9979 - val_loss: 0.3125 - val_auc: 0.9083\n"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(train_dataset,\n",
    "                         steps_per_epoch=train_steps_per_epoch,\n",
    "                         validation_data=validation_dataset,\n",
    "                         epochs=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Validation data for 2 epochs further to fine-tune on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:17:39.839805Z",
     "iopub.status.busy": "2023-05-08T08:17:39.839062Z",
     "iopub.status.idle": "2023-05-08T08:19:50.565792Z",
     "shell.execute_reply": "2023-05-08T08:19:50.564151Z",
     "shell.execute_reply.started": "2023-05-08T08:17:39.839770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "62/62 [==============================] - 23s 367ms/step - loss: 0.2286 - auc: 0.9315\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - 107s 365ms/step - loss: 0.1472 - auc: 0.9726\n"
     ]
    }
   ],
   "source": [
    "validation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "validation_history = model.fit(validation_dataset.repeat(),\n",
    "                              steps_per_epoch=validation_steps_per_epoch,\n",
    "                              epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Public LeaderBoard score on kaggle (test dataset): 0.936 and Private LeaderBoard score : 0.9346\n",
    "\n",
    "## Results\n",
    "| Experiment | Public Test LeaderBoard Score | Private Test LeaderBoard Score |\n",
    "| --- | --- | --- |\n",
    "| 1 (mBERT + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8850 | 0.8869 |\n",
    "|2 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.9259 | 0.9264 |\n",
    "|3 (mBERT + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8259 | 0.8239 |\n",
    "|4 (xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8755 | 0.8754 |\n",
    "|5 (mBERT + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.9195 | 0.9212 |\n",
    "|6 ((xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.9329 | 0.9212 |\n",
    "|7 (mBERT + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.8696 | 0.9212 |\n",
    "|8 ((xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.8861 | 0.8866 |\n",
    "|9 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 1e-5) | 0.936 | 0.9346 |\n",
    "\n",
    "## Predicting on Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T08:19:55.118899Z",
     "iopub.status.busy": "2023-05-08T08:19:55.118459Z",
     "iopub.status.idle": "2023-05-08T08:21:15.297689Z",
     "shell.execute_reply": "2023-05-08T08:21:15.296342Z",
     "shell.execute_reply.started": "2023-05-08T08:19:55.118866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 08:20:04.172862: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-05-08 08:20:04.668074: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 80s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:21:53.414762Z",
     "iopub.status.busy": "2023-05-08T08:21:53.414255Z",
     "iopub.status.idle": "2023-05-08T08:21:53.429904Z",
     "shell.execute_reply": "2023-05-08T08:21:53.428621Z",
     "shell.execute_reply.started": "2023-05-08T08:21:53.414725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.266148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.000308\n",
       "1   1  0.000241\n",
       "2   2  0.266148\n",
       "3   3  0.000063\n",
       "4   4  0.000078"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T08:21:57.599260Z",
     "iopub.status.busy": "2023-05-08T08:21:57.598292Z",
     "iopub.status.idle": "2023-05-08T08:23:19.245054Z",
     "shell.execute_reply": "2023-05-08T08:23:19.243642Z",
     "shell.execute_reply.started": "2023-05-08T08:21:57.599220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 829). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"../working/Multilingual_toxic_comment_classifier\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:23:25.622630Z",
     "iopub.status.busy": "2023-05-08T08:23:25.622191Z",
     "iopub.status.idle": "2023-05-08T08:24:35.677429Z",
     "shell.execute_reply": "2023-05-08T08:24:35.676025Z",
     "shell.execute_reply.started": "2023-05-08T08:23:25.622576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 37s 37s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.0799874e-04],\n",
       "       [2.2472920e-04],\n",
       "       [2.6646560e-01],\n",
       "       [5.7183450e-05],\n",
       "       [7.6287179e-05],\n",
       "       [3.1223629e-02]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_save_path = \"../working/Multilingual_toxic_comment_classifier\"\n",
    "loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "y = loaded_model.predict(test_dataset.take(1))\n",
    "y[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the function to prepare for the new text, we encode the text using the `tokenizer with the sentence length=192`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:24:42.590889Z",
     "iopub.status.busy": "2023-05-08T08:24:42.589669Z",
     "iopub.status.idle": "2023-05-08T08:24:43.396330Z",
     "shell.execute_reply": "2023-05-08T08:24:43.394865Z",
     "shell.execute_reply.started": "2023-05-08T08:24:42.590852Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "text = \"politicians are like cancer for this country\"\n",
    "def prep_data(text, tokenizer, max_len=192):\n",
    "    tokens = tokenizer(text, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='tf')\n",
    "    \n",
    "    return {\"input_ids\": tokens['input_ids'],\n",
    "            \"attention_mask\": tokens['attention_mask']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the probability of toxic and non-toxic on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T08:24:46.139977Z",
     "iopub.status.busy": "2023-05-08T08:24:46.138911Z",
     "iopub.status.idle": "2023-05-08T08:24:55.313920Z",
     "shell.execute_reply": "2023-05-08T08:24:55.312636Z",
     "shell.execute_reply.started": "2023-05-08T08:24:46.139939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prob_of_toxic_comment': 0.26497197,\n",
       " 'prob_of_non_toxic_comment': 0.7350280284881592}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\n",
    "prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n",
    "prob_of_toxic_comment, prob_of_non_toxic_comment\n",
    "probs = {\"prob_of_toxic_comment\": prob_of_toxic_comment,\n",
    " \"prob_of_non_toxic_comment\": prob_of_non_toxic_comment}\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with the Gradio App before final pushing the model to HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-08T09:10:07.958578Z",
     "iopub.status.busy": "2023-05-08T09:10:07.958160Z",
     "iopub.status.idle": "2023-05-08T09:10:59.295870Z",
     "shell.execute_reply": "2023-05-08T09:10:59.294407Z",
     "shell.execute_reply.started": "2023-05-08T09:10:07.958550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRunning on local URL:  http://127.0.0.1:7865\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running on public URL: https://af370decb4339b429e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://af370decb4339b429e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 1s 513ms/step\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gradio --quiet\n",
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "examples_list = [\"politicians are like cancer for this country\", \n",
    "                 \"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было,\",\n",
    "                \"Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хох\",\n",
    "                ]\n",
    "\n",
    "def prep_data(text, tokenizer, max_len=192):\n",
    "    tokens = tokenizer(text, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='tf')\n",
    "    \n",
    "    return {\"input_ids\": tokens['input_ids'],\n",
    "            \"attention_mask\": tokens['attention_mask']}\n",
    "\n",
    "def predict(text):\n",
    "    prob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\n",
    "    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n",
    "    prob_of_toxic_comment, prob_of_non_toxic_comment\n",
    "    probs = {\"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n",
    "             \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment)}\n",
    "    return probs\n",
    "\n",
    "interface = gr.Interface(fn=predict, inputs=gr.components.Textbox(lines=4,label='Comment'),\n",
    "                        outputs=[gr.Label(label='Probabilities')], examples=examples_list,\n",
    "                        title='Multi-Lingual Toxic Comment Classification.',\n",
    "                        description='XLM-Roberta Large model')\n",
    "interface.launch(debug=False, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woow!!!\n",
    "\n",
    "Our application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting,\n",
    "we can upload our Gradio app Interface to [HuggingFace Spaces](https://huggingface.co/spaces). \n",
    "\n",
    "Now download all the files and folders from kaggle output manually & this kaggle kernel locally\n",
    "\n",
    "## Turning our Multi-Lingual Toxic Comment Classification Gradio Demo into a deployable app\n",
    "\n",
    "We'll deploy the demo application on HuggingFace Spaces.\n",
    "\n",
    "What is HuggingFace Spaces?\n",
    "\n",
    "It is a resource that allows anybody to host and share machine learning application.\n",
    "\n",
    "### Deployed Gradio App Structure\n",
    "To upload our gradio app, we'll want to put everything together into a singe directory.\n",
    "\n",
    "For example, our demo might live at the path `demos/melanoma_skin_cancer_files` with the following structure:\n",
    "```    \n",
    "demos/\n",
    "    └── multilingual_toxic_comment_files/\n",
    "        ├── Multilingual_toxic_comment_classifier/\n",
    "        │   ├── variable/\n",
    "        │   │   ├── variables.data-00000-of-00001\n",
    "        │   │   └── variables.index\n",
    "        │   ├── fingerprint.pb\n",
    "        │   ├── keras_metadata.pb\n",
    "        │   └── saved_model.pb \n",
    "        ├── app.py\n",
    "        ├── examples/\n",
    "        │   └── dataset\n",
    "        └── requirements.txt\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Multilingual_toxic_comment_classifier` is our saved fine-tuned model (binary files associated).\n",
    "- `app.py` contains our Gradio app, our data preprocessing function and our predict function.\n",
    "    **Note**: `app.py` is the default filename used for HuggingFace Spaces, if we deploy our apps there.\n",
    "- `examples` contains sample dataframe which contains toxic & non-toxic comments from russian, spanish, english, italian, turkish, portugese and last french languages to showcase the demo of our Gradio application.\n",
    "- `requirements.txt` file contains the dependencies/packages to run our application such as tensorflow, gradio, transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a demo folder to store our Multilingual Toxic Comment Classifier App files\n",
    "To begin, we'll create an empty directory `demos/` that will contain all our necessary files for the application.\n",
    "\n",
    "We can achive this using Python's `pathlib.Path(\"path_of_dir\")` to establish directory path and then `pathlib.Path(\"path_of_dir\").mkdir()` to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### ROOT_DIR : I Have put the files in my E: drive\n",
    "## Importing Packages \n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n",
    "\n",
    "## Create Melanoma skin cancer demo path\n",
    "multilingual_toxic_comment_demo_path = Path(f\"{ROOT_DIR}/demos/multilingual_toxic_comment_files\")\n",
    "\n",
    "## Removing files that might already exist and creating a new directory.\n",
    "if multilingual_toxic_comment_demo_path.exists():\n",
    "    shutil.rmtree(multilingual_toxic_comment_demo_path)\n",
    "    multilingual_toxic_comment_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n",
    "                                exist_ok=True) # Create even if they already exists? \n",
    "else:\n",
    "    ## If the path doesn't exist, create one \n",
    "    multilingual_toxic_comment_demo_path.mkdir(parents=True,\n",
    "                                exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a folder of example images to use with our Melanoma skin cancer demo\n",
    "Now we'll create an empty directory called `examples` and store a sample dataset containing comments from the Russian, Turkish, English, Spanish, Portugese, French, Italian languages. I have collected these comments from online and created a CSV file for them. \n",
    "\n",
    "To do so we'll:\n",
    "\n",
    "1. Create an empty directory `examples/` within the `demos/multilingual_toxic_comment_files` directory.\n",
    "2. Collect some comment samples from online in these languages and create a CSV file out of them containing both toxic as well as non-toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "## Create examples directory\n",
    "multilingual_toxic_comment_examples_path = multilingual_toxic_comment_demo_path / \"examples\"\n",
    "multilingual_toxic_comment_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sample_comments = Path(f\"sample_comments.csv\")\n",
    "\n",
    "comments = {\"comment_text\": [\"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\",\n",
    "                 \"Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\",\n",
    "                 \"В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.\",\n",
    "                 \"Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.\",\n",
    "                 \"This is such an urgent design problem; kudos to you for taking it on. Very impressive!\",\n",
    "                 \"haha you guys are a bunch of losers.\", \"ur a sh*tty comment.\",\n",
    "                 \"Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!\",\n",
    "                 'SIAMO ALLA FOLLIA', \n",
    "                 '20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg ',\n",
    "                 \"Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) \"]}\n",
    "\n",
    "pd.DataFrame(comments, \n",
    "             columns=['comment_text']).to_csv(multilingual_toxic_comment_examples_path / sample_comments,\n",
    "                                              index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we verify our example images are present, let's list the contents of our `demo/melanoma_skin_cancer/examples/` directory with `os.listdir()` and then format the filepaths into a list of lists (to make it compatible with the Gradio's `gradio.Interface()`, example parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.'],\n",
       " ['Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?'],\n",
       " ['В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.'],\n",
       " ['Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.'],\n",
       " ['This is such an urgent design problem; kudos to you for taking it on. Very impressive!'],\n",
       " ['haha you guys are a bunch of losers.'],\n",
       " ['ur a sh*tty comment.'],\n",
       " ['Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!'],\n",
       " ['SIAMO ALLA FOLLIA'],\n",
       " ['20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg '],\n",
       " ['Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) ']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_list = [[example] for example in pd.read_csv(multilingual_toxic_comment_examples_path / sample_comments)['comment_text'].tolist()]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving our trained XLM-Roberta model binary files into our multilingual_toxic_comment_files demo directory.\n",
    "We have saved our fine-tuned model in `outout/working/multilingual_toxic_comment_files/` directory and we'll move our model files to `demos/multilingual_toxic_comment_files/` directory as specified above.  \n",
    "\n",
    "We use Python's `shutil.move()` method and passing in `src`(the source path of the target file) and `dst` (the destination folder path of the target file to be moved into) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to move the E:\\MultiLingual-Toxic-Comment-Classification\\output\\working\\Multilingual_toxic_comment_classifier\\ to E:\\MultiLingual-Toxic-Comment-Classification\\demos\\multilingual_toxic_comment_files\n",
      "Model move completed\n"
     ]
    }
   ],
   "source": [
    "## Importing Libraries\n",
    "import shutil\n",
    "\n",
    "## Create a source path for our target model\n",
    "multilingual_toxic_comment_model_dir_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Multilingual_toxic_comment_classifier\\\\\"\n",
    "\n",
    "## Create a destination path for our target model\n",
    "multilingual_toxic_comment_model_dir_destination = multilingual_toxic_comment_demo_path\n",
    "\n",
    "## Try to move the file\n",
    "try:\n",
    "    print(f\"Attempting to move the {multilingual_toxic_comment_model_dir_path} to {multilingual_toxic_comment_model_dir_destination}\")\n",
    "    \n",
    "    ## Move the model\n",
    "    shutil.move(src=multilingual_toxic_comment_model_dir_path,\n",
    "           dst=multilingual_toxic_comment_model_dir_destination)\n",
    "    \n",
    "    print(\"Model move completed\")\n",
    "## If the model has already been moved, check if it exists\n",
    "except:\n",
    "    print(f\"No model found at {multilingual_toxic_comment_model_dir_path}, perhaps it's already moved.\")\n",
    "    print(f\"Model already exists at {multilingual_toxic_comment_model_dir_destination}: {multilingual_toxic_comment_model_dir_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning our Gradio App into a Python Script (`app.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\MultiLingual-Toxic-Comment-Classification\\\\notebooks'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now if we look into which directory we are currently, we'll find that using the following code\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move into the demos directory where we will write some helper utilities.\n",
    "\n",
    "In `cd ../demos/`: `..` means we are moving outside of the notebooks directory.\n",
    "`demos/`: means we moving inside the demos directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\MultiLingual-Toxic-Comment-Classification\\demos\n"
     ]
    }
   ],
   "source": [
    "cd ../demos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multilingual_toxic_comment_files/app.py\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_save_path = \"Multilingual_toxic_comment_classifier/\"\n",
    "### Loading the fine-tuned model ###\n",
    "loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "### Initializing the tokenizer ###\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "examples_list = [\n",
    "    [example]\n",
    "    for example in pd.read_csv(\"examples/sample_comments.csv\")[\"comment_text\"].tolist()\n",
    "]\n",
    "\n",
    "\n",
    "def prep_data(text, tokenizer, max_len=192):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokens[\"input_ids\"],\n",
    "        \"attention_mask\": tokens[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    prob_of_toxic_comment = loaded_model.predict(\n",
    "        prep_data(text=text, tokenizer=tokenizer_, max_len=192)\n",
    "    )[0][0]\n",
    "    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n",
    "    prob_of_toxic_comment, prob_of_non_toxic_comment\n",
    "    probs = {\n",
    "        \"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n",
    "        \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment),\n",
    "    }\n",
    "    return probs\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.components.Textbox(lines=4, label=\"Comment\"),\n",
    "    outputs=[gr.Label(label=\"Probabilities\")],\n",
    "    examples=examples_list,\n",
    "    title=\"Multi-Lingual Toxic Comment Classification.\",\n",
    "    description=\"XLM-Roberta Large model\",\n",
    ")\n",
    "interface.launch(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a requirements.txt file for our Gradio App(`requirements.txt`)\n",
    "This is the last file we need to create for our application.\n",
    "\n",
    "This file contains all the necessary packages for our Gradio application.\n",
    "\n",
    "When we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n",
    "\n",
    "1. `tensorflow==2.12`\n",
    "2. `pandas==1.5.2`\n",
    "3. `gradio==3.1.4`\n",
    "4. `transformers==4.28.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting multilingual_toxic_comment_files/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile multilingual_toxic_comment_files/requirements.txt\n",
    "tensorflow==2.12\n",
    "pandas==1.5.2\n",
    "gradio==3.1.4\n",
    "transformers==4.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying our Application to HuggingFace Spaces\n",
    "To deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n",
    "\n",
    "1. [Uploading via the Hugging Face Web Interface (easiest)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui)\n",
    "2. [Uploading via the command line or terminal](https://huggingface.co/docs/hub/repositories-getting-started#terminal)\n",
    "\n",
    "NOTE: To host any application on HuggingFace, we first need to [sign up for a free HuggingFace Account](https://huggingface.co/join)\n",
    "\n",
    "### Running our Application locally\n",
    "\n",
    "1. Open the terminal or command prompt.\n",
    "2. Changing the `multilingual_toxic_comment_files` directory (cd multilingual_toxic_comment_files).\n",
    "3. Creating an environment `(python3 -m venv env)` or use `(python -m venv env)`.\n",
    "4. Activating the environment `(source env/Scripts/activate)`.\n",
    "5. Installing the `requirements.txt` using `pip install -r requirements.txt`.\n",
    "> If faced any errors, we might need to upgrade `pip` using `pip install --upgrade pip`.  \n",
    "6. Run the app `(python3 app.py).`\n",
    "\n",
    "This should results in a Gradio demo locally at the URL such as : `http://127.0.0.1:7860/`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading to Hugging Face\n",
    "We've verified our Melanoma_skin_cancer detection application is working in our local system.\n",
    "\n",
    "To upload our application to Hugging Face Spaces, we need to do the following.\n",
    "\n",
    "1. [Sign up](https://huggingface.co/welcome) for a Hugging Face account.\n",
    "2. Start a new Hugging Face Space by going to our profile at the top right corner and then select [New Space](https://huggingface.co/new-space).\n",
    "3. Declare the name to the space like `Chirag1994/multilingual_toxic_comment_classification_app`.\n",
    "4. Select a license (I am using MIT license).\n",
    "5. Select Gradio as the Space SDK (software development kit).\n",
    "6. Choose whether your Space is Public or Private (I am keeping it Public).\n",
    "7. Click Create Space.\n",
    "8. Clone the repository locally by running: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` in the terminal or command prompt. In our case mine would be like - `git clone https://huggingface.co/spaces/Chirag1994/multilingual_toxic_comment_classification_app`.\n",
    "9. Copy/Move the contents of the downloaded `multilingual_toxic_comment_classification_app` folder to the cloned repo folder.\n",
    "10. To upload files and track larger files (e.g., files that are greater than 10MB) for them we need to [install Git LFS](https://git-lfs.github.com/) which stands for Git large File Storage.\n",
    "11. Open up the cloned directory using VS code (I'm using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command `git lfs install` to start tracking the file that we want to track. For example - git lfs track `\"Multilingual_toxic_comment_classifier\" directory files`.\n",
    "12. Create a new .gitignore file and the files & folders that we don't want git to track like :\n",
    "    - `__pycache__/`\n",
    "    - `.vscode/`\n",
    "    - `venv/`\n",
    "    - `.gitignore`\n",
    "    - `.gitattributes`\n",
    "13. Add the rest of the files and commit them with:\n",
    "    - `git add .`\n",
    "    - `git commit -m \"commit message that you want\"`\n",
    "14. Push(load) the files to Hugging Face\n",
    "    - `git push`\n",
    "15. It might a couple of minutes to finish and then the app will be up and running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Final Application deployed on HuggingFace Spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"https://chirag1994-multilingual-toxic-comment-classifier.hf.space\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x252799c6790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IPython is a library to help make Python interactive\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed FoodVision Mini Gradio demo\n",
    "IFrame(src=\"https://chirag1994-multilingual-toxic-comment-classifier.hf.space\", width=1000, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
