[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDropout in Neural Networks Explained\n\n\n6 min\n\n\n\nWeb Scrapping\n\n\n\nIn this post, We’ll understand What Dropout is? Why should we use it? Why does it work? Some tips for using it in practice.\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\nMelanoma Skin Cancer Detection using Transfer Learning with PyTorch\n\n\n12 min\n\n\n\nWeb Scrapping\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping using BeautifulSoup & Selenium in Python.\n\n\n5 min\n\n\n\nWeb Scrapping\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html",
    "href": "posts/Dropout_Explained/Dropout_Explained.html",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Why should we use Dropout?\nWhat Is a Dropout?\nWhy does Dropout works?\nTips for Using Dropout.\nReferences\n\n\n\n\nBefore jumping into what is dropout, let’s try to answer the question of why should we use Dropout. Deep Neural Networks which are trained on smaller datasets can overfit the training data. Now, what is Overfitting? Overfitting happens when your model is too powerful to memorize the training data. But who cares about your training data, we want to take the model and put it into production for the use case. We want the model to perform well on the test/unseen dataset. Usually, Convolutional Neural Networks are less prone to Overfitting because of the idea of weight sharing i.e., you use the same filter/kernel and slide over the image, and therefore, you end up having fewer parameters in CNNs. The problem primarily occurs when you have a Deep Fully Connected Neural Network that tends to have a lot of parameters & they are likely to overfit. So, in short, Dropout is a regularization technique that prevents your Deep Neural Networks to overfit.\nDoes that mean you cannot apply to Convolutional Neural Networks? The answer is No, You can apply to CNNs and even to LSTMs and GRUs.\n\n\n\n\n\nLet’s try to Understand the dropout in the context of Fully Connected Neural Networks since they are more prone to Overfit. Look at the below image: \nIn a Nutshell, Dropout is about killing/turning off the nodes in a network at a layer. Suppose you have an MLP like in figure(a), then during the training stage at layer l, you are essentially ignoring the neurons and that is done randomly with a certain probability, p. This probability, p, is the dropout rate that is a HyperParameter that you set before training the Network. Say you choose the dropout rate p=0.5 then it means you will delete each node with a 50% probability during each forward pass of your minibatch. In each forward pass of your minibatch, you will randomly drop the nodes and these nodes will be not going to update during backpropagation. \nIn the above Image, Consider in the first forward pass in the Middle layer dropout may disable the 2nd & 4th nodes but in the second forward pass maybe 1st & 4th nodes will get disabled. In each iteration, during backpropagation, the weights corresponding to these nodes will not be updated.\nHow do we drop nodes practically? Let’s compare the standard network and a dropout network. \nWe can observe the inputs of layer l, y^l, are multiplied with the weights, wi^(l+1) and bias bi^(l+1) is added to get net inputs for one neuron, and later a non-linearity (activation function) is applied on it to generate the value of the neuron in layer (l+1).\n\n - But in Dropout Network, first, a boolean vector r^(l) is generated which will contain the same number of elements as the number of neurons in layer l, then this vector is multiplied by the input vectors of layer l to get the net input. The net input is finally passed into an activation function. But the question is how do you arrive at this boolean vector? Let’s take an example to illustrate this point, in the above image we have 3 neurons in layer l, suppose you set the dropout rate to p=0.5 then one way to create the boolean vector is first to generate the random number vector containing numbers between 0 & 1. How can you do that? You can sample 3 numbers from a Uniform distribution with an interval [0,1]. Assume that you get a vector of random numbers as [0.1, 0.6, 0.8], then you will set the value of the vector to be zero when the random value &lt; dropout rate(0.5 in our case) else you set 1. So, our random vector becomes [0,1,1] which represents that neuron1 is dropped and the rest of the neurons and not. Now, This vector we are calling r^(l) & the rest of the things are the same as usual.\n\nSo far we talked about the Training stage, But how does dropout work in the Inference stage? Since dropout is dropping neurons randomly in every forward pass of the minibatch, the authors of the paper: A Simple Way to Prevent Neural Networks from Overfitting uses a trick to avoid this randomization, and your model should be deterministic during Inference otherwise your predictions and accuracy computed on the test set will not be deterministic. For instance, we are creating a credit scoring model then due to dropping neurons randomly a customer might get different scores on different dates because the neurons will be dropped randomly. That’s why Dropout is applied to training the model only. During Inference, before finalizing the network the weights are first scaled by the chosen dropout rate (p) i.e., Wtest^(l) = pW^(l). Now the network can be used as normal to make predictions on unseen data.\nNote: The rescaling of the weights can be performed at the training time instead, after each weight update at the end of the min-batch. This is sometimes called an “Inverse Dropout” and it does not require any modification of weights during training. Both Keras & PyTorch deep learning Libraries Dropout in this way.\n\n\n\n\n\nOne interpretation is the Co-adaptation theory: It refers to the fact that now the network doesn’t learn to rely on particular connections too heavily. So, if we have a Fully Connected Neural Network with Dropout applied to a particular layer then the network will rely on neuron1 & neuron4 only if that layer has 4 neurons in total while training. Consider a real-life example where your teacher forms groups for projects and you are randomly assigned to a group. So that you don’t rely on your best friend (let’s say). This way your teacher is making you independent and this is called Co-adaptation & you want to prevent your network from it.\n\n\n\n\n\n\nDropout is a general idea and it can be used with any Network either LSTMs, GRUs, MLPs, CNNs.\n\n\nA good dropout rate is between 0.5 to 0.8 in the hidden layers. But we can use different dropout rates in different hidden layers.\n\n\nMake your network complex by increasing its capacity until it overfits and then add dropout to the layers to prevent overfitting.\n\n\nTo find the optimal dropout rate we can use cross-validation to find it. Since, it is not always possible to do cross-validation because your dataset might be very big. Therefore, the dropout rate = 0.5 works very well for most problems.\n\n\n\n\n\n\nhttps://www.youtube.com/watch?v=FKTWKZU3l6k&list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4&index=5\nPart-1, Part-2, Part-3 : By Professor Sebastian Raschka"
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#why-should-we-use-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#why-should-we-use-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Before jumping into what is dropout, let’s try to answer the question of why should we use Dropout. Deep Neural Networks which are trained on smaller datasets can overfit the training data. Now, what is Overfitting? Overfitting happens when your model is too powerful to memorize the training data. But who cares about your training data, we want to take the model and put it into production for the use case. We want the model to perform well on the test/unseen dataset. Usually, Convolutional Neural Networks are less prone to Overfitting because of the idea of weight sharing i.e., you use the same filter/kernel and slide over the image, and therefore, you end up having fewer parameters in CNNs. The problem primarily occurs when you have a Deep Fully Connected Neural Network that tends to have a lot of parameters & they are likely to overfit. So, in short, Dropout is a regularization technique that prevents your Deep Neural Networks to overfit.\nDoes that mean you cannot apply to Convolutional Neural Networks? The answer is No, You can apply to CNNs and even to LSTMs and GRUs."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#what-is-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#what-is-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Let’s try to Understand the dropout in the context of Fully Connected Neural Networks since they are more prone to Overfit. Look at the below image: \nIn a Nutshell, Dropout is about killing/turning off the nodes in a network at a layer. Suppose you have an MLP like in figure(a), then during the training stage at layer l, you are essentially ignoring the neurons and that is done randomly with a certain probability, p. This probability, p, is the dropout rate that is a HyperParameter that you set before training the Network. Say you choose the dropout rate p=0.5 then it means you will delete each node with a 50% probability during each forward pass of your minibatch. In each forward pass of your minibatch, you will randomly drop the nodes and these nodes will be not going to update during backpropagation. \nIn the above Image, Consider in the first forward pass in the Middle layer dropout may disable the 2nd & 4th nodes but in the second forward pass maybe 1st & 4th nodes will get disabled. In each iteration, during backpropagation, the weights corresponding to these nodes will not be updated.\nHow do we drop nodes practically? Let’s compare the standard network and a dropout network. \nWe can observe the inputs of layer l, y^l, are multiplied with the weights, wi^(l+1) and bias bi^(l+1) is added to get net inputs for one neuron, and later a non-linearity (activation function) is applied on it to generate the value of the neuron in layer (l+1).\n\n - But in Dropout Network, first, a boolean vector r^(l) is generated which will contain the same number of elements as the number of neurons in layer l, then this vector is multiplied by the input vectors of layer l to get the net input. The net input is finally passed into an activation function. But the question is how do you arrive at this boolean vector? Let’s take an example to illustrate this point, in the above image we have 3 neurons in layer l, suppose you set the dropout rate to p=0.5 then one way to create the boolean vector is first to generate the random number vector containing numbers between 0 & 1. How can you do that? You can sample 3 numbers from a Uniform distribution with an interval [0,1]. Assume that you get a vector of random numbers as [0.1, 0.6, 0.8], then you will set the value of the vector to be zero when the random value &lt; dropout rate(0.5 in our case) else you set 1. So, our random vector becomes [0,1,1] which represents that neuron1 is dropped and the rest of the neurons and not. Now, This vector we are calling r^(l) & the rest of the things are the same as usual.\n\nSo far we talked about the Training stage, But how does dropout work in the Inference stage? Since dropout is dropping neurons randomly in every forward pass of the minibatch, the authors of the paper: A Simple Way to Prevent Neural Networks from Overfitting uses a trick to avoid this randomization, and your model should be deterministic during Inference otherwise your predictions and accuracy computed on the test set will not be deterministic. For instance, we are creating a credit scoring model then due to dropping neurons randomly a customer might get different scores on different dates because the neurons will be dropped randomly. That’s why Dropout is applied to training the model only. During Inference, before finalizing the network the weights are first scaled by the chosen dropout rate (p) i.e., Wtest^(l) = pW^(l). Now the network can be used as normal to make predictions on unseen data.\nNote: The rescaling of the weights can be performed at the training time instead, after each weight update at the end of the min-batch. This is sometimes called an “Inverse Dropout” and it does not require any modification of weights during training. Both Keras & PyTorch deep learning Libraries Dropout in this way."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#why-does-dropout-works",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#why-does-dropout-works",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "One interpretation is the Co-adaptation theory: It refers to the fact that now the network doesn’t learn to rely on particular connections too heavily. So, if we have a Fully Connected Neural Network with Dropout applied to a particular layer then the network will rely on neuron1 & neuron4 only if that layer has 4 neurons in total while training. Consider a real-life example where your teacher forms groups for projects and you are randomly assigned to a group. So that you don’t rely on your best friend (let’s say). This way your teacher is making you independent and this is called Co-adaptation & you want to prevent your network from it."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#tips-for-using-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#tips-for-using-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Dropout is a general idea and it can be used with any Network either LSTMs, GRUs, MLPs, CNNs.\n\n\nA good dropout rate is between 0.5 to 0.8 in the hidden layers. But we can use different dropout rates in different hidden layers.\n\n\nMake your network complex by increasing its capacity until it overfits and then add dropout to the layers to prevent overfitting.\n\n\nTo find the optimal dropout rate we can use cross-validation to find it. Since, it is not always possible to do cross-validation because your dataset might be very big. Therefore, the dropout rate = 0.5 works very well for most problems."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#references",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#references",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "https://www.youtube.com/watch?v=FKTWKZU3l6k&list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4&index=5\nPart-1, Part-2, Part-3 : By Professor Sebastian Raschka"
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html",
    "href": "posts/Web_Scraping/Flipkart_.html",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#introduction",
    "href": "posts/Web_Scraping/Flipkart_.html#introduction",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#what-is-web-scraping",
    "href": "posts/Web_Scraping/Flipkart_.html#what-is-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "What is Web Scraping? ",
    "text": "What is Web Scraping? \n\nWeb Scraping is the process of automatically downloading the data displayed on the website using some computer program. A web scraping tool can scrape multiple pages from a website & automate the tedious task of manually copying and pasting the data displayed. Web Scraping is important because, irrespective of the industry, the web contains information that can provide actionable insights for businesses to gain an advantage over competitors."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "href": "posts/Web_Scraping/Flipkart_.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:",
    "text": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:\n\nFind the URL that you want to scrape\nInspecting the Page\nFind the data you want to extract\nWrite the code\nRun the code & extract the data\nFinally, Store the data in the required format"
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#packages-used-for-web-scraping",
    "href": "posts/Web_Scraping/Flipkart_.html#packages-used-for-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Packages used for Web Scraping",
    "text": "Packages used for Web Scraping\n\nWe’ll use the following python packages:\n\nPandas: Pandas is a library used for data manipulation and analysis. It is used to store the data in the desired format.\nBeautifulSoup4: BeautifulSoup is the python web scraping library used for parsing HTML documents. It creates parse trees that are helpful in extracting tags from the HTML string.\nSelenium: Selenium is a tool designed to help you run automated tests in web applications. Although it’s not its main purpose, Selenium is also used in Python for web scraping, because it can access JavaScript-rendered content (which regular scraping tools like BeautifulSoup can’t do). We’ll use Selenium to download the HTML content from Flipkart and see in an interactive way what’s happening."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#project-demonstration",
    "href": "posts/Web_Scraping/Flipkart_.html#project-demonstration",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Project Demonstration",
    "text": "Project Demonstration\n\nImporting necessary Libraries\n\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\n\n\nStarting up the WebDriver\n\n# Creating an instance of webdriver for google chrome\ndriver = webdriver.Chrome()\n\n\n# Using webdriver we'll now open the flipkart website in chrome\nurl = 'https://flipkart.com'\n# We;ll use the get method of driver and pass in the URL\ndriver.get(url)\n\n\nNow there a few ways we can conduct a product search :\n\n\nFirst is to automate the browser by finding the input element and then insert a text and hit enter key on the keyboard. The image like below.\n\n\n\n\n\nHowever, this kind of automation is unnecessary and it creates a potential for program failure. The Rule of thumb for automation is to only automate what you absolutely need to when Web Scraping.\n\nLet’s search the input inside the search area and hit enter. You’ll notice that the search term has now embeded into the URL site. Now we can use this pattern to create a function that will build the necessary URL for our driver to retrieve. This will be much more efficient in the long term and less prone to proram failure. The image like below.\n\n\n\n\nLet’s copy this Pattern and create a function that will insert the search term using string formatting.\n\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    # We'are replacing every space with '+' to adhere with the pattern \n    search_item = search_item.replace(\" \",\"+\")\n    return template.format(search_item)\n\n\nNow we have a function that will generate a URL based on the search term we provide.\n\n\n# Checking whether the function is working properly or not\nurl = get_url('mobile phones')\nprint(url)\n\nhttps://www.flipkart.com/search?q=mobile+phones&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on\n\n\n\nThe fuction produces the same result as before.\n\n\n\nExtracting the collection\n\nNow we are going to extract the contents of the webpage from which we want to extract the information from.\nTo do that we need to create a BeautifulSoup object which will parse the HTML content from the page source.\n\n\n# Creating a soup object using driver.page_source to retreive the HTML text and then we'll use the default html parser to parse\n# the HTML.\nsoup = BeautifulSoup(driver.page_source, 'html.parser')\n\n\n\nNow that we have identified that the above card indicated by the box contains all the information what we need for a mobile phone. So let’s find out all the tags for these boxes/cards which contains information we want to extract.\nWe’ll be extracting Model , stars, number of ratings, number of reviews, RAM, Storage capacity, Exapandable option, display, camera information, battery, processor , warranty and Price information."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#inspecting-the-tags",
    "href": "posts/Web_Scraping/Flipkart_.html#inspecting-the-tags",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Inspecting the tags",
    "text": "Inspecting the tags\n\n\nWe can fetch the a tag & specificallyclass = _1fQZEK to get all the cards/boxes and then we can easily take out information of out these boxes for any mobile phone.\n\n\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nlen(results)\n\n24\n\n\n\nPrototyping for a single record\n\n# picking the 1st card from the complete list of cards\nitem = results[0]\n\n\n# Extracting the model of the phone from the 1st card\nmodel = item.find('div',{'class':\"_4rR01T\"}).text\nmodel\n\n'REDMI 9i (Nature Green, 64 GB)'\n\n\n\n# Extracting Stars from 1st card\nstar = item.find('div',{'class':\"_3LWZlK\"}).text\nstar\n\n'4.3'\n\n\n\n# Extracting Number of Ratings from 1st card\nnum_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\nnum_ratings\n\n'4,06,452 Ratings'\n\n\n\n# Extracting Number of Reviews from 1st card\nreviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\nreviews\n\n'23,336 Reviews'\n\n\n\n# Extracting RAM from the 1st card\nram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\nram\n\n'4 GB RAM '\n\n\n\n# Extracting Storage/ROM from 1st card\nstorage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\nstorage\n\n'64 GB ROM'\n\n\n\n# Extracting whether there is an option of expanding the storage or not\nexpandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\nexpandable\n\n'Expandable Upto 512 GB'\n\n\n\n# Extracting the display option from the 1st card\ndisplay = item.find_all('li')[1].text.strip()\ndisplay\n\n'16.59 cm (6.53 inch) HD+ Display'\n\n\n\n# Extracting camera options from the 1st card\ncamera = item.find_all('li')[2].text.strip()\ncamera\n\n'13MP Rear Camera | 5MP Front Camera'\n\n\n\n# Extracting the battery option from the 1st card\nbattery = item.find_all('li')[3].text\nbattery\n\n'5000 mAh Lithium Polymer Battery'\n\n\n\n# Extracting the processir option from the 1st card\nprocessor = item.find_all('li')[4].text.strip()\nprocessor\n\n'MediaTek Helio G25 Processor'\n\n\n\n# Extracting Warranty from the 1st card\nwarranty = item.find_all('li')[-1].text.strip()\nwarranty\n\n'Brand Warranty of 1 Year Available for Mobile and 6 Months for Accessories'\n\n\n\n# Extracting price of the model from the 1st card\nprice = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n\n\n\nGeneralizing the Pattern\n\nNow let create a function that will extract all the information at once from a single page.\n\n\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\n\n# Now putting all the information from all the cards/phone models and putting them into a list\nrecords_list = []\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nfor item in results:\n    records_list.append(extract_phone_model_info(item))\n\n\nViewing how does our dataframe look like for the 1st page.\n\n\npd.DataFrame(records_list,columns=['model',\"star\",\"num_ratings\"\n   ,\"reviews\",'ram',\"storage\",\"expandable\",\"display\",\"camera\",\"battery\",\"processor\",\"warranty\",\"price\"])\n\n\n\n\n\n\n\n\nmodel\nstar\nnum_ratings\nreviews\nram\nstorage\nexpandable\ndisplay\ncamera\nbattery\nprocessor\nwarranty\nprice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n4\nrealme C21 (Cross Blue, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n5\nREDMI 9 Power (Mighty Black, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n6\nPOCO M3 (Cool Blue, 64 GB)\n4.3\n14,630 Ratings\n930 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹10,499\n\n\n7\nPOCO M2 Reloaded (Mostly Blue, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n8\nrealme C11 2021 (Cool Grey, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n9\nrealme C11 2021 (Cool Blue, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n10\nREDMI 9 Power (Fiery Red, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n11\nREDMI 9i (Midnight Black, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n12\nInfinix Smart 5A (Quetzal Cyan, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n13\nInfinix Smart 5A (Midnight Black, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n14\nInfinix Smart 5A (Ocean Wave, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n15\nPOCO M3 (Power Black, 64 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹11,499\n\n\n16\nPOCO M3 (Power Black, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n17\nPOCO M2 Reloaded (Greyish Black, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n18\nREDMI Note 9 (Aqua Green, 64 GB)\n4.3\n92,325 Ratings\n6,906 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 13MP Front Camera\n5020 mAh Battery\nMediaTek Helio G85 Processor\n1 Year Manufacturer Warranty for Device and 6 ...\n₹11,999\n\n\n19\nrealme Narzo 30 5G (Racing Silver, 128 GB)\n4.4\n27,618 Ratings\n2,243 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 1 TB\n16.51 cm (6.5 inch) Full HD+ Display\n48MP + 2MP + 2MP | 16MP Front Camera\n5000 mAh Battery\nMediaTek Dimensity 700 (MT6833) Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹15,999\n\n\n20\nREDMI 9 Power (Blazing Blue, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n21\nPOCO M3 (Yellow, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n22\nrealme C20 (Cool Grey, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n23\nrealme C20 (Cool Blue, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n\n\n\n\n\n\n\nNavigating to next page\n\nWriting a custom function that will help us getting information from multiple pages\n\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\n\n\nPutting it all together\n\nNow combining all thhe things that we have done so far.\n\n\n# Importing necessary Libraries\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\ndef main(search_item):\n    '''\n    This function will create a dataframe for all the details that we are fetching from all the multiple pages\n    '''\n    driver = webdriver.Chrome()\n    records = []\n    url = get_url(search_item)\n    for page in range(1,464):\n        driver.get(url.format(page))\n        soup = BeautifulSoup(driver.page_source,'html.parser')\n        results = soup.find_all('a',{'class':\"_1fQZEK\"})\n        for item in results:\n            records.append(extract_phone_model_info(item))\n    driver.close()\n    # Saving the data into a csv file\n    with open('Flipkart_results.csv','w',newline='',encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Model','Stars','Num_of_Ratings','Reviews','Ram','Storage','Expandable',\n                        'Display','Camera','Battery','Processor','Warranty','Price'])\n        writer.writerows(records)\n\n\n\nExtracting Informtion of all the Mobile phones present on multiple pages\n\n%%time\nmain('mobile phones')\n\nWall time: 40min 54s\n\n\n\n\nViewing the data\n\nscraped_df = pd.read_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Jupyter Notebook\\\\Jovian Web Scraping\\\\Amazon Products Web Scrapper\\\\Flipkart_results.csv')\nscraped_df.head()\n\n\n\n\n\n\n\n\nModel\nStars\nNum_of_Ratings\nReviews\nRam\nStorage\nExpandable\nDisplay\nCamera\nBattery\nProcessor\nWarranty\nPrice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nSAMSUNG Galaxy S21 Ultra (Phantom Silver, 256 GB)\n4.5\n537 Ratings\n101 Reviews\n12 GB RAM\n256 GB RO\nNaN\n17.27 cm (6.8 inch) Quad HD+ Display\n108MP + 12MP + 10MP + 10MP | 40MP Front Camera\n5000 mAh Lithium-ion Battery\nExynos 2100 Processor\n1 Year Manufacturer Warranty for Handset and 6...\n₹1,05,999\n\n\n4\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating Python modules in Scripts for training and prediction",
    "text": "Creating Python modules in Scripts for training and prediction\nFirst we will write/create python modules like for augmentations, config, training & validation loops, prediction_to_generate_on_test_dataset etc.\nFor training augmentations we’ll be using like flipping the image, creating random patches in the image, randomly rotating 90 degrees, Adjusting the brightness and contrast, adding noise in the images, Shifting and sheering the image and finally normalizing the statistics of the image (since we will be using transfer learning therefore we need to prepare the images in the same way they were trained on - depending on the specific model we want to use).\n\n%%writefile Scripts/augmentations.py\nfrom Scripts.config import Config\nimport albumentations as A\n\ntraining_augmentations = A.Compose(\n    [\n        A.CoarseDropout(p=0.6),\n        A.RandomRotate90(p=0.6),\n        A.Flip(p=0.4),\n        A.OneOf(\n            [\n                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3),\n                A.HueSaturationValue(\n                    hue_shift_limit=20, sat_shift_limit=60, val_shift_limit=50\n                ),\n            ],\n            p=0.7,\n        ),\n        A.OneOf([A.GaussianBlur(), A.GaussNoise()], p=0.65),\n        A.ShiftScaleRotate(\n            shift_limit=0.0625, scale_limit=0.35, rotate_limit=45, p=0.5\n        ),\n        A.OneOf(\n            [\n                A.OpticalDistortion(p=0.3),\n                A.GridDistortion(p=0.1),\n                A.PiecewiseAffine(p=0.3),\n            ],\n            p=0.7,\n        ),\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        ),\n    ]\n)\n\nvalidation_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\ntesting_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\n\nWriting Scripts/augmentations.py\n\n\nCreating a config module which will contain model configurations like number of epochs to run, size of an image, weight decay (for regularization) etc., it also contains the path of the files and folders of the data.\n\n%%writefile Scripts/config.py\nimport torch\n\nclass Config:\n    EPOCHS = 5\n    IMG_SIZE = 512\n    ES_PATIENCE = 2\n    WEIGHT_DECAY = 0.001\n    VAL_BATCH_SIZE = 32 * 2\n    RANDOM_STATE = 1994\n    LEARNING_RATE = 5e-5\n    TRAIN_BATCH_SIZE = 32\n    MEAN = (0.485, 0.456, 0.406)\n    STD = (0.229, 0.224, 0.225)\n    TRAIN_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n        \"target\",\n        \"tfrecord\",\n    ]\n    TEST_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n    ]\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    ################ Setting paths to data input ################\n\n    data_2020 = \"../input/jpeg-melanoma-512x512/\"\n    train_folder_2020 = data_2020 + \"train/\"\n    test_folder_2020 = data_2020 + \"test/\"\n    test_csv_path_2020 = data_2020 + \"test.csv\"\n    train_csv_path_2020 = data_2020 + \"train.csv\"\n    submission_csv_path = data_2020 + \"sample_submission.csv\"\n\nWriting Scripts/config.py\n\n\nCreating a single dataset class to read the images (both training, validation & testing images), the function is capable of handling/reading the tabular features.\nThe function takes a dataframe, a list of tabular features (if we want to use for training) i.e., list of strings like ['sex_missing',anatom_site_general_challenge_head_neck','anatom_site_general_challenge_lower_extremity',     anatom_site_general_challenge_torso','anatom_site_general_challenge_upper_extremity','scaled_age'] , the augmentations we want to use and finally whether the dataset is a training, validation or testing dataset.\nFor training and validation we set is_test=False and for testing we set is_test=True to differentiate between the datasets.\n\n%%writefile Scripts/dataset.py\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom typing import List, Callable\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass DatasetRetriever(nn.Module):\n    \"\"\"\n    Dataset class to read the images and tabular features from a\n    dataframe and returns the dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        tabular_features: List[str] = None,\n        use_tabular_features: bool = False,\n        augmentations: Callable = None,\n        is_test: bool = False,\n    ):\n        \"\"\" \"\"\"\n        self.df = df\n        self.tabular_features = tabular_features\n        self.use_tabular_features = use_tabular_features\n        self.augmentations = augmentations\n        self.is_test = is_test\n\n    def __len__(self):\n        \"\"\"\n        Function returns the number of images in a dataframe.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Function the takes an images and it's corresponding\n        tabular/meta features & target feature (for training\n        and validation) and returns a dictionary, otherwise,\n        for test dataset it only returns a dictionary of\n        an image and tabular features.\n        \"\"\"\n        image_path = self.df[\"image_path\"].iloc[index]\n        image = Image.open(image_path)\n        image = np.array(image)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.tensor(image, dtype=torch.float)\n        if self.use_tabular_features:\n            if len(self.tabular_features) &gt; 0 and self.is_test is False:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"tabular_features\": tabular_features,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif len(self.tabular_features) &gt; 0 and self.is_test is True:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                return {\"image\": image, \"tabular_features\": tabular_features}\n        else:\n            if self.is_test is False:\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif self.is_test is True:\n                return {\"image\": image}\n\nWriting Scripts/dataset.py\n\n\nNow we create a model class to create a model instance of EfficientNet model.\nCurrently, this function is capable of reading the images only and not the tabular features.\nSince in this project/notebook we are using the images only therefore, this function is good enough for that.\n\n%%writefile Scripts/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Class to instantiate EfficientNet-b5 model object which only\n    used images as inputs.\n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n    \n    \n# class Model(nn.Module):\n#     \"\"\"\n#     Class to instantiate EfficientNet-b5 model object which uses images\n#     as well as tabular features as inputs.\n#     \"\"\"\n#     def __init__(self, model_name='efficientnet-b5', pool_type=F.adaptive_avg_pool2d,\n#                 num_tabular_features=0):\n#         super().__init__()\n#         self.pool_type = pool_type\n#         self.model_name = model_name\n#         self.backbone = EfficientNet.from_pretrained(model_name)\n#         in_features = getattr(self.backbone, \"_fc\").in_features\n#         if num_tabular_features&gt;0:\n#             self.meta = nn.Sequential(\n#                 nn.Linear(num_tabular_features, 512),\n#                 nn.BatchNorm1d(512),\n#                 nn.ReLU(),\n#                 nn.Dropout(p=0.5),\n#                 nn.Linear(512, 128),\n#                 nn.BatchNorm1d(128),\n#                 nn.ReLU())\n#             in_features += 128\n#         self.output = nn.Linear(in_features, 1)\n    \n#     def forward(self, image, tabular_features=None):\n#         features = self.pool_type(self.backbone.extract_features(image), 1)\n#         cnn_features = features.view(image.size(0),-1)\n#         if num_tabular_features&gt;0:\n#             tabular_features = self.meta(tabular_features)\n#             all_features = torch.cat((cnn_features, tabular_features), dim=1)\n#             output = self.output(all_features)\n#             return output\n#         else:\n#             output = self.output(cnn_features)\n#             return output\n\nWriting Scripts/model.py\n\n\nWe create a validation function that predicts and generates probabilities only on the validation corresponding to a specific fold.\nThis function might be useful in come cases. This function is capable of running on a single gpu or multi-gpu device as well as on cpu.\n\n%%writefile Scripts/predict_on_validation_data.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import validation_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_validation_dataset(\n    validation_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    validation dataset and returns a submission.csv file.\n    Args:\n        validation_dataset = validation_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    valid_dataset = DatasetRetriever(\n        df=validation_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=True,\n    )\n    valid_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    valid_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(valid_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            valid_predictions.extend(valid_probs)\n    valid_predictions = [\n        valid_predictions[img].item() for img in range(len(valid_predictions))\n    ]\n    return valid_predictions\n\nWriting Scripts/predict_on_validation_data.py\n\n\nThis below function is used to generate the prediction probabilities on the testing dataset provided for the competition and generates a submission.csv file for the public and private leaderboard results.\n\n%%writefile Scripts/predict_on_test.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import testing_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_test_and_generate_submission_file(\n    test_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    test dataset and returns a submission.csv file.\n    Args:\n        test_df = test_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    test_dataset = DatasetRetriever(\n        df=test_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=testing_augmentations,\n        is_test=True,\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    test_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(test_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            test_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            test_predictions.extend(test_probs)\n    submission_df = pd.read_csv(Config.submission_csv_path)\n    test_predictions = [\n        test_predictions[img].item() for img in range(len(test_predictions))\n    ]\n    submission_df[\"target\"] = test_predictions\n    submission_df.to_csv(\"../working/submission.csv\", index=False)\n\nWriting Scripts/predict_on_test.py\n\n\nNow, we create a train_model module which has a run_model function that takes a fold number and the training dataframe.\nThe function creates training and validation dataframe , then we create training and validation datasets which only reads images and no tabular features, next we initialize seed (for reproduciblity of results), model object, loss function, optimizer, scheduler and a scaler object (for mixed precision).\n\n%%writefile Scripts/train_model.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nimport torch.cuda.amp as amp\nfrom Scripts.utils import create_folds\nfrom Scripts.utils import seed_everything\nfrom Scripts.dataset import DatasetRetriever\nfrom timeit import default_timer as timer\nfrom Scripts.training_and_validation_loops import train\nfrom torch.utils.data import Dataset, DataLoader\nfrom Scripts.augmentations import training_augmentations, validation_augmentations\n\ndef run_model(fold, train_df):\n    train_df = create_folds(train_df=train_df)\n    train_data = train_df.loc[train_df[\"fold\"] != fold].reset_index(drop=True)\n    valid_data = train_df.loc[train_df[\"fold\"] == fold].reset_index(drop=True)\n    validation_targets = valid_data[\"target\"]\n    train_dataset = DatasetRetriever(\n        df=train_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=training_augmentations,\n        is_test=False,\n    )\n    valid_dataset = DatasetRetriever(\n        df=valid_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=False,\n    )\n    training_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=Config.TRAIN_BATCH_SIZE,\n        shuffle=True,\n        num_workers=os.cpu_count(),\n    )\n    validation_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    seed_everything(Config.RANDOM_STATE)\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    loss = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(\n        params=model.parameters(),\n        lr=Config.LEARNING_RATE,\n        weight_decay=Config.WEIGHT_DECAY,\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer=optimizer,\n        mode=\"max\",\n        factor=0.2,\n        patience=2,\n        threshold=1e-3,\n        verbose=True,\n    )\n    scaler = amp.GradScaler()\n    start_time = timer()\n    model_save_path = f\"../working/Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"\n    model_results = train(\n        model=model,\n        train_dataloader=training_dataloader,\n        valid_dataloader=validation_dataloader,\n        loss_fn=loss,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=Config.DEVICE,\n        scaler=scaler,\n        epochs=Config.EPOCHS,\n        es_patience=2,\n        model_save_path=model_save_path,\n        validation_targets=validation_targets,\n    )\n    end_time = timer()\n    print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\nWriting Scripts/train_model.py\n\n\nRegular pytorch training and validation loops epochs for a single epoch and finally for N number of epochs a train function.\n\n%%writefile Scripts/training_and_validation_loops.py\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.cuda.amp as amp\nfrom Scripts.utils import EarlyStopping\nfrom sklearn.metrics import roc_auc_score\n\n\ndef train_one_epoch(\n    model, dataloader, loss_fn, optimizer, device, scaler, use_tabular_features=False\n):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, an optimizer, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    , scaler (for mixed precision) and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value on training dataset.\n    \"\"\"\n    train_loss = 0\n    model.train()\n    for data in dataloader:\n        optimizer.zero_grad()\n        if use_tabular_features:\n            data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                data[\"image\"].to(device, dtype=torch.float),\n                data[\"tabular_features\"].to(device, dtype=torch.float),\n                data[\"targets\"].to(device, dtype=torch.float),\n            )\n            with amp.autocast():\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        else:\n            data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                device, dtype=torch.float\n            ), data[\"targets\"].to(device, dtype=torch.float)\n            with amp.autocast():\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        train_loss += loss.item()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n\n\ndef validate_one_epoch(model, dataloader, loss_fn, device, use_tabular_features=False):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value & final predictions on the validation dataset.\n    \"\"\"\n    valid_loss, final_predictions = 0, []\n    model.eval()\n    with torch.inference_mode():\n        for data in dataloader:\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                    data[\"image\"].to(device, dtype=torch.float),\n                    data[\"tabular_features\"].to(device, dtype=torch.float),\n                    data[\"targets\"].to(device, dtype=torch.float),\n                )\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n            else:\n                data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                    device, dtype=torch.float\n                ), data[\"targets\"].to(device, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n            valid_loss += loss.item()\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            final_predictions.extend(valid_probs)\n    valid_loss = valid_loss / len(dataloader)\n    return valid_loss, final_predictions\n\n\ndef train(\n    model,\n    train_dataloader,\n    valid_dataloader,\n    loss_fn,\n    optimizer,\n    scheduler,\n    device,\n    scaler,\n    epochs,\n    es_patience,\n    model_save_path,\n    validation_targets,\n):\n    \"\"\"\n    This function takes a model instance, training dataloader,\n    validation dataloader, loss_fn, optimizer, scheduler, device,\n    scaler (object, for mixed precision), epochs (for how many epochs\n    to run the model), es_patience (number of epochs to wait after which\n    the model should stop training), model_save_path (where to save the\n    model to), validation_targets (used for the calculation of the AUC\n    score) and returns a dictionary object which has training loss,\n    validation loss and validation AUC score.\n    \"\"\"\n    results = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": []}\n\n    early_stopping = EarlyStopping(\n        patience=es_patience, verbose=True, path=model_save_path\n    )\n\n    for epoch in tqdm(range(epochs)):\n        train_loss = train_one_epoch(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            device=device,\n            scaler=scaler,\n            use_tabular_features=False,\n        )\n\n        valid_loss, valid_predictions = validate_one_epoch(\n            model=model,\n            dataloader=valid_dataloader,\n            loss_fn=loss_fn,\n            device=device,\n            use_tabular_features=False,\n        )\n\n        valid_predictions = np.vstack(valid_predictions).ravel()\n\n        valid_auc = roc_auc_score(y_score=valid_predictions, y_true=validation_targets)\n        scheduler.step(valid_auc)\n\n        early_stopping(valid_loss, model)\n\n        if early_stopping.early_stop:\n            print(\"Early Stopping\")\n            break\n\n        model.load_state_dict(torch.load(model_save_path))\n        print(\n            f\"Epoch : {epoch+1} | \"\n            f\"train_loss : {train_loss:.4f} | \"\n            f\"valid_loss : {valid_loss:.4f} | \"\n            f\"valid_auc : {valid_auc:.4f} \"\n        )\n        results[\"train_loss\"].append(train_loss)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_auc\"].append(valid_auc)\n    return results\n\nWriting Scripts/training_and_validation_loops.py\n\n\nIn the utils module we write some useful functions like create_folds (which will divide the training dataset into 5 equal parts and remove duplicate images from the dataset)\nseed_everything (for reproducing the results)\nEarlyStopping class (used to stop model training if our model performance on validation dataset starts to decline), plot_loss_curves (for plotting the training and validation loss and auc_scores for each epoch)\nRareLabelCategoryEncoder (class that combines the category/categories of a feature that appears in the dataset for a certain percentage of times like 5% of the time etc. into a single category called Rare)\nOutlierTreatment (class to cap the values of a feature by learning the lower quantile and upper_quantile values of a feature from the dataset (X) in the fit method and caps(transforms) the feature values in the dataset (x) passed in the transformed method).\n\n%%writefile Scripts/utils.py\nimport os\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import List\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\ndef create_folds(train_df):\n    \"\"\"\n    Function that folds in the training data and removes duplicate\n    images from the training data.\n    \"\"\"\n    train_df = train_df.loc[train_df[\"tfrecord\"] != -1].reset_index(drop=True)\n    train_df[\"fold\"] = train_df[\"tfrecord\"] % 5\n    return train_df\n\n\ndef seed_everything(seed: int):\n    \"\"\"\n    Function to set seed and to make reproducible results.\n    Args:\n        seed (int): like 42\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n    Directly borrowed from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        patience: int = 7,\n        verbose: bool = False,\n        delta: int = 0,\n        trace_func=print,\n    ):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score &lt; self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(\n                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n            )\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --&gt; {val_loss:.6f}).  Saving model ...\"\n            )\n        torch.save(obj=model.state_dict(), f=self.path)\n        self.val_loss_min = val_loss\n\n\ndef plot_loss_curves(results: dict):\n    \"\"\"\n    Function to plot training & validation loss curves & validation AUC\n    Args:\n        results (dict): A dictionary of training loss, validation_loss &\n        validation AUC score.\n    \"\"\"\n    loss = results[\"train_loss\"]\n    valid_loss = results[\"valid_loss\"]\n    # Get the accuracy values of the results dictionary (training and test)\n    valid_auc = results[\"valid_auc\"]\n    # Figure out how many epochs there were\n    epochs = range(len(results[\"train_loss\"]))\n    # Setup a plot\n    plt.figure(figsize=(15, 7))\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, valid_loss, label=\"valid_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, valid_auc, label=\"valid_auc\")\n    plt.title(\"AUC Score\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n\nclass RareLabelCategoryEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to combine rare categories of a categorical variable\n    where a category appearing less than a certain percentage\n    (as a threshold).\n    Example: a category/categories appearing less than 5% of the\n    times are combined a single category called rare.\n    \"\"\"\n\n    def __init__(self, variables: List, tol=0.05):\n        \"\"\"\n        Args:\n            variables (List): A list of variables for which we want\n            to combine into rare categories.\n            tol (int): A Threshold/Tolerance below which we want to\n            consider a category of a feature as rare.\n        \"\"\"\n        if not isinstance(variables, list):\n            raise ValueError(\"Variables should be a list\")\n        self.tol = tol\n        self.variables = variables\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns all the values/categories & the\n        percentage of times it appears in a feature in the\n        dataset passed while using this method.\n\n        Args:\n            X : From this dataset the fit function learns and\n            stores the number of times a category appears in\n            the dataset\n        \"\"\"\n        self.encoder_dict_ = {}\n        for var in self.variables:\n            t = pd.Series(x[var]).value_counts(normalize=True)\n            self.encoder_dict_[var] = list(t[t &gt;= self.tol].index)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        X (pd.DataFrame): Transform/Combines the categories of each\n        features passed into the variables list on the dataset passed\n        in this method and returns the transformed dataset.\n        \"\"\"\n        x = x.copy()\n        for var in self.variables:\n            x[var] = np.where(x[var].isin(self.encoder_dict_[var]), x[var], \"Other\")\n        return x\n\n\nclass OutlierTreatment(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to handle outliers in a continous feature.\n    \"\"\"\n\n    def __init__(\n        self, variable: str, upper_quantile: float = None, lower_quantile: float = None\n    ):\n        \"\"\"\n        Args:\n            variables (str): A variable to cap the upper and\n            lower boundaries of.\n            upper_quantile (float): A maximum value beyond which all the\n            values of a feature are capped at.\n            lower_quantile (float): A minimum value that are lower than\n            of the feature are capped at.\n        \"\"\"\n        if not isinstance(variable, str):\n            raise ValueError(\"Variable should be a string type.\")\n        self.upper_quantile = upper_quantile\n        self.variable = variable\n        self.lower_quantile = lower_quantile\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns the lower & upper quantiles of a feature\n        present in the dataset x.\n        \"\"\"\n        self.upper_quantile = x[self.variable].quantile(self.upper_quantile)\n        self.lower_quantile = x[self.variable].quantile(self.lower_quantile)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        This function caps the upper and lower quantiles in the dataframe\n        x with the values learnt in the dataframe passed in fit() method.\n        \"\"\"\n        x = x.copy()\n        x[self.variable] = np.where(\n            x[self.variable] &gt; self.upper_quantile,\n            self.upper_quantile,\n            np.where(\n                x[self.variable] &lt; self.lower_quantile,\n                self.lower_quantile,\n                x[self.variable],\n            ),\n        )\n        return x\n\nWriting Scripts/utils.py\n\n\n\nInitializing the __init__.py to make the Scripts folder a package.\n\n\n%%writefile Scripts/__init__.py\n\"\"\n\nWriting Scripts/__init__.py"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating requirements.txt file to install all the packages for our model training",
    "text": "Creating requirements.txt file to install all the packages for our model training\nThis will contain all the packages required for modeling/training the model.\n\n%%writefile requirements.txt\n# pandas==2.0.0\ntorch==1.13.0\ntorchvision==0.14.0\n# scikit-learn==1.2.2\nefficientnet_pytorch==0.7.1\nalbumentations==1.2.1\n# numpy==1.22.4\ntqdm==4.65.0\n# matplotlib==3.7.1\nPillow==8.4.0\n\nWriting requirements.txt"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Installing all the packages",
    "text": "Installing all the packages\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.13.0)\nRequirement already satisfied: torchvision==0.14.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.14.0)\nCollecting efficientnet_pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... done\nCollecting albumentations==1.2.1\n  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.7/116.7 kB 5.3 MB/s eta 0:00:00\nCollecting tqdm==4.65.0\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 8.9 MB/s eta 0:00:00\nCollecting Pillow==8.4.0\n  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 32.8 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0-&gt;-r requirements.txt (line 2)) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.7.3)\nRequirement already satisfied: scikit-image&gt;=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.19.3)\nRequirement already satisfied: opencv-python-headless&gt;=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (4.5.4.60)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (6.0)\nRequirement already satisfied: qudida&gt;=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.0.4)\nRequirement already satisfied: scikit-learn&gt;=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.0.2)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2021.11.2)\nRequirement already satisfied: imageio&gt;=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.25.0)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: networkx&gt;=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.6.3)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (23.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.26.14)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.1.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2022.12.7)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (3.1.0)\nRequirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.2.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... done\n  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=d870e4ba77c41d05a67b458d1a03a108aaee250b6f63fca1cad356a67702a3af\n  Stored in directory: /root/.cache/pip/wheels/96/3f/5f/13976445f67f3b4e77b054e65f7f4c39016e92e8358fe088db\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: tqdm, Pillow, efficientnet_pytorch, albumentations\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Uninstalling tqdm-4.64.1:\n      Successfully uninstalled tqdm-4.64.1\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 9.4.0\n    Uninstalling Pillow-9.4.0:\n      Successfully uninstalled Pillow-9.4.0\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.3.0\n    Uninstalling albumentations-1.3.0:\n      Successfully uninstalled albumentations-1.3.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires tqdm&lt;4.65,&gt;=4.48.2, but you have tqdm 4.65.0 which is incompatible.\nSuccessfully installed Pillow-9.4.0 albumentations-1.2.1 efficientnet_pytorch-0.7.1 tqdm-4.65.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore jumping right into the modeling and see the numbers go down or go up, it’s good to look at the data and try to make sense out of it.\nWe’ll do the same here as well, we will look at the distribution of each tabular features we have in the training and testing datasets.\n\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom Scripts.config import Config\n\n## Looking at the Data\ntrain_df = pd.read_csv(Config.train_csv_path_2020,\n                       usecols=Config.TRAIN_COLS)\ntest_df = pd.read_csv(Config.test_csv_path_2020,\n                       usecols=Config.TEST_COLS)\n\n## Creating Image_Path for each images in 2019 & 2020 training datasets\ntrain_df['image_path'] = os.path.join(Config.train_folder_2020) + train_df['image_name'] + \".jpg\"\ntest_df['image_path'] = os.path.join(Config.test_folder_2020) + test_df['image_name'] + \".jpg\"\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nimage_name\npatient_id\nsex\nage_approx\nanatom_site_general_challenge\ntarget\ntfrecord\nimage_path\n\n\n\n\n0\nISIC_2637011\nIP_7279968\nmale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_2637...\n\n\n1\nISIC_0015719\nIP_3075186\nfemale\n45.0\nupper extremity\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0015...\n\n\n2\nISIC_0052212\nIP_2842074\nfemale\n50.0\nlower extremity\n0\n6\n../input/jpeg-melanoma-512x512/train/ISIC_0052...\n\n\n3\nISIC_0068279\nIP_6890425\nfemale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0068...\n\n\n4\nISIC_0074268\nIP_8723313\nfemale\n55.0\nupper extremity\n0\n11\n../input/jpeg-melanoma-512x512/train/ISIC_0074...\n\n\n\n\n\n\n\n\nprint(f\"Number of Unique images id's in the training dataset are - {train_df['image_name'].nunique()} \\n\")\nprint(f\"Number of Unique images id's in the training dataset are - {test_df['image_name'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {train_df['patient_id'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {test_df['patient_id'].nunique()}\")\n\nNumber of Unique images id's in the training dataset are - 33126 \n\nNumber of Unique images id's in the training dataset are - 10982\n\nTotal number of Unique patients id's in the training dataset are - 2056\n\nTotal number of Unique patients id's in the training dataset are - 690"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Patient ID",
    "text": "Patient ID\n\npatients_id_counts_train = train_df['patient_id'].value_counts()\npatients_id_counts_test = test_df['patient_id'].value_counts()\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(19,10), dpi=80, sharex=False, sharey=False)\nsns.distplot(patients_id_counts_train, ax=ax[0,0], color='#3300CC', kde=True)\nax[0,0].set_xlabel(\"Counts\")\nax[0,0].set_ylabel('Frequency')\nax[0,0].set_title('Patient ID counts in training data')\n\nsns.distplot(patients_id_counts_test, ax=ax[0,1], color='#FF0099', kde=True)\nax[0,1].set_xlabel(\"Counts\")\nax[0,1].set_ylabel('Frequency')\nax[0,1].set_title('Patient ID counts in testing data')\n\nsns.boxplot(patients_id_counts_train, ax=ax[1,0], color='#3300CC')\nax[1,0].set_xlabel('Counts')\nax[1,0].set_title('BoxPlot of Patient ID Counts in Train data')\nsns.boxplot(patients_id_counts_test, ax=ax[1,1], color='#FF0099')\nax[1,1].set_xlabel('Counts')\nax[1,1].set_title('BoxPlot of Patient ID Counts in Test data');"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#gender",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#gender",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Gender",
    "text": "Gender\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x=train_df['sex'], color='#3300CC', ax=ax[0])\nax[0].set_ylabel(\"\")\nax[0].set_xlabel('Gender Count')\nax[0].set_title(\"Gender Count in Training data\")\n\nsns.countplot(x=test_df['sex'], color=\"#FF0099\", ax=ax[1])\nax[1].set_ylabel(\"\")\nax[1].set_xlabel('Gender Count')\nax[1].set_title(\"Gender Count in Testing data\");"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#age",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#age",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Age",
    "text": "Age\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\nsns.countplot(train_df['age_approx'], color='#3300CC', ax=ax[0])\nax[0].set_title(\"Distribution of Age feature in Training Data\")\nax[0].set_xlabel(\"Age\")\nax[0].set_ylabel('Frequency')\n\nsns.countplot(test_df['age_approx'], color='#FF0099', ax=ax[1])\nax[1].set_title(\"Distribution of Age feature in Testing Data\")\nax[1].set_xlabel(\"Age\")\nax[1].set_ylabel('Frequency');\n\n\n\n\n\nage_dist_train_test = pd.concat(\n    [train_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93, \n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99]),\n    test_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93,\n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99])\n    ], axis=1)\nage_dist_train_test.columns = ['Train_Age', 'Test_Age']\nage_dist_train_test.T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n10%\n15%\n25%\n50%\n...\n91%\n92%\n93%\n94%\n95%\n96%\n97%\n98%\n99%\nmax\n\n\n\n\nTrain_Age\n33058.0\n48.870016\n14.380360\n0.0\n20.0\n25.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n75.0\n80.0\n90.0\n\n\nTest_Age\n10982.0\n49.525587\n14.370589\n10.0\n20.0\n30.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n80.0\n80.0\n85.0\n90.0\n\n\n\n\n2 rows × 22 columns"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "anatom_site_general_challenge",
    "text": "anatom_site_general_challenge\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\n\ntrain_anatom_site_general = train_df[\"anatom_site_general_challenge\"].value_counts().sort_values(ascending=False)\ntest_anatom_site_general = test_df.anatom_site_general_challenge.value_counts().sort_values(ascending=False)\n\nsns.barplot(x=train_anatom_site_general.index.values, y=train_anatom_site_general.values, ax=ax[0]);\nax[0].set_xlabel(\"\");\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_title(\"Image locations in train\");\n\nsns.barplot(x=test_anatom_site_general.index.values, y=test_anatom_site_general.values, ax=ax[1]);\nax[1].set_xlabel(\"\");\nlabels = ax[1].get_xticklabels();\nax[1].set_xticklabels(labels, rotation=90);\nax[1].set_title(\"Image locations in test\");"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#target",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Target",
    "text": "Target\n\ntrain_df['target'].value_counts(normalize=True, dropna=False) * 100\n\n0    98.237034\n1     1.762966\nName: target, dtype: float64"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Sex Vs. Target & anatom_site_general_challenge Vs. Target",
    "text": "Sex Vs. Target & anatom_site_general_challenge Vs. Target\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x='target', hue='sex',data=train_df, ax=ax[0])\nax[0].set_xlabel(\"Target\", fontsize=15)\nax[0].set_ylabel('Count', fontsize=15)\nax[0].set_title(\"Sex Vs. Target\", fontsize=20)\n\nsns.countplot(x='target', hue='anatom_site_general_challenge',data=train_df, ax=ax[1])\nax[1].set_xlabel(\"Target\", fontsize=15)\nax[1].set_ylabel('Count', fontsize=15)\nax[1].set_title(\"Sex Vs. anatom_site_general_challenge\", fontsize=20);"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Age Vs. Target",
    "text": "Age Vs. Target\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_df.loc[train_df['target'] == 0]['age_approx'], bins=50, label='Benign')\nsns.distplot(train_df.loc[train_df['target'] == 1]['age_approx'], bins=50, label='Malignant')\nplt.legend(loc='best')\nplt.ylabel('Density', fontsize=15)\nplt.xlabel('Age', fontsize=15)\nplt.title(\"Age Vs. Target distribution\", fontsize=20);"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Training our model with No Tabular features",
    "text": "Training our model with No Tabular features\n\nFor now, we will run/train the model on a single fold only.\n\n\nfrom Scripts.train_model import run_model\nrun_model(fold=0, train_df=train_df)\n\n\n\n\nLoaded pretrained weights for efficientnet-b5\nValidation loss decreased (inf --&gt; 0.071890).  Saving model ...\nEpoch : 1 | train_loss : 0.0986 | valid_loss : 0.0719 | valid_auc : 0.8579 \nValidation loss decreased (0.071890 --&gt; 0.067238).  Saving model ...\nEpoch : 2 | train_loss : 0.0749 | valid_loss : 0.0672 | valid_auc : 0.8902 \nEarlyStopping counter: 1 out of 2\nEpoch : 3 | train_loss : 0.0708 | valid_loss : 0.0709 | valid_auc : 0.8746 \nValidation loss decreased (0.067238 --&gt; 0.065598).  Saving model ...\nEpoch : 4 | train_loss : 0.0706 | valid_loss : 0.0656 | valid_auc : 0.8971 \nEarlyStopping counter: 1 out of 2\nEpoch : 5 | train_loss : 0.0683 | valid_loss : 0.0673 | valid_auc : 0.9015 \nTotal training time: 16369.841 seconds"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Generating submission file on the test dataset",
    "text": "Generating submission file on the test dataset\n\nfrom Scripts.predict_on_test import predict_on_test_and_generate_submission_file\n\nmodel_path = \"../working/Models/efficientnet_b5_checkpoint_fold_0.pt\"\npredict_on_test_and_generate_submission_file(test_df=test_df,\n                                            use_tabular_features=False,\n                                            model_path=model_path)\n\nLoaded pretrained weights for efficientnet-b5"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Building Gradio demo",
    "text": "Building Gradio demo\nNow that we have finalized the model we’ll be deploying, we will use this EfficientNet-B5 model to predict on new images.\nFor this project I will be using Gradio library (product of HuggingFace).\nWhy Gradio? The homepage of Gradio descibes it as: &gt; Gradio is the fastest way to build/demo your machine learning model with a friendly web interface so that anyone can use it, anywhere.\n\nGradio Overview\nIn general, we can have any combination of inputs like - Images - Tabular data - Text - Numbers - Video - Audio - etc.\nIn our case we have images and inputs and the output is returned as a probability of whether a patient is sufferig from melanoma skin cancer disease.\nGradio provides an interface that maps from the input(s) to output(s).\ngr.Interface(function, inputs, outputs)\nWhere, fn is a python function to map inputs to outputs\nGradio provides a very helpful Interface class to create an inputs -&gt; model/function -&gt; outputs workflow where the inputs and outputs could be almost anything we want.\n\n## Installing Gradio and importing it. \ntry:\n    import gradio as gr\nexcept:\n    !pip install -q gradio\n    import gradio as gr\nprint(f\"Gradio version : {gr.__version__}\")\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nGradio version : 3.27.0"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating a model instance and putting it on the CPU",
    "text": "Creating a model instance and putting it on the CPU\nFirst, let’s make sure our EfficientNetB5 model on CPU\n\nimport torch\nimport numpy as np\nimport albumentations as A\nfrom Scripts.model import Model\n\nefficientnet_b5_model = Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\nnext(iter(model.parameters())).device\n\nLoaded pretrained weights for efficientnet-b5\n\n\ndevice(type='cpu')"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating a function to predict on a single images",
    "text": "Creating a function to predict on a single images\nWe create a function that takes an input: image -&gt; transform -&gt; predict with EfficientNetB5 -&gt; output: probability.\nThis will be our fn parameter for our Gradio Interface.\n\n## predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n\nLet’s see our function by performing a prediction of an image from the training dataset.\nWe’ll get some images from training dataset and extract the images paths list. Then we’ll open an image with Image.open().\nFinally, we pass the image to predict_on_single_image().\n\nimport torch\nimport pathlib\nimport numpy as np\nfrom PIL import Image\n\n## Taking some images out\nimages = train_df.iloc[1:10,]\nimages_paths_list = images['image_path'].tolist()\n\n## Opening an Image\nimg = Image.open(images_paths_list[8])\n\n## Predicting on the image using the function\npredict_on_single_image(img)\n\n{'Probability of Having Melanoma': 0.5395416617393494,\n 'Probability of Not having Melanoma': 0.46045833826065063}\n\n\n\nCreating a list of example images\nBefore we create a demo, we first create a list of examples.\nGradio’s Interface class takes a list of examples parameter is a list of lists.\nSo, we create a list of lists containing the filepaths of images.\nOur gradio demo will showcase these as example inputs to our demo so people can try.\n\nexample_list = [[str(file_path)] for file_path in images_paths_list]\nexample_list\n\n[['../input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg']]\n\n\n\n\nBuilding a Gradio interface\nPutting everything together -&gt;\nGradio Interface Workflow: input: image -&gt; transform -&gt; predict with EfficientNetB5 model -&gt; probability: output\nWe can do with the Gr.Interface() class with the following parameters: - fn: a python function that maps from inputs to outputs, in our case the predict_on_single_image() function. - inputs: the input to our Interface, such as image using gradio.Image(). - outputs: the output of the Interface once the inputs are processed with the fn, such as a Number gradio.Number() (for our case probability). - examples: a list of examples to showcase for the demo. - title: a string title of the demo. - description: a string description of the demo.\nOnce, we’ve created a demo instance of gr.Interface(), we use demo.launch() command.\n\nimport gradio as gr\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\nRunning on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://e228186381e2781228.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n\n\n\nWoow!!!\nOur application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting, we can upload our Gradio app Interface to HuggingFace Spaces.\nNow download all the files and folders from kaggle output manually & this kaggle kernel locally"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app",
    "text": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app\nWe’ll deploy the demo application on HuggingFace Spaces.\nWhat is HuggingFace Spaces?\nIt is a resource that allows anybody to host and share machine learning application.\n\nDeployed Gradio App Structure\nTo upload our gradio app, we’ll want to put everything together into a singe directory.\nFor example, our demo might live at the path demos/melanoma_skin_cancer_files with the following structure:\ndemos/\n    └── melanoma_skin_cancer_files/\n        ├── efficientnet_b5_checkpoint_fold_0.pt\n        ├── app.py\n        ├── examples/\n        │   ├── image_1.jpg\n        │   ├── image_2.jpg\n        │   └── image_3.jpg\n        ├── model.py\n        └── requirements.txt\nWhere: - efficientnet_b5_checkpoint_fold_0 is our trained model. - app.py contains our Gradio app. Note: app.py is the default filename used for HuggingFace Spaces, if we deploy our apps there. - examples contains sample images to showcase the demo of our Gradio application. - model.py contains the main model/transformations code associated with our model. - requirements.txt file contains the dependencies/packages to run our application such as torch, albumentations, torchvision, gradio, numpy.\n\n\nCreating a demo folder to store our Melanoma skin cancer App files\nTo begin, we’ll create an empty directory demos/ that will contain all our necessary files for the application.\nWe can achive this using Python’s pathlib.Path(\"path_of_dir\") to establish directory path and then pathlib.Path(\"path_of_dir\").mkdir() to create it.\n\n############### ROOT_DIR : I Have put the files in my E: drive\n## Importing Packages \nimport shutil\nfrom pathlib import Path\nimport os\n\nROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n\n## Create Melanoma skin cancer demo path\nmelanoma_app_demo_path = Path(f\"{ROOT_DIR}/demos/melanoma_skin_cancer_files\")\n\n## Removing files that might already exist and creating a new directory.\nif melanoma_app_demo_path.exists():\n    shutil.rmtree(melanoma_app_demo_path)\n    melanoma_app_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n                                exist_ok=True) # Create even if they already exists? \nelse:\n    ## If the path doesn't exist, create one \n    melanoma_app_demo_path.mkdir(parents=True,\n                                exist_ok=True)\n\n\n\nCreating a folder of example images to use with our Melanoma skin cancer demo\nNow we’ll create an empty directory called examples and store some images (namely - ISIC_0015719.jpg, ISIC_0052212.jpg, ISIC_0068279.jpg) from the training dataset provided in the competition (which we download manually) from the here. Download the 1ts three images from the training dataset mentioned above.\nPut these images in the Data/Input (whatever you want to call) folder.\nTo do so we’ll:\n\nCreate an empty directory examples/ within the demos/melanoma_skin_cancer_files directory.\nDownload the top 3 mentioned images from the training dataset from the link above.\nCollect the filepaths into a list.\nCopy these 3 images from the train dataset to the demos/melanom_skin_cancer_files/examples/ directory.\n\n\nimport shutil\nfrom pathlib import Path\n\n## Create examples directory\nmelanoma_app_examples_path = melanoma_app_demo_path / \"examples\"\nmelanoma_app_examples_path.mkdir(parents=True, exist_ok=True)\n\n## collecting the image paths of 4 images \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0149568.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\n\n## collecting the image paths of some more images but this time from the testing folder \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0052060.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082004.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082785.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0105104.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0112420.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0155983.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0171865.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\nNow we verify our example images are present, let’s list the contents of our demo/melanoma_skin_cancer/examples/ directory with os.listdir() and then format the filepaths into a list of lists (to make it compatible with the Gradio’s gradio.Interface(), example parameter).\n\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\nexample_list\n\n[['examples/ISIC_0015719.jpg'],\n ['examples/ISIC_0052060.jpg'],\n ['examples/ISIC_0052212.jpg'],\n ['examples/ISIC_0068279.jpg'],\n ['examples/ISIC_0082004.jpg'],\n ['examples/ISIC_0082785.jpg'],\n ['examples/ISIC_0105104.jpg'],\n ['examples/ISIC_0112420.jpg'],\n ['examples/ISIC_0149568.jpg'],\n ['examples/ISIC_0155983.jpg'],\n ['examples/ISIC_0171865.jpg']]\n\n\n\n\nMoving our trained EfficientNet-B5 model into our Melanoma demo directory.\nWe previously saved our model binary file into the Models directory while training as Models/efficientnet_b5_checkpoint_fold_0.pt.\nWe use Python’s shutil.move() method and passing in src(the source path of the target file) and dst (the destination folder path of the target file to be moved into) parameters.\n\n## Importing Libraries\nimport shutil\n\n## Create a source path for our target model\nefficientnet_b5_model_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Models\\\\efficientnet_b5_checkpoint_fold_0.pt\"\n\n## Create a destination path for our target model\nefficientnet_b5_model_destination = melanoma_app_demo_path / efficientnet_b5_model_path.split(\"\\\\\")[-1]\n\n## Try to move the file\ntry:\n    print(f\"Attempting to move the {efficientnet_b5_model_path} to {efficientnet_b5_model_destination}\")\n    \n    ## Move the model\n    shutil.move(src=efficientnet_b5_model_path,\n               dst=efficientnet_b5_model_destination)\n    \n    print(\"Model move completed\")\n## If the model has already been moved, check if it exists\nexcept:\n    print(f\"No model found at {efficientnet_b5_model_path}, perhaps it's already moved.\")\n    print(f\"Model already exists at {efficientnet_b5_model_destination}: {efficientnet_b5_model_destination.exists()}\")\n\nAttempting to move the E:\\Melanoma_skin_cancer_detection\\output\\working\\Models\\efficientnet_b5_checkpoint_fold_0.pt to E:\\Melanoma_skin_cancer_detection\\demos\\melanoma_skin_cancer_files\\efficientnet_b5_checkpoint_fold_0.pt\nModel move completed\n\n\n\n\nTurning our EfficientNet-B5 model into a Python script (model.py)\nOur current model’s state_dict() is saved to demos/melanoma_skin_cancer/efficientnet_b5_checkpoint_fold_0.pt.\nTo load it it we can use model.load_state_dict() with torch.load(). But before that we need to instantiate a model.\nTo do this in a modular fashion we’ll create a script model.py which contains the model definition into a function called Model().\n\n## Now if we look into which directory we are currently, we'll find that using the following code\nos.getcwd()\n\n'E:\\\\Melanoma_skin_cancer_detection\\\\notebooks'\n\n\nNow we will move into the demos directory where we will write some helper utilities.\nIn cd ../demos/: .. means we are moving outside of the notebooks directory. demos/: means we moving inside the demos directory.\n\ncd ../demos/\n\nE:\\Melanoma_skin_cancer_detection\\demos\n\n\n\n%%writefile melanoma_skin_cancer_files/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Creates an efficientnet-b5 model instance.    \n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n\nWriting melanoma_skin_cancer_files/model.py\n\n\n\n\nTurning our Melanoma Skin Cancer Gradio App into a Python Script (app.py)\n\n%%writefile melanoma_skin_cancer_files/app.py\n## Importing Libraries\nimport os\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom model import Model\nimport albumentations as A\n\n## Creating a model instance\nefficientnet_b5_model =  Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\n\n## Predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    efficientnet_b5_model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(efficientnet_b5_model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n    \n## Gradio App\nimport gradio as gr\n\n## Examples directory path\nmelanoma_app_examples_path = \"examples\"\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\nWriting melanoma_skin_cancer_files/app.py\n\n\n\n\nCreating a requirements.txt file for our Gradio App(requirements.txt)\nThis is the last file we need to create for our application.\nThis file contains all the necessary packages for our Gradio application.\nWhen we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\n\n%%writefile melanoma_skin_cancer_files/requirements.txt\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\nWriting melanoma_skin_cancer_files/requirements.txt"
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Deploying our Application to HuggingFace Spaces",
    "text": "Deploying our Application to HuggingFace Spaces\nTo deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n\nUploading via the Hugging Face Web Interface (easiest)\nUploading via the command line or terminal\n\nNOTE: To host any application on HuggingFace, we first need to sign up for a free HuggingFace Account\n\nRunning our Application locally\n\nOpen the terminal or command prompt.\nChanging the melanoma_skin_cancer_files directory (cd melanoma_skin_cancer_files).\nCreating an environment (python3 -m venv env) or use (python -m venv env).\nActivating the environment (source env/Scripts/activate).\nInstalling the requirements.txt using pip install -r requirements.txt. &gt; If faced any errors, we might need to upgrade pip using pip install --upgrade pip.\n\nRun the app (python3 app.py).\n\nThis should results in a Gradio demo locally at the URL such as : http://127.0.0.1:7860/.\n\n\nUploading to Hugging Face\nWe’ve verified our Melanoma_skin_cancer detection application is working in our local system.\nTo upload our application to Hugging Face Spaces, we need to do the following.\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to our profile at the top right corner and then select New Space.\nDeclare the name to the space like Chirag1994/melanoma_skin_cancer_detection_app.\nSelect a license (I am using MIT license).\nSelect Gradio as the Space SDK (software development kit).\nChoose whether your Space is Public or Private (I am keeping it Public).\nClick Create Space.\nClone the repository locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in the terminal or command prompt. In our case mine would be like - git clone https://huggingface.co/spaces/Chirag1994/melanoma_skin_cancer_detection_app.\nCopy/Move the contents of the downloaded melanoma_skin_cancer_detection_app folder to the cloned repo folder.\nTo upload files and track larger files (e.g., files that are greater than 10MB) for them we need to install Git LFS which stands for Git large File Storage.\nOpen up the cloned directory using VS code (I’m using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command git lfs install to start tracking the file that we want to track. For example - git lfs track \"efficientnet_b5_checkpoint_fold_0.pt\".\nCreate a new .gitignore file and the files & folders that we don’t want git to track like :\n\n__pycache__/\n.vscode/\nvenv/\n.gitignore\n.gitattributes\n\nAdd the rest of the files and commit them with:\n\ngit add .\ngit commit -m \"commit message that you want\"\n\nPush(load) the files to Hugging Face\n\ngit push\n\nIt might a couple of minutes to finish and then the app will be up and running."
  },
  {
    "objectID": "posts/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "href": "posts/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Our Final Application deployed on HuggingFace Spaces",
    "text": "Our Final Application deployed on HuggingFace Spaces\n\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/Chirag1994/Melanoma_Skin_Cancer_Detection_App/+\", width=900, height=750)"
  }
]