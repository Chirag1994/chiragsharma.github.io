[
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html",
    "href": "blog/Chat_with_YouTube_Videos/index.html",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "",
    "text": "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\nIn this project, you’ll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.\n\nUnderstanding the building blocks of working with Multimodal AI projects\nWorking with some of the fundamental concepts of LangChain\n\nHow to use the Whisper API to transcribe audio to text\nHow to combine both LangChain and Whisper API to create ask questions of any YouTube video"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#objectives",
    "href": "blog/Chat_with_YouTube_Videos/index.html#objectives",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "",
    "text": "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\nIn this project, you’ll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.\n\nUnderstanding the building blocks of working with Multimodal AI projects\nWorking with some of the fundamental concepts of LangChain\n\nHow to use the Whisper API to transcribe audio to text\nHow to combine both LangChain and Whisper API to create ask questions of any YouTube video"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#before-you-begin",
    "href": "blog/Chat_with_YouTube_Videos/index.html#before-you-begin",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Before you begin",
    "text": "Before you begin\nYou’ll need a developer account with OpenAI and a create API Key. The API secret key will be stored in your ‘Environment Variables’ on the side menu. See the getting-started.ipynb notebook for details on setting this up."
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-0-setup",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-0-setup",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 0: Setup",
    "text": "Task 0: Setup\nThe project requires several packages that need to be installed into Workspace.\n\nlangchain is a framework for developing generative AI applications.\nyt_dlp lets you download YouTube videos.\ntiktoken converts text into tokens.\ndocarray makes it easier to work with multi-model data (in this case mixing audio and text).\n\n\nInstructions\nRun the following code to install the packages.\n\nInstalling Relevant Libraries\n\n!pip install langchain==0.0.228 yt_dlp==2023.7.6 tiktoken==0.5.1 docarray==0.38.0 chromadb==0.4.19 openai==0.28 --quiet\n\nWrite and Store your OpenAI API Key in the .env file as OPENAI_API_KEY = \"PASTE_YOUR_OPENAI_API_KEY\".\n\n\nLoading the OpenAI API Secret file from .env file.\n\nfrom dotenv import load_dotenv, find_dotenv\n## Loading the Secrets from the .env file\nprint(load_dotenv())\n\nTrue"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-1-import-the-required-libraries",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-1-import-the-required-libraries",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 1: Import The Required Libraries",
    "text": "Task 1: Import The Required Libraries\nFor this project we need the os and the yt_dlp packages to download the YouTube video of your choosing, convert it to an .mp3 and save the file. We will also be using the openai package to make easy calls to the OpenAI models we will use.\nImport the following packages.\n\nImport os\nImport openai\nImport yt_dlp with the alias youtube_dl\nFrom the yt_dlp package, import DowloadError\nAssign openai_api_key to os.getenv(\"OPENAI_API_KEY\")\n\n\nImporting the Required Packages including: “os” “openai” “yt_dlp as youtube_dl” and “from yt_dl import Download Error”\n\n# Import the os package \nimport os\n# Import Glob package\nimport glob\n# Import the openai package \nimport openai\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n# Import DownloadError from yt_dlp \nfrom yt_dlp import DownloadError\n# Import DocArray\nimport docarray\n\nWe will also assign the variable openai_api_key to the environment variable “OPEN_AI_KEY”. This will help keep our key secure and remove the need to write it in the code here.\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-2-download-the-youtube-video",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-2-download-the-youtube-video",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 2: Download the YouTube Video",
    "text": "Task 2: Download the YouTube Video\nAfter creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3).\nWe’ll download a DataCamp tutorial about machine learning in Python.\nWe will do this by setting a variable to store the youtube_url and the output_dir that we want the file to be stored.\nThe yt_dlp allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you.\nLastly, we will create a loop that looks in the output_dir to find any .mp3 files. Then we will store those in a list called audio_files that will be used later to send each file to the Whisper model for transcription.\nCreate the following: - Two variables - youtube_url to store the Video URL and output_dir that will be the directory where the audio files will be saved. - For this tutorial, we can set the youtube_url to the following \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"and the output_dirto files/audio/. In the future, you can change these values. - Use the ydl_config that is provided to you\n\n# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\n# Check if the output directory exists, if not create it\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n# Print a message indicating which video is being downloaded\nprint(f\"Downloading Video from the url : {youtube_url}\")\n# Attempt to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n[debug] Encodings: locale cp1252, fs utf-8, pref cp1252, out UTF-8 (No VT), error UTF-8 (No VT), screen UTF-8 (No VT)\n[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip) API\n[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set()}\n[debug] Python 3.10.5 (CPython AMD64 64bit) - Windows-10-10.0.19044-SP0 (OpenSSL 1.1.1n  15 Mar 2022)\n[debug] exe versions: ffmpeg 4.2.2, ffprobe 4.2.2\n[debug] Optional libraries: Cryptodome-3.20.0, brotli-1.1.0, certifi-2022.12.07, mutagen-1.47.0, sqlite3-2.6.0, websockets-12.0\n[debug] Proxy map: {}\n[debug] Loaded 1855 extractors\n[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n[debug] Invoking http downloader on \"https://rr4---sn-gwpa-qxaee.googlevideo.com/videoplayback?expire=1708196638&ei=vq7QZde2H4ru4-EPpdWaCA&ip=2409%3A40d0%3A100f%3Aa083%3A490c%3Ab20e%3A7354%3A2994&id=o-AOWVFk6tMKFyCdZnCS5VvP0VT03Sukh6VN-al1WR5Hxf&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zw&mm=31%2C29&mn=sn-gwpa-qxaee%2Csn-gwpa-qxae7&ms=au%2Crdu&mv=m&mvi=4&pl=36&pcm2=yes&initcwndbps=622500&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=10932652&dur=752.701&lmt=1654008313150389&mt=1708174646&fvip=4&keepalive=yes&fexp=24007246&c=ANDROID&txp=5318224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhAKjTizAipxwXaTyCpXR5dhzxLl9eqsBGjOnNHGbf-ausAiAjcteMHs-gfhtd0D1gWI7rDbyBjuufbqO-JqBgIPUJVA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=APTiJQcwRQIhAMSBwIxjqtjt_2MwwBItrQLvQMekr1i4XH49TC2r5sMBAiBF4tDjQT_1sLALeO5lYGTVOcQ6QZKu1GfD2PlrVGG7vw%3D%3D\"\n[debug] File locking is not supported. Proceeding without locking\n[debug] ffmpeg command line: ffprobe -show_streams \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\"\n[debug] ffmpeg command line: ffmpeg -y -loglevel \"repeat+info\" -i \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\" -vn -acodec libmp3lame \"-b:a\" 192.0k -movflags \"+faststart\" \"file:files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\"\n\n\nDownloading Video from the url : https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] aqzxYofJ_ck: Downloading webpage\n[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n[youtube] aqzxYofJ_ck: Downloading android player API JSON\n[youtube] aqzxYofJ_ck: Downloading m3u8 information\n[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n[download] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\n[download]   0.0% of   10.43MiB at   47.62KiB/s ETA 03:44[download]   0.0% of   10.43MiB at  120.00KiB/s ETA 01:28[download]   0.1% of   10.43MiB at  250.00KiB/s ETA 00:42[download]   0.1% of   10.43MiB at  468.77KiB/s ETA 00:22[download]   0.3% of   10.43MiB at  861.04KiB/s ETA 00:12[download]   0.6% of   10.43MiB at  851.41KiB/s ETA 00:12[download]   1.2% of   10.43MiB at    1.20MiB/s ETA 00:08[download]   2.4% of   10.43MiB at    1.28MiB/s ETA 00:07[download]   4.8% of   10.43MiB at    1.09MiB/s ETA 00:09[download]   9.6% of   10.43MiB at    1.09MiB/s ETA 00:08[download]  19.2% of   10.43MiB at    1.19MiB/s ETA 00:07[download]  31.8% of   10.43MiB at    1.31MiB/s ETA 00:05[download]  46.6% of   10.43MiB at    1.36MiB/s ETA 00:04[download]  60.9% of   10.43MiB at    1.41MiB/s ETA 00:02[download]  76.1% of   10.43MiB at    1.40MiB/s ETA 00:01[download]  89.3% of   10.43MiB at    1.46MiB/s ETA 00:00[download]  94.5% of   10.43MiB at    1.47MiB/s ETA 00:00[download]  94.5% of   10.43MiB at   76.60KiB/s ETA 00:07[download]  94.5% of   10.43MiB at  199.39KiB/s ETA 00:02[download]  94.6% of   10.43MiB at  387.87KiB/s ETA 00:01[download]  94.6% of   10.43MiB at  748.19KiB/s ETA 00:00[download]  94.8% of   10.43MiB at    1.16MiB/s ETA 00:00[download]  95.1% of   10.43MiB at    1.01MiB/s ETA 00:00[download]  95.7% of   10.43MiB at    1.24MiB/s ETA 00:00[download]  96.9% of   10.43MiB at    1.39MiB/s ETA 00:00[download]  99.3% of   10.43MiB at    1.64MiB/s ETA 00:00[download] 100.0% of   10.43MiB at    1.67MiB/s ETA 00:00[download] 100% of   10.43MiB in 00:00:07 at 1.39MiB/s   \n[ExtractAudio] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\nDeleting original file files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n\n\nTo find the audio files that we will use the globmodule that looks in the output_dir to find any .mp3 files. Then we will append the file to a list called audio_files. This will be used later to send each file to the Whisper model for transcription.\nCreate the following: - A variable called audio_filesthat uses the glob module to find all matching files with the .mp3 file extension - Select the first first file in the list and assign it to audio_filename - To verify the filename, print audio_filename\n\nFind the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio_file = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename = audio_file[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)\n\nfiles/audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-3-transcribe-the-video-using-whisper",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-3-transcribe-the-video-using-whisper",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 3: Transcribe the Video using Whisper",
    "text": "Task 3: Transcribe the Video using Whisper\nIn this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the audio_file, for the output_file and the model.\nUsing these variables we will: - create a list to store the transcripts - Read the Audio File - Send the file to the Whisper Model using the OpenAI package\nTo complete this step, create the following: - A variable named audio_filethat is assigned the audio_filename we created in the last step - A variable named output_file that is assigned the value \"files/transcripts/transcript.txt\" - A variable named model that is assigned the value \"whisper-1\" - An empty list called transcripts - A variable named audio that uses the open method and \"rb\" modifier on the audio_file - A variable to store the response from the openai.Audio.transcribe method that takes in the modeland audio variables - Append the response[\"text\"]to the transcripts list.\n\nimport openai\n\n# Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Set the API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"Converting Audio to Text.....\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\n# Extract the transcript from the response\ntranscript = response['text']\n\nConverting Audio to Text.....\n\n\nTo save the transcripts to text files we will use the below provided code:\n\n# If an output file is specified, save the transcript to a .txt file\nif output_file is not None:\n    # Create the directory for the output file if it doesn't exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    # Write the transcript to the output file\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\n# Print the transcript to the console to verify it worked \nprint(transcript)\n\nHi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using Datacamp Workspace here, and this is one of the data sets that is available as standard with Datacamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually just one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan-applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan-data.csv. Let me just check and see if I got that correct. loan-data.csv. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit.policy. And so each row is an application. So when the application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And I'm going to take the credit policy column and then I'm going to copy and paste this variable name again so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split these, this, the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this list. Function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out. And we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible, despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train two. And you see, even though it's random, we have exactly the same results. So it's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful. you"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-4-create-a-textloader-using-langchain",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-4-create-a-textloader-using-langchain",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 4: Create a TextLoader using LangChain",
    "text": "Task 4: Create a TextLoader using LangChain\nIn order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the TextLoader that will take the text from our transcript and load it into a document.\nTo complete this step, do the following: - Import TextLoader from langchain.document_loaders - Create a variable called loader that uses the TextLoader method which takes in the directory of the transcripts \"./files/text\" - Create a variable called docs that is assigned the result of calling the loader.load() method.\n\n# Import the TextLoader class from the langchain.document_loaders module\nfrom langchain.document_loaders import TextLoader\n\n# Create a new instance of the TextLoader class, specifying the directory containing the text files\nloader = TextLoader(\"./files/transcripts/transcript.txt\")\n\n# Load the documents from the specified directory using the TextLoader instance\ndocs = loader.load()\n\nC:\\Users\\Chirag Sharma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning:\n\nA newer version of deeplake (3.8.20) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n\n\n\n\n# Show the first element of docs to verify it has been loaded \ndocs[0]\n\nDocument(page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using Datacamp Workspace here, and this is one of the data sets that is available as standard with Datacamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually just one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan-applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan-data.csv. Let me just check and see if I got that correct. loan-data.csv. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit.policy. And so each row is an application. So when the application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And I'm going to take the credit policy column and then I'm going to copy and paste this variable name again so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split these, this, the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this list. Function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out. And we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible, despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train two. And you see, even though it's random, we have exactly the same results. So it's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful. you\", metadata={'source': './files/transcripts/transcript.txt'})"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-4-creating-an-in-memory-vector-store",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-4-creating-an-in-memory-vector-store",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 4: Creating an In-Memory Vector Store",
    "text": "Task 4: Creating an In-Memory Vector Store\nNow that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space.\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the docarray package.\nWe will also tokenize our queries using the tiktoken package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model “understand” the text and relationships with other tokens.\n\nInstructions\n\nImport the tiktoken package.\n\n\n# Import the tiktoken package\nimport tiktoken"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-5-create-the-document-search",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-5-create-the-document-search",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 5: Create the Document Search",
    "text": "Task 5: Create the Document Search\nWe will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing:\n\nImport RetrievalQA from langchain.chains - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents\nImport ChatOpenAI from langchain.chat_models - this imports the ChatOpenAI model that we will use to query the data\nImport DocArrayInMemorySearch from langchain.vectorstores - this gives the ability to search over the vector store we have created.\nImport OpenAIEmbeddings from langchain.embeddings - this will create embeddings for the data store in the vector store.\nImport display and Markdownfrom IPython.display - this will create formatted responses to the queries.\n\n\n# Import the RetrievalQA class from the langchain.chains module\nfrom langchain.chains import RetrievalQA\n# Import the ChatOpenAI class from the langchain.chat_models module\nfrom langchain.chat_models import ChatOpenAI\n# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\nfrom langchain.vectorstores import DocArrayInMemorySearch\n# Import the OpenAIEmbeddings class from the langchain.embeddings module\nfrom langchain.embeddings import OpenAIEmbeddings \n\nNow we will create a vector store that will use the DocArrayInMemory search methods which will search through the created embeddings created by the OpenAI Embeddings function.\nTo complete this step: - Create a variable called db - Assign the db variable to store the result of the method DocArrayInMemorySearch.from_documents - In the DocArrayInMemorySearch method, pass in the docs and a function call to OpenAIEmbeddings()\n\n# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(\n    docs, OpenAIEmbeddings()\n)\n\nWe will now create a retriever from the db we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the ChatOpenAI model, will assigned that as our LLM.\nCreate the following: - A variable called retriever that is assigned db.as_retriever() - A variable called llm that creates the ChatOpenAI method with a set temperatureof 0.0. This will controle the variability in the responses we receive from the LLM.\n\n# Convert the DocArrayInMemorySearch instance to a retriever\nretriever = db.as_retriever()\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")\n\nOur last step before starting to ask questions is to create the RetrievalQA chain. This chain takes in the:\n- The llm we want to use - The chain_type which is how the model retrieves the data - The retriever that we have created - An option called verbose that allows use to see the seperate steps of the chain\nCreate a variable called qa_stuff. This variable will be assigned the method RetrievalQA.from_chain_type.\nUse the following settings inside this method: - llm=llm - chain_type=\"stuff\" - retriever=retriever - verbose=True\n\n# Create a new RetrievalQA instance with the specified parameters\nqa_stuff = RetrievalQA.from_chain_type(\nllm=llm,\nchain_type=\"stuff\",\nretriever=retriever,\nverbose=True,\n)\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation"
  },
  {
    "objectID": "blog/Chat_with_YouTube_Videos/index.html#task-5-create-the-queries",
    "href": "blog/Chat_with_YouTube_Videos/index.html#task-5-create-the-queries",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 5: Create the Queries",
    "text": "Task 5: Create the Queries\nNow we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query.\nTo create the questions to ask the model complete the following steps: - Create a variable call query and assigned it a string value of \"What is this tutorial about?\" - Create a response variable that will store the result of qa_stuff.run(query) - Show the resposnse\n\n# Set the query to be used for the QA system\nquery = \"What is this tutorial about?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThis tutorial is about a data pre-processing technique for machine learning called splitting your data. It focuses on splitting a data set into a training set and a testing set to avoid overfitting and to assess the model's performance on unseen data. The tutorial also covers when to split the data in the machine learning workflow and how to handle categorical variables using one-hot encoding. Additionally, it explains how to adjust the size of the training and testing sets and how to ensure reproducibility by setting a random seed.\n\n\n\n# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThe training set is used to train the machine learning model, meaning the model learns patterns and relationships from this data. The test set, on the other hand, is used to evaluate the performance of the trained model on unseen data. This helps assess how well the model generalizes to new data and gives an indication of its predictive accuracy.\n\n\n\n# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThis lesson on splitting data into training and testing sets is beneficial for individuals who are learning about data pre-processing techniques for machine learning. It is particularly useful for those who are new to machine learning and want to understand the importance of splitting data to avoid overfitting and data leakage. Additionally, individuals who are interested in understanding how to implement train-test-split in Python using libraries like pandas and scikit-learn would find this tutorial helpful.\n\n\n\n# Set the query to be used for the QA system\nquery = \"Who is the greatest football team on earth?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nI don't know the answer to that question as it is subjective and varies depending on personal preferences and opinions.\n\n\n\n# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nI don't know the exact length of the circumference of the Earth.\n\n\n\nThat’s all."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "Retrieval Augmented Generation (RAG) is a framework that combines the power of retrieval-based methods and generation-based methods in natural language processing tasks, particularly in question answering and text generation. It was introduced to address the limitations of both approaches and achieve better performance in understanding and generating natural language text. Here’s an explanation of each component in the RAG framework:\n\nRetrieval Component:\n\nThe retrieval component involves retrieving relevant information from a large corpus of text or a knowledge base. This could be achieved using various techniques such as keyword matching, semantic search, or more sophisticated methods like dense vector retrieval (e.g., using models like Dense Retrieval). The goal is to efficiently find passages or documents that contain relevant information related to the input query or prompt.\n\nAugmentation:\n\nAugmentation refers to enriching the input data or query with additional context or information retrieved from the corpus. This can involve appending relevant passages or documents retrieved from the retrieval component to the input query or prompt. By augmenting the input with retrieved knowledge, the model has access to a broader context, which can improve its understanding and generation capabilities.\n\nGeneration Component:\n\nThe generation component involves generating a response or output based on the augmented input. This can be done using techniques such as sequence-to-sequence models, transformers, or other generative models. The model takes the augmented input, along with any additional context provided, and generates a natural language response or text that addresses the query or prompt.\n\nIntegration:\n\nIntegration refers to the seamless combination of the retrieval and generation components within the framework. This involves designing mechanisms to effectively incorporate retrieved knowledge into the generation process. For instance, retrieved passages can be used as additional input tokens or encoded into a fixed-length vector representation to be used alongside the original input during generation. The integration ensures that the generated output is informed by the relevant information retrieved from the corpus.\n\n\nOverall, the Retrieval Augmented Generation (RAG) framework leverages the strengths of both retrieval-based and generation-based approaches to enhance natural language understanding and generation tasks. By effectively integrating retrieval and generation components and augmenting input with retrieved knowledge, RAG models are capable of producing more accurate and contextually relevant responses compared to traditional generation models.\n\n\n\n\nUnlimited Knowledge: The Retriever of RAG system can have access to external sources of information. Therefore, the LLM is not limited to its internal knowledge. The external sources can be proprietary documents and data or event the Internet.\n\n\nComparision of Without RAG Vs. With RAG:\n\nIn case of without RAG, an LLM has knowledge only of the data it has been originally trained on. Also known as Parametric memory that is the information stored in the model parameters.\nIn case of with RAG, Retriever searches and fetches information that the LLM has not necessarily been trained on. This adds to the LLM memory and is passed as the context in the prompts. Also called as Non-Parametric memory that is information available outside the model parameters. The benefits of this approach are it is easy to expand to all the data sources, it is easier to update/maintain, and it is much cheaper than retraining/fine-tuning.\n\n\n\nConfidence in Response: With the context (extra information that is retrieved, provided by the the retriever component) made available to the LLM, the confidence in LLM response is increased.\n\n\nThe benefits of RAG in increasing the confidence of LLM response are as the following:\n\nContext Awareness: The added information assists LLMs in generating responses that are accurate and contextually appropriate.\nSource Citation: We get the know the sources of information which improves the transparency of the LLM responses.\nReduced Hallucinatios: RAG enabled LLM systems are observed to be less prone to hallucinations than the ones without RAG.\n\n\n\n\n\nRevisit the five high level steps of an RAG enabled system:\n\n\n\nRAG_Architecture.png\n\n\n\nUser writes a prompt or a query that is passed to an orchestrator\nOrchestrator sends a search query to the retriever\nRetriever fetches the relevant information from the knowledge sources and sends back\nOrchestrator augments the prompt with the context and sends to the LLM\nLLM responds with the generated text which is displayed to the user via the orchestrator\n\nTwo pipelines become important in setting up the RAG system. The first one being setting up the knowledge sources for efficient search and retrieval and the second one being the five steps of the generation.\n\nIndexing Pipeline: Data for the knowledge is ingested from the source and indexed. This involves steps like splitting, creation of embeddings and storage of data.\nRAG Pipeline: This involves the actual RAG process which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\n\n\n\nThe index pipeline creates the knowledge source for the RAG system. It is generally considered as an offline process. However, information can also be fetched in real time as well. It involves 4 primary steps:\n\nLoading: This step involves extracting information from different knowledge sources and loading them into documents.\nSplitting: This step involves splitting documents into smaller manageable chunks. Smaller chunks are easier to search and to use in LLM context windows.\nEmbedding: This step involves converting text documents into numerical vector representations. ML models are mathematical models, and therefore require numerical data.\nStoring: This step involves storing the embedding vectors. Vector are typically stored in Vector Databases which are best suited for searching.\n\n\n\n\nNow that we’ve the knowledge base created in the Indexing Pipeline, the main generation, or the RAG pipeline will have to be setup for receiving the input and generating the output.\nGeneration Steps:\n\nUser writes a prompt or a query that is passed to an orchestrator.\nOrchestrator sends a search query to the retriever\nRetriever fetches the relevant information from the knowledge sources and returns\nOrchestrator augments the prompt with the context and sends to the LLM\nLLM responds with the generated text which is displayed to the user via the orchestrator\n\n\n\n\nThe 3 primary steps in a RAG pipeline are:\n\nSearch & Retrieval: This step involves searching for the context from the source (for instance, vector database).\nAugmentation: This step involves adding the context to the prompt depending on the use case.\nGeneration: This step involves generating the final response from an LLM.\n\nAn important consideration is how knowledge is stored and accessed. This has a bearing on the search & retrieval step.\n\nPersistent Vector DBs: When a large volume of data is stored in vector databases, the retrieval and search needs to be quick. The relevance and accuracy of the search can be tested.\nTemporary Vector Index: When the data is temporarily stored in vector indices for one time use, the accuracy and relevance of the search needs to be ascertained.\nSmall Data: Generally, when small amount of data is retrieved from pre-determined external sources, the augmentation of the data becomes more critical."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#what-is-retrieval-augmented-generation-rag",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#what-is-retrieval-augmented-generation-rag",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "Retrieval Augmented Generation (RAG) is a framework that combines the power of retrieval-based methods and generation-based methods in natural language processing tasks, particularly in question answering and text generation. It was introduced to address the limitations of both approaches and achieve better performance in understanding and generating natural language text. Here’s an explanation of each component in the RAG framework:\n\nRetrieval Component:\n\nThe retrieval component involves retrieving relevant information from a large corpus of text or a knowledge base. This could be achieved using various techniques such as keyword matching, semantic search, or more sophisticated methods like dense vector retrieval (e.g., using models like Dense Retrieval). The goal is to efficiently find passages or documents that contain relevant information related to the input query or prompt.\n\nAugmentation:\n\nAugmentation refers to enriching the input data or query with additional context or information retrieved from the corpus. This can involve appending relevant passages or documents retrieved from the retrieval component to the input query or prompt. By augmenting the input with retrieved knowledge, the model has access to a broader context, which can improve its understanding and generation capabilities.\n\nGeneration Component:\n\nThe generation component involves generating a response or output based on the augmented input. This can be done using techniques such as sequence-to-sequence models, transformers, or other generative models. The model takes the augmented input, along with any additional context provided, and generates a natural language response or text that addresses the query or prompt.\n\nIntegration:\n\nIntegration refers to the seamless combination of the retrieval and generation components within the framework. This involves designing mechanisms to effectively incorporate retrieved knowledge into the generation process. For instance, retrieved passages can be used as additional input tokens or encoded into a fixed-length vector representation to be used alongside the original input during generation. The integration ensures that the generated output is informed by the relevant information retrieved from the corpus.\n\n\nOverall, the Retrieval Augmented Generation (RAG) framework leverages the strengths of both retrieval-based and generation-based approaches to enhance natural language understanding and generation tasks. By effectively integrating retrieval and generation components and augmenting input with retrieved knowledge, RAG models are capable of producing more accurate and contextually relevant responses compared to traditional generation models."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#how-does-rag-helps",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#how-does-rag-helps",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "Unlimited Knowledge: The Retriever of RAG system can have access to external sources of information. Therefore, the LLM is not limited to its internal knowledge. The external sources can be proprietary documents and data or event the Internet.\n\n\nComparision of Without RAG Vs. With RAG:\n\nIn case of without RAG, an LLM has knowledge only of the data it has been originally trained on. Also known as Parametric memory that is the information stored in the model parameters.\nIn case of with RAG, Retriever searches and fetches information that the LLM has not necessarily been trained on. This adds to the LLM memory and is passed as the context in the prompts. Also called as Non-Parametric memory that is information available outside the model parameters. The benefits of this approach are it is easy to expand to all the data sources, it is easier to update/maintain, and it is much cheaper than retraining/fine-tuning.\n\n\n\nConfidence in Response: With the context (extra information that is retrieved, provided by the the retriever component) made available to the LLM, the confidence in LLM response is increased.\n\n\nThe benefits of RAG in increasing the confidence of LLM response are as the following:\n\nContext Awareness: The added information assists LLMs in generating responses that are accurate and contextually appropriate.\nSource Citation: We get the know the sources of information which improves the transparency of the LLM responses.\nReduced Hallucinatios: RAG enabled LLM systems are observed to be less prone to hallucinations than the ones without RAG."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-architecture",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-architecture",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "Revisit the five high level steps of an RAG enabled system:\n\n\n\nRAG_Architecture.png\n\n\n\nUser writes a prompt or a query that is passed to an orchestrator\nOrchestrator sends a search query to the retriever\nRetriever fetches the relevant information from the knowledge sources and sends back\nOrchestrator augments the prompt with the context and sends to the LLM\nLLM responds with the generated text which is displayed to the user via the orchestrator\n\nTwo pipelines become important in setting up the RAG system. The first one being setting up the knowledge sources for efficient search and retrieval and the second one being the five steps of the generation.\n\nIndexing Pipeline: Data for the knowledge is ingested from the source and indexed. This involves steps like splitting, creation of embeddings and storage of data.\nRAG Pipeline: This involves the actual RAG process which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#indexing-pipeline",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#indexing-pipeline",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "The index pipeline creates the knowledge source for the RAG system. It is generally considered as an offline process. However, information can also be fetched in real time as well. It involves 4 primary steps:\n\nLoading: This step involves extracting information from different knowledge sources and loading them into documents.\nSplitting: This step involves splitting documents into smaller manageable chunks. Smaller chunks are easier to search and to use in LLM context windows.\nEmbedding: This step involves converting text documents into numerical vector representations. ML models are mathematical models, and therefore require numerical data.\nStoring: This step involves storing the embedding vectors. Vector are typically stored in Vector Databases which are best suited for searching."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-pipeline",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-pipeline",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "Now that we’ve the knowledge base created in the Indexing Pipeline, the main generation, or the RAG pipeline will have to be setup for receiving the input and generating the output.\nGeneration Steps:\n\nUser writes a prompt or a query that is passed to an orchestrator.\nOrchestrator sends a search query to the retriever\nRetriever fetches the relevant information from the knowledge sources and returns\nOrchestrator augments the prompt with the context and sends to the LLM\nLLM responds with the generated text which is displayed to the user via the orchestrator"
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-pipeline-steps",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#rag-pipeline-steps",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "",
    "text": "The 3 primary steps in a RAG pipeline are:\n\nSearch & Retrieval: This step involves searching for the context from the source (for instance, vector database).\nAugmentation: This step involves adding the context to the prompt depending on the use case.\nGeneration: This step involves generating the final response from an LLM.\n\nAn important consideration is how knowledge is stored and accessed. This has a bearing on the search & retrieval step.\n\nPersistent Vector DBs: When a large volume of data is stored in vector databases, the retrieval and search needs to be quick. The relevance and accuracy of the search can be tested.\nTemporary Vector Index: When the data is temporarily stored in vector indices for one time use, the accuracy and relevance of the search needs to be ascertained.\nSmall Data: Generally, when small amount of data is retrieved from pre-determined external sources, the augmentation of the data becomes more critical."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#installing-and-loading-necessary-libraries",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#installing-and-loading-necessary-libraries",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "2.1 Installing and Loading Necessary Libraries",
    "text": "2.1 Installing and Loading Necessary Libraries\n\n!pip3 install python-dotenv openai==0.28 langchain==0.0.226 pydantic==1.10.11 --quiet\n!pip3 install yt_dlp pydub pypdf chromadb==0.4.0 --quiet\n!pip3 install unstructured selenium tiktoken --quiet\n!pip3 install sentence_transformers===2.2.2 --quiet\n\n\nimport os\nimport sys\nimport nltk\nimport openai\nimport numpy as np\nnltk.download('punkt')\nsys.path.append(\"../..\")\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nimport chromadb\nfrom chromadb.config import Settings\n\n################## Data Loaders ##################\n## PDF Files Loader\nfrom langchain.document_loaders import PyPDFLoader\n## YouTube \nfrom langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n## URL Loader\nfrom langchain.document_loaders import WebBaseLoader\n## Text File Loader\nfrom langchain.document_loaders import TextLoader\n## Loader for loading HTML documents from a list of URLs requiring JavaScript rendering. \nfrom langchain.document_loaders import SeleniumURLLoader\n## Loader to load/import data from Google Drive \nfrom langchain.document_loaders import GoogleDriveLoader\n\n################## Document Splitting ##################\nfrom langchain.text_splitter import (\n    CharacterTextSplitter,\n    RecursiveCharacterTextSplitter,\n    SpacyTextSplitter,\n    NLTKTextSplitter,\n    MarkdownTextSplitter,\n    MarkdownHeaderTextSplitter,\n    TokenTextSplitter\n)\n\n################## VectorStores & Embeddings ##################\n## OpenAI Embeddings\nfrom langchain.embeddings import OpenAIEmbeddings\n## ChromaDB VectorStore\nfrom langchain.vectorstores import Chroma\n\n################## LLMs and Retrievers ##################\n## LLM\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n## \nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n## \nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n## Prompt Templates\nfrom langchain.prompts import PromptTemplate\n## Conversation Memory\nfrom langchain.memory import ConversationBufferMemory\n## ConversationalRetrievalChain\nfrom langchain.chains import ConversationalRetrievalChain\n## cosine similarity function\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom langchain_core.documents import Document\n\n[nltk_data] Downloading package punkt to C:\\Users\\Chirag\n[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!"
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#building-vectorstore-and-retriever",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#building-vectorstore-and-retriever",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "2.2 Building VectorStore and Retriever",
    "text": "2.2 Building VectorStore and Retriever\n\n# directory to store vector database\npersist_directory = \"docs/chroma\"\n# PDF Loader\nloader = PyPDFLoader(\"Introducing MLOps How to Scale Machine Learning in the Enterprise (Mark Treveil, Nicolas Omont, Clément Stenac etc.) (z-lib.org).pdf\")\ndocuments = loader.load_and_split()\n\n# Converting documents into chunks\nrecursive_character_text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, \n    chunk_overlap=150,\n    length_function=len\n)\nchunks = recursive_character_text_splitter.split_documents(documents)\n\n# Embedding function\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\nclient = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=True))\n\n# ChromaDB VectorStore\nvectordb = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    client=client\n)\n\n# Defining the retriever\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n\n# Defining the memory\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=True)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=retriever,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)"
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#asking-question-to-our-bot",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#asking-question-to-our-bot",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "2.3 Asking Question to our Bot",
    "text": "2.3 Asking Question to our Bot\n\nresult = qa({\"question\": \"What does streamlining the machine learning lifecycle means?\"})\nresult['answer']\n\n'Streamlining the machine learning lifecycle means standardizing and optimizing the management process of developing, deploying, and maintaining machine learning models. It involves creating efficient workflows, automating tasks, improving communication and collaboration between different teams involved in the process, and ensuring that the models align with business goals and expectations.'\n\n\n\nresult = qa({\"question\": \"What is the difference between MLOps, AIOps, and ModelOps?\" + \n             \"Make sure to provide a very in-depth answer as well as comparision of each.\"})\nprint(result['answer'])\n\nMLOps, ModelOps, and AIOps are related concepts but have distinct focuses:\n\n1. MLOps (Machine Learning Operations): MLOps is specifically concerned with the operationalization of machine learning models. It involves the entire lifecycle of machine learning models, including development, deployment, monitoring, and maintenance. MLOps aims to streamline the process of deploying machine learning models into production efficiently and effectively.\n\n2. ModelOps: ModelOps is a broader concept that encompasses not only machine learning models but any kind of model, including rule-based models. While MLOps focuses on the operational aspects of machine learning models, ModelOps extends this to cover the operationalization of various types of models used in data science and AI projects.\n\n3. AIOps (Artificial Intelligence for IT Operations): AIOps is distinct from MLOps and ModelOps as it focuses on using artificial intelligence techniques to solve operational challenges in IT environments. AIOps leverages AI to enhance IT operations, automate tasks, and improve system performance. An example of AIOps is predictive maintenance for network failures.\n\nIn summary, MLOps is specifically tailored for machine learning model operations, ModelOps is a broader concept covering operationalization of various models, and AIOps focuses on using AI for IT operations. Each of these disciplines plays a crucial role in different aspects of data science, machine learning, and AI initiatives."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#document-loading-streamlined-data-ingestion",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#document-loading-streamlined-data-ingestion",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "3.1 Document Loading: Streamlined Data Ingestion",
    "text": "3.1 Document Loading: Streamlined Data Ingestion\nDocument Loaders deal with the specifics of accessing and converting data into from a variety of formats and different sources to a standardized format. The data can come from many sources like WebSites, DataBases, YouTube, arXiv etc. and these documents can come into different data types such as PDF file(s), HTML format, JSON Object(s), etc.\nThe whole purpose of these data loaders is to take this variety of data sources and load them into a standard document object which consists of content and associated metadata.\nWe’ll cover some of the document loaders only that are availabe in Langchain.\n\n3.1.1 TextLoader\nThe TextLoader handles plain text files.\n\n## Creating a sample text file\n\ntext = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\nit says will help businesses “generate text, images, code, videos, audio, and more from\nsimple natural language prompts.”\n\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\nexample, or you could use it for tasks like summarizing text or even writing code.\n(It’s similar to features Google also announced today for its Workspace apps like Google\nDocs and Gmail.)\n\"\"\"\n\n## Writing the above text into a .txt file\nwith open(\"my_file.txt\" , \"w\") as file:\n    file.write(text)\n    \n## Using the TextLoader to load the above created file\nloader = TextLoader(\"my_file.txt\")\ndocuments = loader.load()\n\nprint(f\"Number of documents in the text file: {len(documents)}\\n\")\nprint(f\"Sample Output:\\n\\n{documents}\")\n\nNumber of documents in the text file: 1\n\nSample Output:\n\n[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': 'my_file.txt'})]\n\n\n\n\n3.1.2 PyPDFLoader\nThe LangChain library provides two methods for loading and processing PDF files: PyPDFLoader and PDFMinerLoader. We mainly focus on the former, which is used to load PDF files into an array of documents, where each document contains the page content and metadata with the page number.\n\n## Sample Code\nloader = PyPDFLoader(\"Introducing MLOps How to Scale Machine Learning in the Enterprise (Mark Treveil, Nicolas Omont, Clément Stenac etc.) (z-lib.org).pdf\")\ndocuments = loader.load_and_split()\n\nprint(f\"Number of documents in the PDF file: {len(documents)}\\n\")\nprint(f\"Sample Output of 1st document:\\n\\n{documents[10].page_content[:500]}\")\n\nNumber of documents in the PDF file: 176\n\nSample Output of 1st document:\n\nPreface\nWe’ve reached a turning point in the story of machine learning where the technology\nhas moved from the realm of theory and academics and into the “real world”—that is,\nbusinesses providing all kinds of services and products to people across the globe.\nWhile this shift is exciting, it’s also challenging, as it combines the complexities of\nmachine learning models with the complexities of the modern organization.\nOne difficulty, as organizations move from experimenting with machine learning\n\n\n\n\n3.1.3 SeleniumURLLoader (URL)\nSeleniumURLLoader is designed for loading HTML documents from URLs that require JavaScript rendering.\n\n## Sample Code\nurls = [\n    \"https://www.youtube.com/watch?v=TFa539R09EQ&t=139s\",\n    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n]\n\nloader = SeleniumURLLoader(urls=urls)\ndocuments = loader.load()\n\nprint(f\"Number of documents : {len(documents)}\\n\")\nprint(f\"Output :\\n\\n{documents}\")\n\nNumber of documents : 2\n\nOutput :\n\n[Document(page_content=\"OPENASSISTANT TAKES ON CHATGPT!\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nWatch Later\\n\\nShare\\n\\nCopy link\\n\\nTap to unmute\\n\\n2x\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nUp next\\n\\nLiveUpcoming\\n\\nPlay now\\n\\nMachine Learning Street Talk\\n\\nSubscribe\\n\\nSubscribed\\n\\nYou're signed out\\n\\nVideos that you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nShare\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\n2:19\\n\\n2:19 / 59:51\\n\\nWatch full video\\n\\n•\\n\\nScroll for details\\n\\nNaN / NaN\\n\\nNaN / NaN\\n\\nSearch\", metadata={'source': 'https://www.youtube.com/watch?v=TFa539R09EQ&t=139s'}), Document(page_content=\"TAJ HOUSE BOAT  6 SEC  HINDI\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nWatch Later\\n\\nShare\\n\\nCopy link\\n\\nTap to unmute\\n\\n2x\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nYou're signed out\\n\\nVideos that you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nShare\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\n0:00\\n\\n0:00 / 0:06\\n\\nWatch full video\\n\\n•\\n\\nScroll for details\\n\\nAbout\\n\\nPress\\n\\nCopyright\\n\\nContact us\\n\\nCreator\\n\\nAdvertise\\n\\nDevelopers\\n\\nTerms\\n\\nPrivacy\\n\\nPolicy & Safety\\n\\nHow YouTube works\\n\\nTest new features\\n\\n© 2024 Google LLC\", metadata={'source': 'https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s'})]\n\n\n\n\n3.1.4 Google Drive loader\n\nThe LangChain Google Drive Loader efficiently imports data from Google Drive by using the GoogleDriveLoader class. It can fetch data from a list of Google Docs document IDs or a single folder ID.\nPrepare necessary credentials and tokens:\n\nBy default, the GoogleDriveLoader searches for the credentials.json file in ~/.credentials/credentials.json. Use the credentials_file keyword argument to modify this path.\nThe token.json file follows the same principle and will be created automatically upon the loader’s first use.\n\n\n\nTo set up the credentials_file, follow these steps:\n1) Create a new Google Cloud Platform project or use an existing one by visiting the Google Cloud Console. Ensure that billing is enabled for your project.\n2) Enable the Google Drive API by navigating to its dashboard in the Google Cloud Console and clicking \"Enable.\"\n3) Create a service account by going to the Service Accounts page in the Google Cloud Console. Follow the prompts to set up a new service account.\n4) Assign necessary roles to the service account, such as \"Google Drive API - Drive File Access\" and \"Google Drive API - Drive Metadata Read/Write Access,\" depending on your needs.\n5) After creating the service account, access the \"Actions\" menu next to it, select \"Manage keys,\" click \"Add Key,\" and choose \"JSON\" as the key type. This generates a JSON key file and downloads it to your computer, which serves as your credentials_file.\n\nRetrieve the folder or document ID from the URL:\n\nFolder: https://drive.google.com/drive/u/0/folders/{folder_id}\nDocument: https://docs.google.com/document/d/{document_id}/edit\n\nImport the GoogleDriveLoader class:\n\n\nloader = GoogleDriveLoader(\n    folder_id=\"your_folder_id\",\n    recursive=False,\n)\ndocuments = loader.load()\n# - Note that currently, only Google Docs are supported.\n\n\n\n\n3.1.5 WebBaseLoader\nIt loads all text from HTML webpages into a document format.\n\nloader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")\ndocuments = loader.load()\n\nprint(f\"Number of documents : {len(documents)}\\n\")\nprint(f\"Output :\\n\\n{documents[:50]}\")\n\nNumber of documents : 1\n\nOutput :\n\n[Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFile not found · GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Sign in\\n        \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nPackages\\n        Host and manage packages\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nCopilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\nFor\\n\\n\\n\\n      Enterprise\\n\\n    \\n\\n\\n\\n      Teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Education\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy Solution\\n\\n\\n\\n      CI/CD & Automation\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n              Search syntax tips\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n              Sign in\\n            \\n\\n\\n                  Sign in to GitHub\\n\\n \\n      Username or email address\\n    \\n\\n\\n\\n      Password\\n    \\n\\nForgot password?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n \\nor sign in with a passkey\\n\\n\\n\\n\\n \\n\\n              Sign up\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        basecamp\\n \\n/\\n\\nhandbook\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n\\n\\n\\n \\n\\nFork\\n    740\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 6.2k\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nIssues\\n1\\n\\n\\n\\n\\n\\n\\nPull requests\\n0\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Issues\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2024 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://github.com/basecamp/handbook/blob/master/37signals-is-you.md', 'title': 'File not found · GitHub', 'description': 'Basecamp Employee Handbook. Contribute to basecamp/handbook development by creating an account on GitHub.', 'language': 'en'})]\n\n\n\n\n3.1.6 YouTube\n\nurl=\"https://www.youtube.com/watch?v=zjkBMFhNj_g&t=9s\"\nsave_dir=\"docs/youtube/\"\nloader = GenericLoader(\n    YoutubeAudioLoader([url],save_dir),\n    OpenAIWhisperParser()\n)\ndocuments = loader.load()\ndocument = documents[0]\n\nprint(f\"Number of documents in the PDF file: {len(documents)}\\n\")\nprint(f\"Sample Output of 1st document:\\n\\n{document}\")\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=zjkBMFhNj_g&t=9s\n[youtube] zjkBMFhNj_g: Downloading webpage\n[youtube] zjkBMFhNj_g: Downloading ios player API JSON\n[youtube] zjkBMFhNj_g: Downloading android player API JSON\n[youtube] zjkBMFhNj_g: Downloading m3u8 information\n[info] zjkBMFhNj_g: Downloading 1 format(s): 140\n[download] Destination: docs\\youtube\\[1hr Talk] Intro to Large Language Models.m4a\n[download] 100% of   55.37MiB in 00:00:12 at 4.30MiB/s     \n[FixupM4a] Correcting container of \"docs\\youtube\\[1hr Talk] Intro to Large Language Models.m4a\"\n[ExtractAudio] Not converting audio docs\\youtube\\[1hr Talk] Intro to Large Language Models.m4a; file is already in target format m4a\nTranscribing part 1!\nTranscribing part 2!\nTranscribing part 3!\nNumber of documents in the PDF file: 3\n\nSample Output of 1st document:\n\npage_content=\"Hi everyone. So recently I gave a 30-minute talk on large language models, just kind of like an intro talk. Unfortunately that talk was not recorded, but a lot of people came to me after the talk and they told me that they really liked the talk, so I thought I would just re-record it and basically put it up on YouTube. So here we go, the busy person's intro to large language models, Director Scott. Okay, so let's begin. First of all, what is a large language model really? Well, a large language model is just two files, right? There will be two files in this hypothetical directory. So, for example, working with the specific example of the LLAMA2 70b model, this is a large language model released by Meta.ai, and this is basically the LLAMA series of language models, the second iteration of it, and this is the 70 billion parameter model of this series. So there's multiple models belonging to the LLAMA2 series, 7 billion, 13 billion, 34 billion, and 70 billion is the biggest one. Now many people like this model specifically because it is probably today the most powerful open weights model. So basically the weights and the architecture and a paper was all released by Meta, so anyone can work with this model very easily by themselves. This is unlike many other language models that you might be familiar with. For example, if you're using ChatsGPT or something like that, the model architecture was never released. It is owned by OpenAI, and you're allowed to use the language model through a web interface, but you don't have actually access to that model. So in this case, the LLAMA2 70b model is really just two files on your file system, the parameters file and the run some kind of a code that runs those parameters. So the parameters are basically the weights or the parameters of this neural network that is the language model. We'll go into that in a bit. Because this is a 70 billion parameter model, every one of those parameters is stored as two bytes, and so therefore the parameters file here is 140 gigabytes, and it's two bytes because this is a float 16 number as the data type. Now in addition to these parameters, that's just like a large list of parameters for that neural network. You also need something that runs that neural network, and this piece of code is implemented in our run file. Now this could be a C file, or a Python file, or any other programming language really. It can be written any arbitrary language, but C is sort of like a very simple language just to give you a sense, and it would only require about 500 lines of C with no other dependencies to implement the neural network architecture, and that uses basically the parameters to run the model. So it's only these two files. You can take these two files, and you can take your MacBook, and this is a fully self-contained package. This is everything that's necessary. You don't need any connectivity to the internet or anything else. You can take these two files, you compile your C code, you get a binary that you can point at the parameters, and you can talk to this language model. So for example, you can send it text, like for example, write a poem about the company Scale.ai, and this language model will start generating text, and in this case it will follow the directions and give you a poem about Scale.ai. Now the reason that I'm picking on Scale.ai here, and you're going to see that throughout the talk, is because the event that I originally presented this talk with was run by Scale.ai, and so I'm picking on them throughout the slides a little bit, just in an effort to make it concrete. So this is how we can run the model. Just requires two files, just requires a MacBook. I'm slightly cheating here because this was not actually, in terms of the speed of this video here, this was not running a 70 billion parameter model, it was only running a 7 billion parameter model. A 70B would be running about 10 times slower, but I wanted to give you an idea of sort of just the text generation and what that looks like. So not a lot is necessary to run the model. This is a very small package, but the computational complexity really comes in when we'd like to get those parameters. So how do we get the parameters, and where are they from? Because whatever is in the run.c file, the neural network architecture, and sort of the forward pass of that network, everything is algorithmically understood and open and so on. But the magic really is in the parameters, and how do we obtain them? So to obtain the parameters, basically the model training, as we call it, is a lot more involved than model inference, which is the part that I showed you earlier. So model inference is just running it on your MacBook. Model training is a competitionally very involved process. So basically what we're doing can best be sort of understood as kind of a compression of a good chunk of internet. So because Lama270B is an open source model, we know quite a bit about how it was trained, because Meta released that information in paper. So these are some of the numbers of what's involved. You basically take a chunk of the internet that is roughly, you should be thinking, 10 terabytes of text. This typically comes from like a crawl of the internet. So just imagine just collecting tons of text from all kinds of different websites and collecting it together. So you take a large chunk of internet, then you procure a GPU cluster. And these are very specialized computers intended for very heavy computational workloads like training of neural networks. You need about 6,000 GPUs, and you would run this for about 12 days to get a Lama270B. And this would cost you about $2 million. And what this is doing is basically it is compressing this large chunk of text into what you can think of as a kind of a zip file. So these parameters that I showed you in an earlier slide are best thought of as like a zip file of the internet. And in this case, what would come out are these parameters, 140 gigabytes. So you can see that the compression ratio here is roughly like 100x, roughly speaking. But this is not exactly a zip file, because a zip file is lossless compression. What's happening here is a lossy compression. We're just kind of like getting a kind of a gestalt of the text that we trained on. We don't have an identical copy of it in these parameters. And so it's kind of like a lossy compression. You can think about it that way. The one more thing to point out here is these numbers here are actually, by today's standards, in terms of state-of-the-art, rookie numbers. So if you want to think about state-of-the-art neural networks, like, say, what you might use in ChatGPT, or Clod, or BARD, or something like that, these numbers are off by a factor of 10 or more. So you would just go in, and you would just like start multiplying by quite a bit more. And that's why these training runs today are many tens, or even potentially hundreds of millions of dollars, very large clusters, very large data sets. And this process here is very involved to get those parameters. Once you have those parameters, running the neural network is fairly computationally cheap. Okay, so what is this neural network really doing, right? I mentioned that there are these parameters. This neural network basically is just trying to predict the next word in a sequence. You can think about it that way. So you can feed in a sequence of words, for example, cat sat on A. This feeds into a neural net, and these parameters are dispersed throughout this neural network. And there's neurons, and they're connected to each other, and they all fire in a certain way. You can think about it that way. And out comes a prediction for what word comes next. So for example, in this case, this neural network might predict that in this context of four words, the next word will probably be a mat with, say, 97% probability. So this is fundamentally the problem that the neural network is performing. And you can show mathematically that there's a very close relationship between prediction and compression, which is why I sort of allude to this neural network as kind of training it as kind of like a compression of the internet, because if you can predict sort of the next word very accurately, you can use that to compress the data set. So it's just a next word prediction neural network. You give it some words, it gives you the next word. Now, the reason that what you get out of the training is actually quite a magical artifact is that basically the next word prediction task you might think is a very simple objective, but it's actually a pretty powerful objective, because it forces you to learn a lot about the world inside the parameters of the neural network. So here I took a random webpage at the time when I was making this talk. I just grabbed it from the main page of Wikipedia, and it was about Ruth Handler. And so think about being the neural network, and you're given some amount of words and trying to predict the next word in a sequence. Well, in this case, I'm highlighting here in red some of the words that would contain a lot of information. And so, for example, if your objective is to predict the next word, presumably your parameters have to learn a lot of this knowledge. You have to know about Ruth and Handler and when she was born and when she died, who she was, what she's done, and so on. And so in the task of next word prediction, you're learning a ton about the world, and all this knowledge is being compressed into the weights, the parameters. Now, how do we actually use these neural networks? Well, once we've trained them, I showed you that the model inference is a very simple process. We basically generate what comes next. We sample from the model, so we pick a word, and then we continue feeding it back in and get the next word and continue feeding that back in. So we can iterate this process, and this network then dreams internet documents. So, for example, if we just run the neural network, or as we say, perform inference, we would get sort of like web page dreams. You can almost think about it that way, right? Because this network was trained on web pages, and then you can sort of like let it loose. So on the left, we have some kind of a Java code dream, it looks like. In the middle, we have some kind of what looks like almost like an Amazon product dream. And on the right, we have something that almost looks like a Wikipedia article. Focusing for a bit on the middle one as an example, the title, the author, the ISBN number, everything else, this is all just totally made up by the network. The network is dreaming text from the distribution that it was trained on. It's mimicking these documents. But this is all kind of like hallucinated. So, for example, the ISBN number, this number probably, I would guess, almost certainly does not exist. The model network just knows that what comes after ISBN colon is some kind of a number of roughly this length, and it's got all these digits. And it just like puts it in. It just kind of like puts in whatever looks reasonable. So it's parroting the training dataset distribution. On the right, the black nose dace, I looked it up, and it is actually a kind of fish. And what's happening here is this text verbatim is not found in the training set documents. But this information, if you actually look it up, is actually roughly correct with respect to this fish. And so the network has knowledge about this fish. It knows a lot about this fish. It's not going to exactly parrot documents that it saw in the training set. But again, it's some kind of a lossy compression of the internet. It kind of remembers the gestalt. It kind of knows the knowledge, and it just kind of like goes, and it creates the form. It creates kind of like the correct form and fills it with some of its knowledge. And you're never 100% sure if what it comes up with is, as we call, hallucination, or like an incorrect answer, or like a correct answer necessarily. So some of this stuff could be memorized, and some of it is not memorized, and you don't exactly know which is which. But for the most part, this is just kind of like hallucinating or like dreaming internet text from its data distribution. Okay, let's now switch gears to how does this network work? How does it actually perform this next word prediction task? What goes on inside it? Well, this is where things complicate a little bit. This is kind of like the schematic diagram of the neural network. If we kind of like zoom in into the toy diagram of this neural net, this is what we call the transformer neural network architecture, and this is kind of like a diagram of it. Now, what's remarkable about this neural net is we actually understand in full detail the architecture. We know exactly what mathematical operations happen at all the different stages of it. The problem is that these 100 billion parameters are dispersed throughout the entire neural network. And so basically, these billions of parameters are throughout the neural net. And all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task. So we know how to optimize these parameters. We know how to adjust them over time to get a better next word prediction. But we don't actually really know what these 100 billion parameters are doing. We can measure that it's getting better at the next word prediction, but we don't know how these parameters collaborate to actually perform that. We have some kind of models that you can try to think through on a high level for what the network might be doing. So we kind of understand that they build and maintain some kind of a knowledge database. But even this knowledge database is very strange and imperfect and weird. So a recent viral example is what we call the reversal course. So as an example, if you go to chat GPT and you talk to GPT-4, the best language model currently available, you say, who is Tom Cruise's mother? It will tell you it's Mary Lee Pfeiffer, which is correct. But if you say, who is Mary Lee Pfeiffer's son? It will tell you it doesn't know. So this knowledge is weird and it's kind of one dimensional. And you have to sort of like, this knowledge isn't just like stored and can be accessed in all the different ways. You have to sort of like ask it from a certain direction almost. And so that's really weird and strange. And fundamentally, we don't really know because all you can kind of measure is whether it works or not and with what probability. So long story short, think of LLMs as kind of like mostly inscrutable artifacts. They're not similar to anything else you might build in an engineering discipline. They're not like a car where we sort of understand all the parts. They're these neural nets that come from a long process of optimization. And so we don't currently understand exactly how they work, although there's a field called interpretability or mechanistic interpretability, trying to kind of go in and try to figure out what all the parts of this neural net are doing. And you can do that to some extent, but not fully right now. But right now, we kind of treat them mostly as empirical artifacts. We can give them some inputs and we can measure the outputs. We can basically measure their behavior. We can look at the text that they generate in many different situations. And so I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical. So now let's go to how we actually obtain an assistant. So far, we've only talked about these internet document generators, right? And so that's the first stage of training. We call that stage pre-training. We're now moving to the second stage of training, which we call fine-tuning. And this is where we obtain what we call an assistant model because we don't actually really just want document generators. That's not very helpful for many tasks. We want to give questions to something and we want it to generate answers based on those questions. So we really want an assistant model instead. And the way you obtain these assistant models is fundamentally through the following process. We basically keep the optimization identical, so the training will be the same. It's just a next word prediction task. But we're going to swap out the dataset on which we are training. So it used to be that we are trying to train on internet documents. We're going to now swap it out for datasets that we collect manually. And the way we collect them using lots of people. So typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them. So here's an example of a single example that might basically make it into your training set. So there's a user and it says something like, can you write a short introduction about the relevance of the term monopsony in economics and so on. And then there's assistant. And again, the person fills in what the ideal response should be. And the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people. And the engineers at a company like OpenAI or Anthropic or whatever else will come up with these labeling documentations. Now, the pre-training stage is about a large quantity of text, but potentially low quality because it just comes from the internet and there's tens of or hundreds of terabytes of it and it's not all very high quality. But in this second stage, we prefer quality over quantity. So we may have many fewer documents, for example, 100,000, but all of these documents now are conversations and they should be very high quality conversations and fundamentally people create them based on labeling instructions. So we swap out the dataset now and we train on these Q&A documents. And this process is called fine tuning. Once you do this, you obtain what we call an assistant model. So this assistant model now subscribes to the form of its new training documents. So for example, if you give it a question like, can you help me with this code? It seems like there's a bug. Print hello world. Even though this question specifically was not part of the training set, the model after it's fine tuning understands that it should answer in the style of a helpful assistant to these kinds of questions. And it will do that. So it will sample word by word again, from left to right, from top to bottom, all these words that are the response to this query. And so it's kind of remarkable and also kind of empirical and not fully understood that these models are able to sort of like change their formatting into now being helpful assistants because they've seen so many documents of it in the fine tuning stage, but they're still able to access and somehow utilize all of the knowledge that was built up during the first stage, the pre-training stage. So roughly speaking, pre-training stage is trains on a ton of internet and it's about knowledge. And the fine tuning stage is about what we call alignment. It's about sort of giving, it's about changing the formatting from internet documents to question and answer documents in kind of like a helpful assistant manner. So roughly speaking, here are the two major parts of obtaining something like chatGPT. There's the stage one pre-training and stage two fine tuning. In the pre-training stage, you get a ton of text from the internet. You need a cluster of GPUs. So these are special purpose sort of computers for these kinds of parallel processing workloads. This is not just things that you can buy and best buy. These are very expensive computers. And then you compress the text into this neural network, into the parameters of it. Typically, this could be a few sort of millions of dollars. And then this gives you the base model. Because this is a very computationally expensive part, this only happens inside companies maybe once a year or once after multiple months, because this is kind of like very expensive to actually perform. Once you have the base model, you enter the fine tuning stage, which is computationally a lot cheaper. In this stage, you write out some labeling instructions that basically specify how your assistant should behave. Then you hire people. So for example, Scale.ai is a company that actually would work with you to actually basically create documents according to your labeling instructions. You collect 100,000, as an example, high quality, ideal Q&A responses. And then you would fine tune the base model on this data. This is a lot cheaper. This would only potentially take like one day or something like that, instead of a few months or something like that. And you obtain what we call an assistant model. Then you run a lot of evaluations, you deploy this, and you monitor, collect misbehaviors. And for every misbehavior, you want to fix it. And you go to step on and repeat. And the way you fix the misbehaviors, roughly speaking, is you have some kind of a conversation where the assistant gave an incorrect response. So you take that, and you ask a person to fill in the correct response. And so the person overwrites the response with the correct one. And this is then inserted as an example into your training data. And the next time you do the fine tuning stage, the model will improve in that situation.\" metadata={'source': 'docs\\\\youtube\\\\[1hr Talk] Intro to Large Language Models.m4a', 'chunk': 0}\n\n\n\n## Storing all the documents in the list\ndocument_list = []\nfor document in documents:\n    document_list.append(document.page_content)\n    \n## Combining all the documents into one document\ndocument_list = \" \".join(document_list)\n\n## Writing all the component in a file\nwith open(\"IntroToLLM_AndrejKarpathy.txt\", 'w') as file:\n    file.write(document_list)"
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#document-splitting",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#document-splitting",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "3.2 Document Splitting",
    "text": "3.2 Document Splitting\nDocument splitting happends after we’ve loaded the data. Now, we are going to talk about how to split the documents into smaller chunks. Document splitting should be done in such a way that it retains the meaningful relationships between the adjacent chunks.\n\n3.2.1 What are Text Splitters and Why they are useful?\n\nLarge Language Models, while recognized for creating human-like text, can also “hallucinate” and produce seemingly plausible yet incorrect or nonsensical information. Interestingly, this tendency can be advantageous in creative tasks, as it generates a range of unique and imaginative ideas, sparking new perspectives and driving the creative process. However, this poses a challenge in situations where accuracy is critical, such as code reviews, insurance-related tasks, or research question responses.\nOne approach to mitigating hallucination is to provide documents as sources of information to the LLM and ask it to generate an answer based on the knowledge extracted from the document. This can help reduce the likelihood of hallucination, and users can verify the information with the source document.\n\nLet’s discuss the pros and cons of this approach:\nPros:\n- Reduced hallucination: By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n- Increased accuracy: With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n- Verifiable information: Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\nCons:\n- Limited scope: Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n- Dependence on document quality: The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n- Inability to eliminate hallucination completely: Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information.\n\nAddressing another challenge, LLMs have a maximum prompt size, preventing them from feeding entire documents. This makes it crucial to divide documents into smaller parts, and Text Splitters prove to be extremely useful in achieving this. Text Splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively.\nThe basis of all the text splitters in Langchain involves splitting on chunks in some chunk size with some chunk overlap.\nMethods:\n\ncreate_documents(): Create documents from a list of texts.\nsplit_documents(): Split documents.\n\nUsing a Text Splitter can also improve vector store search results, as smaller segments might be more likely to match a query. Experimenting with different chunk sizes and overlaps can be beneficial in tailoring results to suit your specific needs.\n\n\n\n3.2.2 Customizing Text Splitter\n\nWhen handling lengthy pieces of text, it’s crucial to break them down into manageable chunks. This seemingly simple task can quickly become complex, as keeping semantically related text segments intact is essential. The definition of “semantically related” may vary depending on the type of text. In this article, we’ll explore various strategies to achieve this.\nAt a high level, text splitters follow these steps:\n\nDivide the text into small, semantically meaningful chunks (often sentences).\nCombine these small chunks into a larger one until a specific size is reached (determined by a particular function).\nOnce the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n\nConsequently, there are two primary dimensions to consider when customizing your text splitter:\n\nThe method used to split the text\nThe approach for measuring chunk size\n\n\n\n\n3.3.3 Character Text Splitter\n\nThis type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks. For example, you might use it to split a long article into smaller chunks for easier processing or analysis. The splitter allows you to customize the chunking process along two axes - chunk size and chunk overlap - to balance the trade-offs between splitting the text into manageable pieces and preserving semantic context between chunks.\n\n\nloader = PyPDFLoader(\"Introducing MLOps How to Scale Machine Learning in the Enterprise (Mark Treveil, Nicolas Omont, Clément Stenac etc.) (z-lib.org).pdf\")\npages = loader.load_and_split()\n\n\nBy loading the text file, we can ask more specific questions related to the subject, which helps minimize the likelihood of LLM hallucinations and ensures more accurate, context-driven responses.\n\n\ncharacter_text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ntexts = character_text_splitter.split_documents(pages)\n\nprint (f\"You have {len(texts)} documents\\n\\n\")\nprint(texts[0])\n\nYou have 176 documents\n\n\npage_content='Mark Treveil and the Dataiku TeamIntroducing MLOps\\nHow to Scale Machine Learning in the Enterprise\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': 'Introducing MLOps How to Scale Machine Learning in the Enterprise (Mark Treveil, Nicolas Omont, Clément Stenac etc.) (z-lib.org).pdf', 'page': 2}\n\n\n\nNo universal approach for chunking text will fit all scenarios - what’s effective for one case might not be suitable for another. Finding the best chunk size for your project means going through a few steps. First, clean up your data by getting rid of anything that’s not needed, like HTML tags from websites. Then, pick a few different chunk sizes to test. The best size will depend on what kind of data you’re working with and the model you’re using. Finally, test out how well each size works by running some queries and comparing the results. You might need to try a few different sizes before finding the best one. This process might take some time, but getting the best results from your project is worth it.\n\n\n\n\n3.3.4 Recursive Character Text Splitter\n\nThe Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in order until the resulting chunks are small enough. By default, the list of characters used for splitting is [“”, “”, ” “,””], which tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text. This means that the class first tries to split the text into two new-line characters. If the resulting chunks are still larger than the desired chunk size, it will then try to split the output by a single new-line character, followed by a space character, and so on, until the desired chunk size is achieved.\nTo use the RecursiveCharacterTextSplitter, you can create an instance of it and provide the following parameters:\n\nchunk_size : The maximum size of the chunks, as measured by the length_function (default is 100).\nchunk_overlap: The maximum overlap between chunks to maintain continuity between them (default is 20).\nlength_function: parameter is used to calculate the length of the chunks. By default, it is set to len, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n\nUsing a token counter instead of the default len function can benefit specific scenarios, such as when working with language models with token limits. For example, OpenAI’s GPT-3 has a token limit of 4096 tokens per request, so you might want to count tokens instead of characters to better manage and optimize your requests.\n\n\nrecursive_character_text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, \n    chunk_overlap=50,\n    length_function=len\n)\ntexts = recursive_character_text_splitter.split_documents(pages)\n\nprint (f\"You have {len(texts)} documents\\n\\n\")\nprint(texts[0])\n\nYou have 481 documents\n\n\npage_content='Mark Treveil and the Dataiku TeamIntroducing MLOps\\nHow to Scale Machine Learning in the Enterprise\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': 'Introducing MLOps How to Scale Machine Learning in the Enterprise (Mark Treveil, Nicolas Omont, Clément Stenac etc.) (z-lib.org).pdf', 'page': 2}\n\n\n\nWe created an instance of the RecursiveCharacterTextSplitter class with the desired parameters. The default list of characters to split by is [\"\\n\\n\", \"\\n\", \" \", \"\"].\nThe text is first split by two new-line characters (\\n\\n). Then, since the chunks are still larger than the desired chunk size (50), the class tries to split the output by a single new-line character (\\n).\nIn this example, the text is loaded from a file, and the RecursiveCharacterTextSplitter is used to split it into chunks with a maximum size of 50 characters and an overlap of 10 characters. The output will be a list of documents containing the split text.\nTo use a token counter, you can create a custom function that calculates the number of tokens in a given text and pass it as the length_function parameter. This will ensure that your text splitter calculates the length of chunks based on the number of tokens instead of the number of characters.\n\n\n\n3.3.5 NLTK Text Splitter\n\nThe NLTKTextSplitter in LangChain is an implementation of a text splitter that uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers. The goal is to split long texts into smaller chunks without breaking the structure of sentences and paragraphs.\n\n\nIf it is your first time using this package, it is required to install the NLTK library using pip install -q nltk and run the following Python code to download the packages that LangChain needs. import nltk; nltk.download(’punkt’);`\n\n\n!echo \"Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.\" &gt; LLM.txt\n\n# Load a long document\nwith open('LLM.txt', encoding= 'unicode_escape') as f:\n    sample_text = f.read()\n\nfrom langchain.text_splitter import NLTKTextSplitter\ntext_splitter = NLTKTextSplitter(chunk_size=500)\n\n\ntexts = text_splitter.split_text(sample_text)\nprint(texts)\n\n['\"Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.\"']\n\n\n\n\n3.3.6 SpacyTextSplitter\n\nThe SpacyTextSplitter helps split large text documents into smaller chunks based on a specified size. This is useful for better management of large text inputs. It’s important to note that the SpacyTextSplitter is an alternative to NLTK-based sentence splitting. You can create a SpacyTextSplitter object by specifying the chunk_size parameter, measured by a length function passed to it, which defaults to the number of characters.\n\n\n# !python -m spacy download en_core_web_sm\n\n\n# Load a long document\nwith open('LLM.txt', encoding= 'unicode_escape') as f:\n    sample_text = f.read()\n\n# Instantiate the SpacyTextSplitter with the desired chunk size\ntext_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n\n# Split the text using SpacyTextSplitter\ntexts = text_splitter.split_text(sample_text)\n\n# Print the first chunk\nprint(texts)\n\n['\"Helllo, my name is Ala\\n Hello again\\n\\ntesting newline.\"']\n\n\n\n\n3.3.7 MarkdownTextSplitter\n\nThe MarkdownTextSplitter is designed to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of RecursiveCharacterSplitter with Markdown-specific separators. By default, these separators are determined by the Markdown syntax, but they can be customized by providing a list of characters during the initialization of the MarkdownTextSplitter instance. The chunk size, which is initially set to the number of characters, is measured by the length function passed in. To customize the chunk size, provide an integer value when initializing an instance.\n\n\nmarkdown_text = \"\"\"\n#\n\n# Welcome to My Blog!\n\n## Introduction\nHello everyone! My name is **Chirag Sharma** and I am a customer support agent. \nI am an aspiring Data Scientist and I specialize in Python, Machine Learning, SQL.\n\nHere's a list of my favorite programming languages:\n\n1. Python\n2. SQL\n3. HTML\n\nYou can check out some of my projects on [GitHub](https://github.com).\n\n## About this Blog\nIn this blog, I will share my journey as an aspiring Data Scientist. \nI'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n\nHere's a small piece of Python code to say hello:\n\n\\``` python\ndef say_hello(name):\n    print(f\"Hello, {name}!\")\n\nsay_hello(\"Chirag\")\n\\```\n\nStay tuned for more updates!\n\n## Contact Me\nFeel free to reach out to me on [Gmail](https://chirag.sharma0378@gmail.com).\n\n\"\"\"\nmarkdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\ndocuments = markdown_splitter.create_documents([markdown_text])\nprint(documents)\n\n[Document(page_content='#\\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction\\nHello everyone! My name is **Chirag Sharma** and I am a customer support agent.', metadata={}), Document(page_content='I am an aspiring Data Scientist and I specialize in Python, Machine Learning, SQL.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. SQL\\n3. HTML\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog\\nIn this blog, I will share my journey as an aspiring Data Scientist.', metadata={}), Document(page_content=\"I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\", metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"Chirag\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me\\nFeel free to reach out to me on [Gmail](https://chirag.sharma0378@gmail.com).', metadata={})]\n\n\n\nThe MarkdownTextSplitter offers a practical solution for dividing text while preserving the structure and meaning provided by Markdown formatting. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks. This splitter is especially valuable when managing extensive Markdown documents.\n\n\n\n3.3.7 TokenTextSplitter\n\nThe main advantage of using TokenTextSplitter over other text splitters, like CharacterTextSplitter, is that it respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This can be particularly helpful in maintaining the semantic integrity of the text when working with language models and embeddings.\nThis type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks. It then reassembles the tokens within each chunk back into text. The tiktoken python package is required for using this class. (pip install -q tiktoken)\n\n\n# Load a long document\nwith open('LLM.txt', encoding= 'unicode_escape') as f:\n    sample_text = f.read()\n\n# Initialize the TokenTextSplitter with desired chunk size and overlap\ntext_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n\n# Split into smaller chunks\ntexts = text_splitter.split_text(sample_text)\nprint(texts[0])\n\n\"Helllo, my name is Ala\n Hello again\n\ntesting newline.\" \n\n\n\n\nThe chunk_size parameter sets the maximum number of BPE tokens in each chunk, while chunk_overlap defines the number of overlapping tokens between adjacent chunks. By modifying these parameters, you can fine-tune the granularity of the text chunks.\nOne potential drawback of using TokenTextSplitter is that it may require additional computation when converting text to BPE tokens and back. If you need a faster and simpler text-splitting method, you might consider using CharacterTextSplitter, which directly splits the text based on character count, offering a more straightforward approach to text segmentation.\n\n\n\n3.3.8 MarkdownHeaderTextSplitter\nChunking aims to keep text with common context together (Context aware splitting).\nA text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\nWe can use MarkdownHeaderTextSplitter to preserve header metadata in our chunks, as show below.\n\nmarkdown_document = \"\"\"# Title\\n\\n \\\n## Chapter 1\\n\\n \\\nHi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n### Section \\n\\n \\\nHi this is Lance \\n\\n \n## Chapter 2\\n\\n \\\nHi this is Molly\"\"\"\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n\nmarkdown_splitter = MarkdownHeaderTextSplitter(\n    headers_to_split_on=headers_to_split_on\n)\nmd_header_splits = markdown_splitter.split_text(markdown_document)\nprint(md_header_splits[0])\n\npage_content='Hi this is Jim  \\nHi this is Joe' metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}\n\n\n\nmd_header_splits[1]\n\nDocument(page_content='Hi this is Lance', metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'})\n\n\n\n\n3.3.8 RECAP:\nText splitters are essential for managing long text, improving language model processing efficiency, and enhancing vector store search results. Customizing text splitters involves selecting the splitting method and measuring chunk size.\nCharacterTextSplitter is an example that helps balance manageable pieces and semantic context preservation. Experimenting with different chunk sizes and overlaps tailor the results for specific use cases.\nRecursiveCharacterTextSplitter focuses on preserving semantic relationships while offering customizable chunk sizes and overlaps.\nNLTKTextSplitter utilizes the Natural Language Toolkit library for more accurate text segmentation. SpacyTextSplitter leverages the popular SpaCy library to split texts based on linguistic features. MarkdownTextSplitter is tailored for Markdown-formatted texts, ensuring content is split meaningfully according to the syntax. Lastly, TokenTextSplitter employs BPE tokens for splitting, offering a fine-grained approach to text segmentation.\nSelecting the appropriate text splitter depends on the specific requirements and nature of the text you are working with, ensuring optimal results for your text processing tasks."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#vectorstores-and-embeddings",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#vectorstores-and-embeddings",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "4.1 VectorStores and Embeddings",
    "text": "4.1 VectorStores and Embeddings\nWe’ve now got our documents split up into small & sematically meaningful chunks, and we then put these chunks into an index whereby we can easily retrieve them when a new query comes to answer questions. To achieve that we utilize VectorStores and Embeddings.\n\n4.1.1 Introduction to the World of Embeddings\n\nVector embeddings are among the most intriguing and beneficial aspects of machine learning, playing a pivotal role in many natural language processing, recommendation, and search algorithms. If you’ve interacted with recommendation engines, voice assistants, or language translators, you’ve engaged with systems that utilize embeddings.\nEmbeddings are dense vector representations of data that encapsulate semantic information, making them suitable for various machine-learning tasks such as clustering, recommendation, and classification. They transform human-perceived semantic similarity into closeness in vector space and can be generated for different data types, including text, images, and audio.\nFor text data, models like the GPT family of models and Llama are employed to create vector embeddings for words, sentences, or paragraphs. In the case of images, convolutional neural networks (CNNs) such as VGG and Inception can generate embeddings. Audio recordings can be converted into vectors using image embedding techniques applied to visual representations of audio frequencies, like spectrograms. Deep neural networks are commonly employed to train models that convert objects into vectors. The resulting embeddings are typically high-dimensional and dense.\nEmbeddings are extensively used in similarity search applications, such as KNN and ANN, which require calculating distances between vectors to determine similarity. Nearest neighbor search can be employed for tasks like de-duplication, recommendations, anomaly detection, and reverse image search.\n\n\n\n4.1.2 Similarity search and vector embeddings\n\nOpenAI offers a powerful language model called GPT-3, which can be used for various tasks, such as generating embeddings and performing similarity searches. In this example, we’ll use the OpenAI API to generate embeddings for a set of documents and then perform a similarity search using cosine similarity.\nFirst, let’s install the required packages with the following command: pip install langchain==0.0.208 deeplake openai tiktoken scikit-learn.\nNext, create an API key from the OpenAI website and set it as an environment variable:\nLet’s generate embeddings for our documents and perform a similarity search:\n\n\n## Defining the documents\ndocuments = [\n    \"The cat is on the mat.\",\n    \"There is a dog sitting on the couch.\",\n    \"The dog is in the yard.\",\n    \"The whale is blue in color.\"\n]\n## Initializing the embeddings\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n## Generate embeddings\ndocument_embeddings = embeddings.embed_documents(documents)\n## Perform a similarity search for a given query\nquery = \"The cat is walking nearby the store.\"\nquery_embedding = embeddings.embed_query(query)\n## Calculate simialarity scores\nsimilarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n## Find the most similar documents\nmost_similar_index = np.argmax(similarity_scores)\nmost_similar_document = documents[most_similar_index]\n\nprint(f\"Most similar document to the query: {query}\")\nprint(most_similar_document)\n\nMost similar document to the query: The cat is walking nearby the store.\nThe cat is on the mat.\n\n\n\nWe initialize the OpenAI API client by setting the OpenAI API key. This allows us to use OpenAI’s services for generating embeddings.\nWe then define a list of documents as strings. These documents are the text data we want to analyze for semantic similarity.\nIn order to perform this analysis, we need to convert our documents into a format that our similarity computation algorithm can understand. This is where OpenAIEmbeddings class comes in. We use it to generate embeddings for each document, transforming them into vectors that represent their semantic content.\nSimilarly, we also transform our query string into an embedding. The query string is the text we want to find the most similar document too.\nWith our documents and query now in the form of embeddings, we compute the cosine similarity between the query embedding and each document embedding. The cosine similarity is a metric used to determine how similar two vectors are. In our case, it gives us a list of similarity scores for our query against each document.\nWith our similarity scores in hand, we then identify the document most similar to our query. We do this by finding the index of the highest similarity score and retrieving the corresponding document from our list of documents.\nEmbedding vectors positioned near each other are regarded as similar. At times, they are directly applied to display related items in online shops. In other instances, they are incorporated into various models to share insights across akin items rather than considering them as entirely distinct entities. This renders embeddings effective in representing aspects like web browsing patterns, textual data, and e-commerce transactions for subsequent model applications.\n\n\n\n4.1.3 Embedding Models\n\nEmbedding models are a type of machine learning model that convert discrete data into continuous vectors. In the context of natural language processing, these discrete data points can be words, sentences, or even entire documents. The generated vectors, also known as embeddings, are designed to capture the semantic meaning of the original data.\nFor instance, words that are semantically similar (e.g., ‘cat’ and ‘kitten’) would have similar embeddings. These embeddings are dense, which means that they use many dimensions (often hundreds) to capture nuances in meaning.\nThe primary benefit of embeddings is that they allow us to use mathematical operations to reason about semantic meaning. For example, we can calculate the cosine similarity between two embeddings to assess how semantically similar the corresponding words or documents are."
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#retrieval",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#retrieval",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "5.1 Retrieval",
    "text": "5.1 Retrieval\nRetriever is the system that is responsible for accurately fetching the correct snippet of information that is used in responding to the user query. Retrievers accept a Query as an input and return a list of Documents as an output.\n\n5.1.1 Some Popular Retrieval Methods\n\nSimilarity Search: This method works by calculating the distance between the embedding vectors of the input and the documents.\n\n\n## Loading the texts and splitting them into chunks\nloader = TextLoader(\"IntroToLLM_AndrejKarpathy.txt\")\ndocuments = loader.load()\n## Split it into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\ndocs = text_splitter.split_documents(documents)\n## Create the embeddings\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n## Load it in ChromaDB\ndb = Chroma.from_documents(docs, embedding=embeddings)\n## Input Query\nquery = \"What did Andrej Karpathy say about LLM operating system?\"\ndocs = db.similarity_search(query)\n## Printing the results\nprint(docs[0].page_content)\n\nthat I've shown you, I'm just tying it all together. I don't think it's accurate to think of large language models as a chatbot or like some kind of a word generator. I think it's a lot more correct to think about it as the kernel process of an emerging operating system. Basically, this process is coordinating a lot of resources, be they memory or computational tools for problem-solving. So let's think through based on everything I've shown you, what an LLM might look like in a few years. It can read and generate text. It has a lot more knowledge than any single human about all the subjects. It can browse the Internet or reference local files through retrieval augmented generation. It can use existing software infrastructure like Calculator, Python, et cetera. It can see and generate images and videos. It can hear and speak and generate music. It can think for a long time using System 2. It can maybe self-improve in some narrow domains that have a reward function available. Maybe it\n\n\n\nMaximal Margincal Relevance: This method addresses redundancy in retrieval. This considers the relevance of each document only in terms of how much new information it brings given the previous results. MMR tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents.\n\nfetch_k = Number of documents in the initial retrieval\nk = final number of reranked documents to output\n\n\n\n## Loading the texts and splitting them into chunks\nloader = TextLoader(\"IntroToLLM_AndrejKarpathy.txt\")\ndocuments = loader.load()\n## Split it into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\ndocs = text_splitter.split_documents(documents)\n## Create the embeddings\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n## Load it in ChromaDB\ndb = Chroma.from_documents(docs, embedding=embeddings)\n## Input Query\nquery = \"What did Andrej Karpathy say about LLM operating system?\"\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=10)\n## Printing the results\nfor i, doc in enumerate(docs):\n    print(f\"{i + 1}.\", doc.page_content, \"\\n\")\n\n1. that I've shown you, I'm just tying it all together. I don't think it's accurate to think of large language models as a chatbot or like some kind of a word generator. I think it's a lot more correct to think about it as the kernel process of an emerging operating system. Basically, this process is coordinating a lot of resources, be they memory or computational tools for problem-solving. So let's think through based on everything I've shown you, what an LLM might look like in a few years. It can read and generate text. It has a lot more knowledge than any single human about all the subjects. It can browse the Internet or reference local files through retrieval augmented generation. It can use existing software infrastructure like Calculator, Python, et cetera. It can see and generate images and videos. It can hear and speak and generate music. It can think for a long time using System 2. It can maybe self-improve in some narrow domains that have a reward function available. Maybe it \n\n2. 7b beta that is based on the Mistral series from another startup in France. But roughly speaking, what you're seeing today in the ecosystem is that the closed models work a lot better, but you can't really work with them, fine-tune them, download them, etc. You can use them through a web interface, and then behind that are all the open source models and the entire open source ecosystem. And all this stuff works worse, but depending on your application that might be good enough. And so currently I would say the open source ecosystem is trying to boost performance and sort of chase the proprietary ecosystems, and that's roughly the dynamic that you see today in the industry. Okay, so now I'm going to switch gears and we're going to talk about the language models, how they're improving, and where all of it is going in terms of those improvements. The first very important thing to understand about the large language model space are what we call scaling laws. It turns out that the \n\n\n\n\nContextual compression: Sometimes, relevant info is hidden in long documents with a lot of extra stuff. Contextual Compression helps with this by squeezing down the documents to only the important parts that match your search.\n\nWhat occurred behind the scenes?\nInitially, we employed a so-called “stuff chain” (refer to CombineDocuments Chains). Stuffing is one way to supply information to the LLM. Using this technique, we “stuff” all the information into the LLM’s prompt. However, this method is only effective with shorter documents, as most LLMs have a context length limit.\nAdditionally, a similarity search is conducted using the embeddings to identify matching documents to be used as context for the LLM. Although it might not seem particularly useful with just one document, we are effectively working with multiple documents since we “chunked” our text. Preselecting the most suitable documents based on semantic similarity enables us to provide the model with meaningful knowledge through the prompt while remaining within the allowed context size.\nSo, in this exploration, we have discovered the significant role that indexes and retrievers play in improving the performance of Large Language Models when handling document-based data.\nThe system becomes more efficient in finding and presenting relevant information by converting documents and user queries into numerical vectors (embeddings) and storing them in specialized databases like Deep Lake, which serves as our vector store database.\nThe retriever’s ability to identify documents that are closely related to a user’s query in the embedding space demonstrates the effectiveness of this approach in enhancing the overall language understanding capabilities of LLMs.\nA Potential Problem\nThis method has a downside: you might not know how to get the right documents later when storing data. In the Q&A example, we cut the text into equal parts, causing both useful and useless text to show up when a user asks a question.\nIncluding unrelated information in the LLM prompt is detrimental because: It can divert the LLM’s focus from pertinent details. It occupies valuable space that could be utilized for more relevant information.\nPossible Solution\nA DocumentCompressor abstraction has been introduced to address this issue, allowing compress_documents on the retrieved documents.\nThe ContextualCompressionRetriever is a wrapper around another retriever in LangChain. It takes a base retriever and a DocumentCompressor and automatically compresses the retrieved documents from the base retriever. This means that only the most relevant parts of the retrieved documents are returned, given a specific query.\nA popular compressor choice is the LLMChainExtractor, which uses an LLMChain to extract only the statements relevant to the query from the documents. To improve the retrieval process, a ContextualCompressionRetriever is used, wrapping the base retriever with an LLMChainExtractor compressor. The LLMChainExtractor compressor iterates over the initially returned documents and extracts only the content relevant to the query.\nHere’s an example of how to use ContextualCompressionRetriever with LLMChainExtractor:\n\n## Loading the texts and splitting them into chunks\nloader = TextLoader(\"IntroToLLM_AndrejKarpathy.txt\")\ndocuments = loader.load()\n## Split it into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\ndocs = text_splitter.split_documents(documents)\n## Create the embeddings\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n## Load it in ChromaDB\ndb = Chroma.from_documents(docs, embedding=embeddings)\n## Input Query\nquery = \"What did Andrej Karpathy say about LLM operating system?\"\n# create GPT3 wrapper\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n# create compressor for the retriever\ncompressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=db.as_retriever()\n)\n# retrieving compressed documents\nretrieved_docs = compression_retriever.get_relevant_documents(query)\nprint(retrieved_docs[0].page_content)\n\nC:\\Users\\Chirag Sharma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n  warnings.warn(\n\n\nIt's a lot more correct to think about it as the kernel process of an emerging operating system.\n\n\n\nSelf Query: A self-querying retriever is a system that can ask itself questions. When you give it a question in normal language, it uses a special process to turn that question into a structured query. Then, it uses this structured query to search through its stored information. This way, it doesn’t just compare your question with the documents. It also looks for specific details in the documents based on your question, making the search more efficient and accurate.\n\nAddressing Specificity: working with metadata using self-query retriever\n\nThere are several situations where the Query applied to the DB is more than just than the Question asked.\nOne is SelfQuery, where we use an LLM to convert the user question into a query.\n\nBut we have an interesting challenge: we often want to infer the metadata from the query itself.\nTo address this, we can use SelfQueryRetriever, which uses an LLM to extract:\n\nThe query string to use for vector search\nA metadata filter to pass in as well\n\nMost vector databases support metadata filters, so this doesn’t require any new databases or indexes.\n\ndocs = [\n    Document(\n        page_content=\"A group of astronauts discover a mysterious monolith on the Moon, leading to a journey of cosmic exploration and self-discovery\",\n        metadata={\"year\": 1968, \"director\": \"Stanley Kubrick\", \"rating\": 8.5, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"A man embarks on a quest to find his missing wife, uncovering dark secrets and facing existential dilemmas along the way\",\n        metadata={\"year\": 2014, \"director\": \"David Fincher\", \"rating\": 8.1, \"genre\": \"mystery\"},\n    ),\n    Document(\n        page_content=\"A retired detective with memory loss tries to piece together clues from his past to solve a complex murder case\",\n        metadata={\"year\": 2000, \"director\": \"Christopher Nolan\", \"rating\": 8.7, \"genre\": \"thriller\"},\n    ),\n    Document(\n        page_content=\"A family struggles to survive in a post-apocalyptic world overrun by flesh-eating zombies\",\n        metadata={\"year\": 2010, \"director\": \"Frank Darabont\", \"rating\": 8.9, \"genre\": \"horror\"},\n    ),\n    Document(\n        page_content=\"A brilliant mathematician overcomes adversity and discrimination to revolutionize the field of cryptography\",\n        metadata={\"year\": 2014, \"director\": \"Morten Tyldum\", \"rating\": 8.0, \"genre\": \"biography\"},\n    ),\n    Document(\n        page_content=\"A young girl discovers her magical powers and must learn to control them while navigating the challenges of adolescence\",\n        metadata={\"year\": 2001, \"director\": \"Chris Columbus\", \"rating\": 7.8, \"genre\": \"fantasy\"},\n    ),\n]\n\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n\nNow we can instantiate our retriever. To do this we’ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"genre\",\n        description=\"The genre of the movie. One of ['science fiction', 'mystery', 'thriller', 'horror', 'biography', 'fantasy']\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"year\",\n        description=\"The year the movie was released\",\n        type=\"integer\",\n    ),\n    AttributeInfo(\n        name=\"director\",\n        description=\"The name of the movie director\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"rating\",\n        description=\"A 1-10 rating for the movie\",\n        type=\"float\"\n    ),\n]\ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n\n\n# This example only specifies a filter\nretriever.get_relevant_documents(\"I want to watch a movie rated higher than 8.5\")\n\n[Document(page_content='A family struggles to survive in a post-apocalyptic world overrun by flesh-eating zombies', metadata={'director': 'Frank Darabont', 'genre': 'horror', 'rating': 8.9, 'year': 2010}),\n Document(page_content='A retired detective with memory loss tries to piece together clues from his past to solve a complex murder case', metadata={'director': 'Christopher Nolan', 'genre': 'thriller', 'rating': 8.7, 'year': 2000})]\n\n\n\nfor d in docs:\n    print(d.metadata)\n\n{'year': 1968, 'director': 'Stanley Kubrick', 'rating': 8.5, 'genre': 'science fiction'}\n{'year': 2014, 'director': 'David Fincher', 'rating': 8.1, 'genre': 'mystery'}\n{'year': 2000, 'director': 'Christopher Nolan', 'rating': 8.7, 'genre': 'thriller'}\n{'year': 2010, 'director': 'Frank Darabont', 'rating': 8.9, 'genre': 'horror'}\n{'year': 2014, 'director': 'Morten Tyldum', 'rating': 8.0, 'genre': 'biography'}\n{'year': 2001, 'director': 'Chris Columbus', 'rating': 7.8, 'genre': 'fantasy'}"
  },
  {
    "objectID": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#question-answering",
    "href": "blog/Chat_with_you_Data_LangChain/Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.html#question-answering",
    "title": "Unveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.",
    "section": "6.1 Question Answering",
    "text": "6.1 Question Answering\nPost-Retrieval the next set of steps include merging the user query and the retrieved context (Augmentation step) and passing this merged prompt as an instruction to an LLM (Generation step)."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#introduction",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#introduction",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#what-is-web-scraping",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#what-is-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "What is Web Scraping? ",
    "text": "What is Web Scraping? \n\nWeb Scraping is the process of automatically downloading the data displayed on the website using some computer program. A web scraping tool can scrape multiple pages from a website & automate the tedious task of manually copying and pasting the data displayed. Web Scraping is important because, irrespective of the industry, the web contains information that can provide actionable insights for businesses to gain an advantage over competitors."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:",
    "text": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:\n\nFind the URL that you want to scrape\nInspecting the Page\nFind the data you want to extract\nWrite the code\nRun the code & extract the data\nFinally, Store the data in the required format"
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#packages-used-for-web-scraping",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#packages-used-for-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Packages used for Web Scraping",
    "text": "Packages used for Web Scraping\n\nWe’ll use the following python packages:\n\nPandas: Pandas is a library used for data manipulation and analysis. It is used to store the data in the desired format.\nBeautifulSoup4: BeautifulSoup is the python web scraping library used for parsing HTML documents. It creates parse trees that are helpful in extracting tags from the HTML string.\nSelenium: Selenium is a tool designed to help you run automated tests in web applications. Although it’s not its main purpose, Selenium is also used in Python for web scraping, because it can access JavaScript-rendered content (which regular scraping tools like BeautifulSoup can’t do). We’ll use Selenium to download the HTML content from Flipkart and see in an interactive way what’s happening."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#project-demonstration",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#project-demonstration",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Project Demonstration",
    "text": "Project Demonstration\n\nImporting necessary Libraries\n\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\n\n\nStarting up the WebDriver\n\n# Creating an instance of webdriver for google chrome\ndriver = webdriver.Chrome()\n\n\n# Using webdriver we'll now open the flipkart website in chrome\nurl = 'https://flipkart.com'\n# We;ll use the get method of driver and pass in the URL\ndriver.get(url)\n\n\nNow there a few ways we can conduct a product search :\n\n\nFirst is to automate the browser by finding the input element and then insert a text and hit enter key on the keyboard. The image like below.\n\n\n\n\n\nHowever, this kind of automation is unnecessary and it creates a potential for program failure. The Rule of thumb for automation is to only automate what you absolutely need to when Web Scraping.\n\nLet’s search the input inside the search area and hit enter. You’ll notice that the search term has now embeded into the URL site. Now we can use this pattern to create a function that will build the necessary URL for our driver to retrieve. This will be much more efficient in the long term and less prone to proram failure. The image like below.\n\n\n\n\nLet’s copy this Pattern and create a function that will insert the search term using string formatting.\n\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    # We'are replacing every space with '+' to adhere with the pattern \n    search_item = search_item.replace(\" \",\"+\")\n    return template.format(search_item)\n\n\nNow we have a function that will generate a URL based on the search term we provide.\n\n\n# Checking whether the function is working properly or not\nurl = get_url('mobile phones')\nprint(url)\n\nhttps://www.flipkart.com/search?q=mobile+phones&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on\n\n\n\nThe fuction produces the same result as before.\n\n\n\nExtracting the collection\n\nNow we are going to extract the contents of the webpage from which we want to extract the information from.\nTo do that we need to create a BeautifulSoup object which will parse the HTML content from the page source.\n\n\n# Creating a soup object using driver.page_source to retreive the HTML text and then we'll use the default html parser to parse\n# the HTML.\nsoup = BeautifulSoup(driver.page_source, 'html.parser')\n\n\n\nNow that we have identified that the above card indicated by the box contains all the information what we need for a mobile phone. So let’s find out all the tags for these boxes/cards which contains information we want to extract.\nWe’ll be extracting Model , stars, number of ratings, number of reviews, RAM, Storage capacity, Exapandable option, display, camera information, battery, processor , warranty and Price information."
  },
  {
    "objectID": "blog/Web_Scraping/Web_Scraping_Tutorial.html#inspecting-the-tags",
    "href": "blog/Web_Scraping/Web_Scraping_Tutorial.html#inspecting-the-tags",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Inspecting the tags",
    "text": "Inspecting the tags\n\n\nWe can fetch the a tag & specificallyclass = _1fQZEK to get all the cards/boxes and then we can easily take out information of out these boxes for any mobile phone.\n\n\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nlen(results)\n\n24\n\n\n\nPrototyping for a single record\n\n# picking the 1st card from the complete list of cards\nitem = results[0]\n\n\n# Extracting the model of the phone from the 1st card\nmodel = item.find('div',{'class':\"_4rR01T\"}).text\nmodel\n\n'REDMI 9i (Nature Green, 64 GB)'\n\n\n\n# Extracting Stars from 1st card\nstar = item.find('div',{'class':\"_3LWZlK\"}).text\nstar\n\n'4.3'\n\n\n\n# Extracting Number of Ratings from 1st card\nnum_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\nnum_ratings\n\n'4,06,452 Ratings'\n\n\n\n# Extracting Number of Reviews from 1st card\nreviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\nreviews\n\n'23,336 Reviews'\n\n\n\n# Extracting RAM from the 1st card\nram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\nram\n\n'4 GB RAM '\n\n\n\n# Extracting Storage/ROM from 1st card\nstorage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\nstorage\n\n'64 GB ROM'\n\n\n\n# Extracting whether there is an option of expanding the storage or not\nexpandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\nexpandable\n\n'Expandable Upto 512 GB'\n\n\n\n# Extracting the display option from the 1st card\ndisplay = item.find_all('li')[1].text.strip()\ndisplay\n\n'16.59 cm (6.53 inch) HD+ Display'\n\n\n\n# Extracting camera options from the 1st card\ncamera = item.find_all('li')[2].text.strip()\ncamera\n\n'13MP Rear Camera | 5MP Front Camera'\n\n\n\n# Extracting the battery option from the 1st card\nbattery = item.find_all('li')[3].text\nbattery\n\n'5000 mAh Lithium Polymer Battery'\n\n\n\n# Extracting the processir option from the 1st card\nprocessor = item.find_all('li')[4].text.strip()\nprocessor\n\n'MediaTek Helio G25 Processor'\n\n\n\n# Extracting Warranty from the 1st card\nwarranty = item.find_all('li')[-1].text.strip()\nwarranty\n\n'Brand Warranty of 1 Year Available for Mobile and 6 Months for Accessories'\n\n\n\n# Extracting price of the model from the 1st card\nprice = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n\n\n\nGeneralizing the Pattern\n\nNow let create a function that will extract all the information at once from a single page.\n\n\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\n\n# Now putting all the information from all the cards/phone models and putting them into a list\nrecords_list = []\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nfor item in results:\n    records_list.append(extract_phone_model_info(item))\n\n\nViewing how does our dataframe look like for the 1st page.\n\n\npd.DataFrame(records_list,columns=['model',\"star\",\"num_ratings\"\n   ,\"reviews\",'ram',\"storage\",\"expandable\",\"display\",\"camera\",\"battery\",\"processor\",\"warranty\",\"price\"])\n\n\n\n\n\n\n\n\nmodel\nstar\nnum_ratings\nreviews\nram\nstorage\nexpandable\ndisplay\ncamera\nbattery\nprocessor\nwarranty\nprice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n4\nrealme C21 (Cross Blue, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n5\nREDMI 9 Power (Mighty Black, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n6\nPOCO M3 (Cool Blue, 64 GB)\n4.3\n14,630 Ratings\n930 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹10,499\n\n\n7\nPOCO M2 Reloaded (Mostly Blue, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n8\nrealme C11 2021 (Cool Grey, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n9\nrealme C11 2021 (Cool Blue, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n10\nREDMI 9 Power (Fiery Red, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n11\nREDMI 9i (Midnight Black, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n12\nInfinix Smart 5A (Quetzal Cyan, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n13\nInfinix Smart 5A (Midnight Black, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n14\nInfinix Smart 5A (Ocean Wave, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n15\nPOCO M3 (Power Black, 64 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹11,499\n\n\n16\nPOCO M3 (Power Black, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n17\nPOCO M2 Reloaded (Greyish Black, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n18\nREDMI Note 9 (Aqua Green, 64 GB)\n4.3\n92,325 Ratings\n6,906 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 13MP Front Camera\n5020 mAh Battery\nMediaTek Helio G85 Processor\n1 Year Manufacturer Warranty for Device and 6 ...\n₹11,999\n\n\n19\nrealme Narzo 30 5G (Racing Silver, 128 GB)\n4.4\n27,618 Ratings\n2,243 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 1 TB\n16.51 cm (6.5 inch) Full HD+ Display\n48MP + 2MP + 2MP | 16MP Front Camera\n5000 mAh Battery\nMediaTek Dimensity 700 (MT6833) Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹15,999\n\n\n20\nREDMI 9 Power (Blazing Blue, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n21\nPOCO M3 (Yellow, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n22\nrealme C20 (Cool Grey, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n23\nrealme C20 (Cool Blue, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n\n\n\n\n\n\n\nNavigating to next page\n\nWriting a custom function that will help us getting information from multiple pages\n\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\n\n\nPutting it all together\n\nNow combining all thhe things that we have done so far.\n\n\n# Importing necessary Libraries\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\ndef main(search_item):\n    '''\n    This function will create a dataframe for all the details that we are fetching from all the multiple pages\n    '''\n    driver = webdriver.Chrome()\n    records = []\n    url = get_url(search_item)\n    for page in range(1,464):\n        driver.get(url.format(page))\n        soup = BeautifulSoup(driver.page_source,'html.parser')\n        results = soup.find_all('a',{'class':\"_1fQZEK\"})\n        for item in results:\n            records.append(extract_phone_model_info(item))\n    driver.close()\n    # Saving the data into a csv file\n    with open('Flipkart_results.csv','w',newline='',encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Model','Stars','Num_of_Ratings','Reviews','Ram','Storage','Expandable',\n                        'Display','Camera','Battery','Processor','Warranty','Price'])\n        writer.writerows(records)\n\n\n\nExtracting Informtion of all the Mobile phones present on multiple pages\n\n%%time\nmain('mobile phones')\n\nWall time: 40min 54s\n\n\n\n\nViewing the data\n\nscraped_df = pd.read_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Jupyter Notebook\\\\Jovian Web Scraping\\\\Amazon Products Web Scrapper\\\\Flipkart_results.csv')\nscraped_df.head()\n\n\n\n\n\n\n\n\nModel\nStars\nNum_of_Ratings\nReviews\nRam\nStorage\nExpandable\nDisplay\nCamera\nBattery\nProcessor\nWarranty\nPrice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nSAMSUNG Galaxy S21 Ultra (Phantom Silver, 256 GB)\n4.5\n537 Ratings\n101 Reviews\n12 GB RAM\n256 GB RO\nNaN\n17.27 cm (6.8 inch) Quad HD+ Display\n108MP + 12MP + 10MP + 10MP | 40MP Front Camera\n5000 mAh Lithium-ion Battery\nExynos 2100 Processor\n1 Year Manufacturer Warranty for Handset and 6...\n₹1,05,999\n\n\n4\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nUnveiling the Secrets of Document Question-Answering Chatbots - A Complete Guide with LangChain.\n\n\n\nLLM\n\n\nLangchain\n\n\n\nIn this post, we’ll show how to create a Document Question-Answering RAG-based Chatbot with Langchain and explain each component used by the Langchain library.\n\n\n\nChirag Sharma\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.\n\n\n\nGenerativeAI\n\n\nLangchain\n\n\nOpenAI\n\n\nLLMs\n\n\n\nIn this post, we’ll create a simple chatbot that can talk with YouTube videos using Langchain & OpenAI API.\n\n\n\nChirag Sharma\n\n\nJan 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping using BeautifulSoup & Selenium in Python.\n\n\n\nWeb Scrapping\n\n\nPython\n\n\nBeautifulSoup\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\nChirag Sharma\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chirag Sharma",
    "section": "",
    "text": "About Me\nHey there! 👋🏼 I’m Chirag, and my journey into the world of data science is driven by a passion for problem-solving and a fascination with the power of data.\nI stumbled upon the magic of data science during my academic journey, where I found myself captivated by the idea of using data to uncover insights and drive meaningful change.\nMy interest in this field deepened further during an internship at Ineuron Intelligence Pvt. Limited, where I witnessed firsthand the transformative potential of data analysis. From unraveling complex patterns to devising innovative solutions, I was hooked.\nNow, armed with a blend of academic knowledge and practical experience, I’m on a mission to harness the power of data to tackle real-world challenges. Whether it’s optimizing processes, identifying trends, or driving decision-making, I’m passionate about leveraging technology to make a positive impact.\nI believe that every data point holds a key to solving problems and unlocking opportunities, and I’m excited to continue this journey alongside fellow enthusiasts. Let’s connect and explore the endless possibilities of a data-driven world together!\n\n\nEducation\nMaster’s in Economics | 2020 | Delhi School of Economics, University of Delhi\nBachelor’s in Economics (Hons.) | 2016 | Motitlal Nehru College, University of Delhi\n\n\nExperience\nConcentrix | Sept 2023 - Present\n\nTechnical Advisor I\n\nData Science Intern | May 2022 - Nov 2022\n\nIneuron Intelligence Pvt. Limited"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "Balanced Tree Clothing Company prides themselves on providing an optimised range of clothing and lifestyle wear for the modern adventurer!\nDanny, the CEO of this trendy fashion company has asked you to assist the team’s merchandising teams analyse their sales performance and generate a basic financial report to share with the wider business.\n\n\n\n\nFor this case study there is a total of 4 datasets - however you will only need to utilise 2 main tables to solve all of the regular questions, and the additional 2 tables are used only for the bonus challenge question!\n\n\nbalanced_tree.product_details includes all information about the entire range that Balanced Clothing sells in their store.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nprice\nproduct_name\ncategory_id\nsegment_id\nstyle_id\ncategory_name\nsegment_name\nstyle_name\n\n\n\n\nc4a632\n13\nNavy Oversized Jeans - Womens\n1\n3\n7\nWomens\nJeans\nNavy Oversized\n\n\ne83aa3\n32\nBlack Straight Jeans - Womens\n1\n3\n8\nWomens\nJeans\nBlack Straight\n\n\ne31d39\n10\nCream Relaxed Jeans - Womens\n1\n3\n9\nWomens\nJeans\nCream Relaxed\n\n\nd5e9a6\n23\nKhaki Suit Jacket - Womens\n1\n4\n10\nWomens\nJacket\nKhaki Suit\n\n\n72f5d4\n19\nIndigo Rain Jacket - Womens\n1\n4\n11\nWomens\nJacket\nIndigo Rain\n\n\n9ec847\n54\nGrey Fashion Jacket - Womens\n1\n4\n12\nWomens\nJacket\nGrey Fashion\n\n\n5d267b\n40\nWhite Tee Shirt - Mens\n2\n5\n13\nMens\nShirt\nWhite Tee\n\n\nc8d436\n10\nTeal Button Up Shirt - Mens\n2\n5\n14\nMens\nShirt\nTeal Button Up\n\n\n2a2353\n57\nBlue Polo Shirt - Mens\n2\n5\n15\nMens\nShirt\nBlue Polo\n\n\nf084eb\n36\nNavy Solid Socks - Mens\n2\n6\n16\nMens\nSocks\nNavy Solid\n\n\nb9a74d\n17\nWhite Striped Socks - Mens\n2\n6\n17\nMens\nSocks\nWhite Striped\n\n\n2feb6b\n29\nPink Fluro Polkadot Socks - Mens\n2\n6\n18\nMens\nSocks\nPink Fluro Polkadot\n\n\n\n\n\n\nbalanced_tree.sales contains product level information for all the transactions made for Balanced Tree including quantity, price, percentage discount, member status, a transaction ID and also the transaction timestamp.\n\n\n\n\n\n\n\n\n\n\n\n\nprod_id\nqty\nprice\ndiscount\nmember\ntxn_id\nstart_txn_time\n\n\n\n\nc4a632\n4\n13\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n5d267b\n4\n40\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nb9a74d\n4\n17\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n2feb6b\n2\n29\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nc4a632\n5\n13\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\ne31d39\n2\n10\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n72f5d4\n3\n19\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n2a2353\n3\n57\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nf084eb\n3\n36\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nc4a632\n1\n13\n21\nf\nef648d\n2021-01-27 02:18:17.1648\n\n\n\n\n\n\nThes tables are used only for the bonus question where we will use them to recreate the balanced_tree.product_details table.\nbalanced_tree.product_hierarchy\n\n\n\nid\nparent_id\nlevel_text\nlevel_name\n\n\n\n\n1\n\nWomens\nCategory\n\n\n2\n\nMens\nCategory\n\n\n3\n1\nJeans\nSegment\n\n\n4\n1\nJacket\nSegment\n\n\n5\n2\nShirt\nSegment\n\n\n6\n2\nSocks\nSegment\n\n\n7\n3\nNavy Oversized\nStyle\n\n\n8\n3\nBlack Straight\nStyle\n\n\n9\n3\nCream Relaxed\nStyle\n\n\n10\n4\nKhaki Suit\nStyle\n\n\n11\n4\nIndigo Rain\nStyle\n\n\n12\n4\nGrey Fashion\nStyle\n\n\n13\n5\nWhite Tee\nStyle\n\n\n14\n5\nTeal Button Up\nStyle\n\n\n15\n5\nBlue Polo\nStyle\n\n\n16\n6\nNavy Solid\nStyle\n\n\n17\n6\nWhite Striped\nStyle\n\n\n18\n6\nPink Fluro Polkadot\nStyle\n\n\n\nbalanced_tree.product_prices\n\n\n\nid\nproduct_id\nprice\n\n\n\n\n7\nc4a632\n13\n\n\n8\ne83aa3\n32\n\n\n9\ne31d39\n10\n\n\n10\nd5e9a6\n23\n\n\n11\n72f5d4\n19\n\n\n12\n9ec847\n54\n\n\n13\n5d267b\n40\n\n\n14\nc8d436\n10\n\n\n15\n2a2353\n57\n\n\n16\nf084eb\n36\n\n\n17\nb9a74d\n17\n\n\n18\n2feb6b\n29\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA balanced_tree;\n\nCREATE TABLE balanced_tree.product_hierarchy (\n  `id` INTEGER,\n  `parent_id` INTEGER,\n  `level_text` VARCHAR(19),\n  `level_name` VARCHAR(8)\n);\n\nINSERT INTO balanced_tree.product_hierarchy\n  (`id`, `parent_id`, `level_text`, `level_name`)\nVALUES\n  ('1', NULL, 'Womens', 'Category'),\n  ('2', NULL, 'Mens', 'Category'),\n  ('3', '1', 'Jeans', 'Segment'),\n  ('4', '1', 'Jacket', 'Segment'),\n  ('5', '2', 'Shirt', 'Segment'),\n  ('6', '2', 'Socks', 'Segment'),\n  ('7', '3', 'Navy Oversized', 'Style'),\n  ('8', '3', 'Black Straight', 'Style'),\n  ('9', '3', 'Cream Relaxed', 'Style'),\n  ('10', '4', 'Khaki Suit', 'Style'),\n  ('11', '4', 'Indigo Rain', 'Style'),\n  ('12', '4', 'Grey Fashion', 'Style'),\n  ('13', '5', 'White Tee', 'Style'),\n  ('14', '5', 'Teal Button Up', 'Style'),\n  ('15', '5', 'Blue Polo', 'Style'),\n  ('16', '6', 'Navy Solid', 'Style'),\n  ('17', '6', 'White Striped', 'Style'),\n  ('18', '6', 'Pink Fluro Polkadot', 'Style');\n\nCREATE TABLE balanced_tree.product_prices (\n  `id` INTEGER,\n  `product_id` VARCHAR(6),\n  `price` INTEGER\n);\n\nINSERT INTO balanced_tree.product_prices\n  (`id`, `product_id`, `price`)\nVALUES\n  ('7', 'c4a632', '13'),\n  ('8', 'e83aa3', '32'),\n  ('9', 'e31d39', '10'),\n  ('10', 'd5e9a6', '23'),\n  ('11', '72f5d4', '19'),\n  ('12', '9ec847', '54'),\n  ('13', '5d267b', '40'),\n  ('14', 'c8d436', '10'),\n  ('15', '2a2353', '57'),\n  ('16', 'f084eb', '36'),\n  ('17', 'b9a74d', '17'),\n  ('18', '2feb6b', '29');\n\nCREATE TABLE balanced_tree.product_details (\n  `product_id` VARCHAR(6),\n  `price` INTEGER,\n  `product_name` VARCHAR(32),\n  `category_id` INTEGER,\n  `segment_id` INTEGER,\n  `style_id` INTEGER,\n  `category_name` VARCHAR(6),\n  `segment_name` VARCHAR(6),\n  `style_name` VARCHAR(19)\n);\n\nINSERT INTO balanced_tree.product_details\n  (`product_id`, `price`, `product_name`, `category_id`, `segment_id`, `style_id`, `category_name`, \n   `segment_name`, `style_name`)\nVALUES\n  ('c4a632', '13', 'Navy Oversized Jeans - Womens', '1', '3', '7', 'Womens', 'Jeans', 'Navy Oversized'),\n  ('e83aa3', '32', 'Black Straight Jeans - Womens', '1', '3', '8', 'Womens', 'Jeans', 'Black Straight'),\n  ('e31d39', '10', 'Cream Relaxed Jeans - Womens', '1', '3', '9', 'Womens', 'Jeans', 'Cream Relaxed'),\n  ('d5e9a6', '23', 'Khaki Suit Jacket - Womens', '1', '4', '10', 'Womens', 'Jacket', 'Khaki Suit'),\n  ('72f5d4', '19', 'Indigo Rain Jacket - Womens', '1', '4', '11', 'Womens', 'Jacket', 'Indigo Rain'),\n  ('9ec847', '54', 'Grey Fashion Jacket - Womens', '1', '4', '12', 'Womens', 'Jacket', 'Grey Fashion'),\n  ('5d267b', '40', 'White Tee Shirt - Mens', '2', '5', '13', 'Mens', 'Shirt', 'White Tee'),\n  ('c8d436', '10', 'Teal Button Up Shirt - Mens', '2', '5', '14', 'Mens', 'Shirt', 'Teal Button Up'),\n  ('2a2353', '57', 'Blue Polo Shirt - Mens', '2', '5', '15', 'Mens', 'Shirt', 'Blue Polo'),\n  ('f084eb', '36', 'Navy Solid Socks - Mens', '2', '6', '16', 'Mens', 'Socks', 'Navy Solid'),\n  ('b9a74d', '17', 'White Striped Socks - Mens', '2', '6', '17', 'Mens', 'Socks', 'White Striped'),\n  ('2feb6b', '29', 'Pink Fluro Polkadot Socks - Mens', '2', '6', '18', 'Mens', 'Socks', 'Pink Fluro Polkadot');\n\nCREATE TABLE balanced_tree.sales (\n  `prod_id` VARCHAR(6),\n  `qty` INTEGER,\n  `price` INTEGER,\n  `discount` INTEGER,\n  `member` VARCHAR(2),\n  `txn_id` VARCHAR(6),\n  `start_txn_time` TIMESTAMP\n);\n\nINSERT INTO balanced_tree.sales\n  (`prod_id`, `qty`, `price`, `discount`, `member`, `txn_id`, `start_txn_time`)\nVALUES\n  ('c4a632', '4', '13', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('5d267b', '4', '40', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('b9a74d', '4', '17', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('2feb6b', '2', '29', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('c4a632', '5', '13', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('e31d39', '2', '10', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('72f5d4', '3', '19', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('2a2353', '3', '57', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('f084eb', '3', '36', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('c4a632', '1', '13', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('e83aa3', '5', '32', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('d5e9a6', '1', '23', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('72f5d4', '1', '19', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('5d267b', '3', '40', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('f084eb', '4', '36', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('b9a74d', '4', '17', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('c4a632', '2', '13', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('e31d39', '5', '10', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('9ec847', '3', '54', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('f084eb', '4', '36', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('2feb6b', '2', '29', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('c4a632', '5', '13', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('e31d39', '4', '10', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('72f5d4', '5', '19', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('f084eb', '2', '36', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('b9a74d', '2', '17', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('e83aa3', '4', '32', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('d5e9a6', '2', '23', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('c8d436', '4', '10', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('2a2353', '1', '57', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('f084eb', '1', '36', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('72f5d4', '3', '19', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('9ec847', '3', '54', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('2a2353', '4', '57', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('f084eb', '2', '36', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('c4a632', '3', '13', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792'),\n  ('e31d39', '3', '10', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792'),\n  ('d5e9a6', '3', '23', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792')\n\n\n\n\nThe following questions can be considered key business questions and metrics that the Balanced Tree team requires for their monthly reports.\nEach question can be answered using a single query - but as you are writing the SQL to solve each individual problem, keep in mind how you would generate all of these metrics in a single SQL script which the Balanced Tree team can run each month.\n\n\n\nWhat was the total quantity sold for all products?\nWhat is the total generated revenue for all products before discounts?\nWhat was the total discount amount for all products?\n\n\n\n\n\nHow many unique transactions were there?\nWhat is the average unique products purchased in each transaction?\nWhat are the 25th, 50th and 75th percentile values for the revenue per transaction?\nWhat is the average discount value per transaction?\nWhat is the percentage split of all transactions for members vs non-members?\nWhat is the average revenue for member transactions and non-member transactions?\n\n\n\n\n\nWhat are the top 3 products by total revenue before discount?\nWhat is the total quantity, revenue and discount for each segment?\nWhat is the top selling product for each segment?\nWhat is the total quantity, revenue and discount for each category?\nWhat is the top selling product for each category?\nWhat is the percentage split of revenue by product for each segment?\nWhat is the percentage split of revenue by segment for each category?\nWhat is the percentage split of total revenue by category?\nWhat is the total transaction “penetration” for each product? (hint: penetration = number of transactions where at least 1 quantity of a product was purchased divided by total number of transactions)\nWhat is the most common combination of at least 1 quantity of any 3 products in a 1 single transaction?\n\n\n\n\nWrite a single SQL script that combines all of the previous questions into a scheduled report that the Balanced Tree team can run at the beginning of each month to calculate the previous month’s values.\nImagine that the Chief Financial Officer (which is also Danny) has asked for all of these questions at the end of every month.\nHe first wants you to generate the data for January only - but then he also wants you to demonstrate that you can easily run the samne analysis for February without many changes (if at all).\nFeel free to split up your final outputs into as many tables as you need - but be sure to explicitly reference which table outputs relate to which question for full marks :)\n\n\n\nUse a single SQL query to transform the product_hierarchy and product_prices datasets to the product_details table.\nHint: you may want to consider using a recursive CTE to solve this problem!\n\n\n\n\n\n\nTotal quantity sold from all the products is given by the following query:\nSELECT\n    SUM(qty) AS total_quantity_sold\nFROM sales;\nOutput:\n\n\n\ntotal_quantity_sold\n\n\n\n\n45216\n\n\n\nTotal quantity sold for all the products is given by the following query:\nSELECT\n    PD.product_name, SUM(S.qty) AS total_quantity_sold\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_quantity_sold DESC;\nOutput:\n\n\n\nproduct_name\ntotal_quantity_sold\n\n\n\n\nGrey Fashion Jacket - Womens\n3876\n\n\nNavy Oversized Jeans - Womens\n3856\n\n\nBlue Polo Shirt - Mens\n3819\n\n\nWhite Tee Shirt - Mens\n3800\n\n\nNavy Solid Socks - Mens\n3792\n\n\nBlack Straight Jeans - Womens\n3786\n\n\nPink Fluro Polkadot Socks - Mens\n3770\n\n\nIndigo Rain Jacket - Womens\n3757\n\n\nKhaki Suit Jacket - Womens\n3752\n\n\nCream Relaxed Jeans - Womens\n3707\n\n\nWhite Striped Socks - Mens\n3655\n\n\nTeal Button Up Shirt - Mens\n3646\n\n\n\n\n\n\nInsights:\n\nThe total quantity sold for all products is 45216 with the Grey Fashion Jacket and Navy Oversized Jeans being the top-selling items.\n\n\n\n\n\n\nTotal revenue before discounts\nSELECT\n    SUM(qty * price) AS total_revenue\nFROM sales;\nOutput:\n\n\n\ntotal_revenue\n\n\n\n\n1289453\n\n\n\nTotal revenue before discounts for all the products\nSELECT\n    PD.product_name, SUM(S.qty * S.price) AS total_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_revenue DESC;\nOutput:\n\n\n\nproduct_name\ntotal_revenue\n\n\n\n\nBlue Polo Shirt - Mens\n217683\n\n\nGrey Fashion Jacket - Womens\n209304\n\n\nWhite Tee Shirt - Mens\n152000\n\n\nNavy Solid Socks - Mens\n136512\n\n\nBlack Straight Jeans - Womens\n121152\n\n\nPink Fluro Polkadot Socks - Mens\n109330\n\n\nKhaki Suit Jacket - Womens\n86296\n\n\nIndigo Rain Jacket - Womens\n71383\n\n\nWhite Striped Socks - Mens\n62135\n\n\nNavy Oversized Jeans - Womens\n50128\n\n\nCream Relaxed Jeans - Womens\n37070\n\n\nTeal Button Up Shirt - Mens\n36460\n\n\n\n\n\n\nInsights:\n\nThe total revenue generated for all products before discounts is $1,289,453.\n\n\n\n\n\n\nTotal discount given is given by\nSELECT\n    ROUND(SUM((qty*price*discount)/100),2) AS total_discount\nFROM sales;\nOutput:\n\n\n\ntotal_discount\n\n\n\n\n156229.14\n\n\n\nTotal discount for all the products is given by\nSELECT\n    PD.product_name, ROUND(SUM((S.qty*S.price*S.discount)/100),2) AS total_discount\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_discount DESC;\nOutput:\n\n\n\nproduct_name\ntotal_discount\n\n\n\n\nBlue Polo Shirt - Mens\n26819.07\n\n\nGrey Fashion Jacket - Womens\n25391.88\n\n\nWhite Tee Shirt - Mens\n18377.60\n\n\nNavy Solid Socks - Mens\n16650.36\n\n\nBlack Straight Jeans - Womens\n14744.96\n\n\nPink Fluro Polkadot Socks - Mens\n12952.27\n\n\nKhaki Suit Jacket - Womens\n10243.05\n\n\nIndigo Rain Jacket - Womens\n8642.53\n\n\nWhite Striped Socks - Mens\n7410.81\n\n\nNavy Oversized Jeans - Womens\n6135.61\n\n\nCream Relaxed Jeans - Womens\n4463.40\n\n\nTeal Button Up Shirt - Mens\n4397.60\n\n\n\n\n\n\nInsights:\n\nThe total discount amount given across all products is $156,229.14.\n\n\n\n\n\n\nSELECT\n    COUNT(DISTINCT txn_id) AS unique_number_of_transactions\nFROM sales;\nOutput:\n\n\n\nunique_number_of_transactions\n\n\n\n\n2500\n\n\n\n\n\n\nInsights:\n\nThe total number of unique transactions recorded is 2,500. Each transaction represents a distinct instance of a purchase made by a customer, providing valuable insights into the volume of sales and customer engagement with Balanced Tree Clothing Company.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(unique_products_purchased),0) AS avg_unique_products_purchased\nFROM (\n    SELECT txn_id,\n        COUNT(DISTINCT prod_id) AS unique_products_purchased\n    FROM sales\n    GROUP BY txn_id\n) AS unique_products_count;\nOutput:\n\n\n\navg_unique_products_purchased\n\n\n\n\n6\n\n\n\n\n\n\nInsights:\n\nThe average number of unique products purchased in each transaction is 6. This metric indicates the diversity of products that customers typically buy in a single transaction, reflecting their preferences and shopping behavior.\n\n\n\n\n\n\nWITH revenue_per_transaction AS (\n    SELECT\n        S.txn_id,\n        SUM(S.qty * S.price) AS total_revenue\n    FROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\n    GROUP BY S.txn_id\n    ORDER BY S.txn_id\n)\nSELECT\n    MAX(CASE WHEN percentile_group = 1 THEN total_revenue END) AS percentile_25,\n    MAX(CASE WHEN percentile_group = 2 THEN total_revenue END) AS percentile_50,\n    MAX(CASE WHEN percentile_group = 3 THEN total_revenue END) AS percentile_75\nFROM (\n    SELECT\n        txn_id,\n        total_revenue,\n        NTILE(4) OVER (ORDER BY total_revenue) as percentile_group\n    FROM revenue_per_transaction\n    ) AS percentile_groups;\nOutput:\n\n\n\npercentile_25\npercentile_50\npercentile_75\n\n\n\n\n375\n509\n647\n\n\n\n\n\n\nInsights:\n\nThese percentile values represent the revenue per transaction at different points in the distribution. For instance, the 25th percentile indicates that 25% of transactions have a revenue of $375 or lower, while the 75th percentile signifies that 75% of transactions have a revenue of $647 or lower. Understanding these values helps in assessing the distribution of revenue and identifying potential areas for improvement or optimization in sales strategies.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(discount_value),1) AS avg_discount_value\nFROM (\n    SELECT\n        txn_id,\n        ROUND(SUM((price * qty * discount)/100),0) AS discount_value\n    FROM sales\n    GROUP BY txn_id\n    ) AS discount_table;\nOutput:\n\n\n\navg_discount_value\n\n\n\n\n62.5\n\n\n\n\n\n\nInsights:\n\nThe Average Discount Value of $62.5, indicates the average discount applied per transaction across all purchases.\n\n\n\n\n\n\nSELECT\n    ROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 't' THEN txn_id ELSE 0 END))\n        /(SELECT COUNT(DISTINCT txn_id)  FROM sales),2) As member_transaction_pct,\n    ROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 'f' THEN txn_id ELSE 0 END))\n        /(SELECT COUNT(DISTINCT txn_id)  FROM sales),2) As non_member_transaction_pct\nFROM sales;\nOutput:\n\n\n\nmember_transaction_pct\nnon_member_transaction_pct\n\n\n\n\n60.24\n39.84\n\n\n\n\n\n\nInsights:\n\nThe data reveals that a significant portion of transactions, approximately 60.24%, is attributed to members, while non-members contribute to about 39.84% of the total transactions. Understanding this split helps in assessing the contribution of membership programs to overall sales and in formulating strategies to attract and retain both member and non-member customers. It highlights the importance of analyzing the behavior and preferences of each segment to tailor marketing and loyalty initiatives effectively.\n\n\n\n\n\n\nWITH member_transactions_cte AS (\n    SELECT\n        member,\n        txn_id,\n        SUM(qty*price) AS avg_revenue\n    FROM sales GROUP BY member,txn_id\n    )\n    SELECT\n        member,\n        ROUND(AVG(avg_revenue),2) AS avg_member_transactions\n    FROM member_transactions_cte\n    GROUP BY member;\nOutput:\n\n\n\nmember\navg_member_transactions\n\n\n\n\nt\n516.27\n\n\nf\n515.04\n\n\n\n\n\n\nInsights:\n\nOn average, member transactions generate slightly higher revenue compared to non-member transactions. This insight underscores the potential value of membership programs in driving higher spending per transaction. It suggests that members may be more inclined to make larger purchases or buy higher-priced items, indicating their engagement and loyalty to the brand. Understanding these differences in average revenue can inform targeted marketing strategies and personalized offerings to enhance customer satisfaction and retention.\n\n\n\n\n\n\nSELECT PD.product_name,\n    SUM(S.price * S.qty) AS total_revenue_before_discount\nFROM product_details AS PD\nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_revenue_before_discount DESC\nLIMIT 3;\nOutput:\n\n\n\nproduct_name\ntotal_revenue_before_discount\n\n\n\n\nBlue Polo Shirt - Mens\n217683\n\n\nGrey Fashion Jacket - Womens\n209304\n\n\nWhite Tee Shirt - Mens\n152000\n\n\n\n\n\n\nInsights:\n\nBlue Polo Shirt - Mens: $217,683\nGrey Fashion Jacket - Womens: $209,304\nWhite Tee Shirt - Mens: $152,000\n\nObservation:\n\nThese top-selling products significantly contribute to the total revenue generated by the company before applying any discounts. Understanding the popularity and revenue performance of these products can help the company focus on effective marketing strategies, optimize inventory management, and identify opportunities for product bundling or upselling to further maximize revenue. Additionally, analyzing customer preferences and purchasing behavior related to these products can provide valuable insights for product development and assortment planning.\n\n\n\n\n\n\nSELECT PD.segment_name,\n    ROUND(SUM(S.qty),2) AS total_quantity,\n    ROUND(SUM(S.qty * S.price),2) AS total_revenue_before_discount,\n    ROUND(SUM((S.qty * S.price * S.discount)/100), 2) AS discount\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.segment_name;\nOutput:\n\n\n\n\n\n\n\n\n\nsegment_name\ntotal_quantity\ntotal_revenue_before_discount\ntotal_revenue_after_discount\n\n\n\n\nJeans\n11349\n208350\n25343.97\n\n\nShirt\n11265\n406143\n49594.27\n\n\nSocks\n11217\n307977\n37013.44\n\n\nJacket\n11385\n366983\n44277.46\n\n\n\n\n\n\nInsights:\n\nJeans Segment:\n\nTotal Quantity: 11,349\nTotal Revenue Before Discount: $208,350\nTotal Discount: $25,343.97\n\nShirt Segment:\n\nTotal Quantity: 11,265\nTotal Revenue Before Discount: $406,143\nTotal Discount: $49,594.27\n\nSocks Segment:\n\nTotal Quantity: 11,217\nTotal Revenue Before Discount: $307,977\nTotal Discount: $37,013.44\n\nJacket Segment:\n\nTotal Quantity: 11,385\nTotal Revenue Before Discount: $366,983\nTotal Discount: $44,277.46\n\n\nObservation:\n\nThese insights provide a breakdown of the total quantity sold, revenue generated before discounts, and total discount offered for each segment of products. Understanding the performance of different segments can help in strategic decision-making, such as allocating resources effectively, optimizing pricing strategies, and identifying areas for improvement or growth within each segment.\n\n\n\n\n\n\nWITH segment_product_qty_sales_cte AS (\n    SELECT\n        PD.segment_name, PD.product_name,\n        SUM(S.qty) AS total_qty_sold\n    FROM product_details AS PD JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.segment_name, PD.product_name\n    ),\n    top_selling_products_cte AS (\n    SELECT\n        segment_product_qty_sales_cte.segment_name,\n        segment_product_qty_sales_cte.product_name,\n        segment_product_qty_sales_cte.total_qty_sold,\n    ROW_NUMBER() OVER (PARTITION BY segment_product_qty_sales_cte.segment_name\n        ORDER BY segment_product_qty_sales_cte.total_qty_sold DESC) AS row_num\n    FROM segment_product_qty_sales_cte\n    )\n    SELECT\n        top_selling_products_cte.segment_name,\n        top_selling_products_cte.product_name,\n        top_selling_products_cte.total_qty_sold\n    FROM top_selling_products_cte\n    WHERE row_num = 1;\nOutput:\n\n\n\nsegment_name\nproduct_name\ntotal_qty_sold\n\n\n\n\nJacket\nGrey Fashion Jacket - Womens\n3876\n\n\nJeans\nNavy Oversized Jeans - Womens\n3856\n\n\nShirt\nBlue Polo Shirt - Mens\n3819\n\n\nSocks\nNavy Solid Socks - Mens\n3792\n\n\n\n\n\n\nInsights:\n\nJacket Segment:\n\nProduct: Grey Fashion Jacket - Womens\nTotal Quantity Sold: 3,876\n\nJeans Segment:\n\nProduct: Navy Oversized Jeans - Womens\nTotal Quantity Sold: 3,856\n\nShirt Segment:\n\nProduct: Blue Polo Shirt - Mens\nTotal Quantity Sold: 3,819\n\nSocks Segment:\n\nNavy Solid Socks - Mens\nTotal Quantity Sold: 3,792\n\n\nObservation:\n\nIdentifying the top-selling products in each segment helps in understanding consumer preferences and market demand within different product categories. This information can be valuable for inventory management, marketing strategies, and product development initiatives to further enhance sales and customer satisfaction.\n\n\n\n\n\n\nSELECT PD.category_name,\n    ROUND(SUM(S.qty),2) AS total_quantity,\n    ROUND(SUM(S.qty * S.price),2) AS total_revenue_before_discount,\n    ROUND(SUM((S.qty * S.price * S.discount)/100), 2) AS discount\nFROM product_details AS PD\nJOIN sales AS S\nON PD.product_id = S.prod_id\nGROUP BY PD.category_name;\nOutput:\n\n\n\n\n\n\n\n\n\ncategory_name\ntotal_quantity\ntotal_revenue_before_discount\ndiscount\n\n\n\n\nWomens\n22734\n575333\n69621.43\n\n\nMens\n22482\n714120\n86607.71\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_product_qty_sales_cte AS (\n    SELECT\n        PD.category_name,\n        PD.product_name,\n        SUM(S.qty) AS total_qty_sold\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name, PD.product_name\n    ),\n    top_selling_products_cte AS (\n    SELECT\n        category_product_qty_sales_cte.category_name,\n        category_product_qty_sales_cte.product_name,\n        category_product_qty_sales_cte.total_qty_sold,\n    ROW_NUMBER() OVER (PARTITION BY category_product_qty_sales_cte.category_name\n        ORDER BY category_product_qty_sales_cte.total_qty_sold DESC) AS row_num\n    FROM category_product_qty_sales_cte\n    )\n    SELECT\n        top_selling_products_cte.category_name,\n        top_selling_products_cte.product_name,\n    top_selling_products_cte.total_qty_sold\n    FROM top_selling_products_cte\n    WHERE row_num = 1;\nOutput:\n\n\n\ncategory_name\nproduct_name\ntotal_qty_sold\n\n\n\n\nMens\nBlue Polo Shirt - Mens\n3819\n\n\nWomens\nGrey Fashion Jacket - Womens\n3876\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH segment_product_revenue_cte AS (\n    SELECT\n        PD.segment_name,\n        PD.product_name,\n        SUM(S.price * S.qty) AS segment_product_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.segment_name, PD.product_name\n    )\n    SELECT\n        segment_name,\n        product_name,\n        ROUND(100.0 * segment_product_revenue / (\n            SUM(segment_product_revenue) OVER\n            (PARTITION BY segment_name)), 2) AS revenue_pct\n    FROM segment_product_revenue_cte\n    ORDER BY segment_name, product_name;\nOutput:\n\n\n\nsegment_name\nproduct_name\nrevenue_pct\n\n\n\n\nJacket\nGrey Fashion Jacket - Womens\n57.03\n\n\nJacket\nIndigo Rain Jacket - Womens\n19.45\n\n\nJacket\nKhaki Suit Jacket - Womens\n23.51\n\n\nJeans\nBlack Straight Jeans - Womens\n58.15\n\n\nJeans\nCream Relaxed Jeans - Womens\n17.79\n\n\nJeans\nNavy Oversized Jeans - Womens\n24.06\n\n\nShirt\nBlue Polo Shirt - Mens\n53.60\n\n\nShirt\nTeal Button Up Shirt - Mens\n8.98\n\n\nShirt\nWhite Tee Shirt - Mens\n37.43\n\n\nSocks\nNavy Solid Socks - Mens\n44.33\n\n\nSocks\nPink Fluro Polkadot Socks - Mens\n35.50\n\n\nSocks\nWhite Striped Socks - Mens\n20.18\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_segment_revenue_cte AS (\n    SELECT\n        PD.category_name,\n        PD.segment_name,\n        SUM(S.price * S.qty) AS category_segment_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name, PD.segment_name\n    )\n    SELECT\n        category_name,\n        segment_name,\n        ROUND(100.0 * category_segment_revenue / (\n            SUM(category_segment_revenue) OVER\n            (PARTITION BY category_name)), 2) AS revenue_pct\n    FROM category_segment_revenue_cte\n    ORDER BY category_name, segment_name;\nOutput:\n\n\n\ncategory_name\nsegment_name\nrevenue_pct\n\n\n\n\nMens\nShirt\n56.87\n\n\nMens\nSocks\n43.13\n\n\nWomens\nJacket\n63.79\n\n\nWomens\nJeans\n36.21\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_revenue_cte AS (\n    SELECT\n        PD.category_name,\n        SUM(S.price * S.qty) AS category_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name\n    )\n    SELECT category_name,\n        ROUND(100.0 * category_revenue / (\n            SUM(category_revenue) OVER()), 2) AS revenue_pct\n    FROM category_revenue_cte\n    GROUP BY category_name\n    ORDER BY category_name;\nOutput:\n\n\n\ncategory_name\nrevenue_pct\n\n\n\n\nMens\n55.38\n\n\nWomens\n44.62\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH product_transactions AS (\n    SELECT\n        PD.product_name,\n        COUNT(DISTINCT S.txn_id) AS product_transactions,\n        (SELECT\n            COUNT(DISTINCT txn_id)\n        FROM sales) AS total_number_of_transactions\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.product_name\n    )\n    SELECT\n        product_name,\n        ROUND(100.0 * (product_transactions/total_number_of_transactions),2) AS product_penetration\n    FROM product_transactions\n    ORDER BY product_penetration DESC;\nOutput:\n\n\n\nproduct_name\nproduct_penetration\n\n\n\n\nNavy Solid Socks - Mens\n51.24\n\n\nGrey Fashion Jacket - Womens\n51.00\n\n\nNavy Oversized Jeans - Womens\n50.96\n\n\nBlue Polo Shirt - Mens\n50.72\n\n\nWhite Tee Shirt - Mens\n50.72\n\n\nPink Fluro Polkadot Socks - Mens\n50.32\n\n\nIndigo Rain Jacket - Womens\n50.00\n\n\nKhaki Suit Jacket - Womens\n49.88\n\n\nBlack Straight Jeans - Womens\n49.84\n\n\nCream Relaxed Jeans - Womens\n49.72\n\n\nWhite Striped Socks - Mens\n49.72\n\n\nTeal Button Up Shirt - Mens\n49.68\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH products_per_transaction AS (\n    SELECT s.txn_id, pd.product_id, pd.product_name, s.qty,\n        COUNT(pd.product_id) OVER (PARTITION BY txn_id) AS cnt\n    FROM sales s\n    JOIN product_details pd ON s.prod_id = pd.product_id\n), combinations AS (\n    SELECT\n        GROUP_CONCAT(product_id ORDER BY product_id) AS product_ids,\n        GROUP_CONCAT(product_name ORDER BY product_id) AS product_names\n    FROM products_per_transaction\n    WHERE cnt = 3\n    GROUP BY txn_id\n), combination_count AS (\n    SELECT product_ids, product_names, COUNT(*) AS common_combinations\n    FROM combinations\n    GROUP BY product_ids, product_names\n) SELECT product_ids, product_names\nFROM combination_count\nWHERE common_combinations = (SELECT MAX(common_combinations) FROM combination_count);\nOutput:\n\n\n\n\n\n\n\nProduct IDs\nProduct Names\n\n\n\n\n5d267b,c4a632,e31d39\nWhite Tee Shirt - Mens, Navy Oversized Jeans - Womens, Cream Relaxed Jeans - Womens\n\n\nb9a74d,c4a632,d5e9a6\nWhite Striped Socks - Mens, Navy Oversized Jeans - Womens, Khaki Suit Jacket - Womens\n\n\n2a2353,2feb6b,c4a632\nBlue Polo Shirt - Mens, Pink Fluro Polkadot Socks - Mens, Navy Oversized Jeans - Womens\n\n\n5d267b,c4a632,e83aa3\nWhite Tee Shirt - Mens, Navy Oversized Jeans - Womens, Black Straight Jeans - Womens\n\n\nc4a632,c8d436,e83aa3\nNavy Oversized Jeans - Womens, Teal Button Up Shirt - Mens, Black Straight Jeans - Womens\n\n\n\n\n\n\nInsights:"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#introduction",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "Balanced Tree Clothing Company prides themselves on providing an optimised range of clothing and lifestyle wear for the modern adventurer!\nDanny, the CEO of this trendy fashion company has asked you to assist the team’s merchandising teams analyse their sales performance and generate a basic financial report to share with the wider business."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#available-data",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#available-data",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "For this case study there is a total of 4 datasets - however you will only need to utilise 2 main tables to solve all of the regular questions, and the additional 2 tables are used only for the bonus challenge question!\n\n\nbalanced_tree.product_details includes all information about the entire range that Balanced Clothing sells in their store.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nprice\nproduct_name\ncategory_id\nsegment_id\nstyle_id\ncategory_name\nsegment_name\nstyle_name\n\n\n\n\nc4a632\n13\nNavy Oversized Jeans - Womens\n1\n3\n7\nWomens\nJeans\nNavy Oversized\n\n\ne83aa3\n32\nBlack Straight Jeans - Womens\n1\n3\n8\nWomens\nJeans\nBlack Straight\n\n\ne31d39\n10\nCream Relaxed Jeans - Womens\n1\n3\n9\nWomens\nJeans\nCream Relaxed\n\n\nd5e9a6\n23\nKhaki Suit Jacket - Womens\n1\n4\n10\nWomens\nJacket\nKhaki Suit\n\n\n72f5d4\n19\nIndigo Rain Jacket - Womens\n1\n4\n11\nWomens\nJacket\nIndigo Rain\n\n\n9ec847\n54\nGrey Fashion Jacket - Womens\n1\n4\n12\nWomens\nJacket\nGrey Fashion\n\n\n5d267b\n40\nWhite Tee Shirt - Mens\n2\n5\n13\nMens\nShirt\nWhite Tee\n\n\nc8d436\n10\nTeal Button Up Shirt - Mens\n2\n5\n14\nMens\nShirt\nTeal Button Up\n\n\n2a2353\n57\nBlue Polo Shirt - Mens\n2\n5\n15\nMens\nShirt\nBlue Polo\n\n\nf084eb\n36\nNavy Solid Socks - Mens\n2\n6\n16\nMens\nSocks\nNavy Solid\n\n\nb9a74d\n17\nWhite Striped Socks - Mens\n2\n6\n17\nMens\nSocks\nWhite Striped\n\n\n2feb6b\n29\nPink Fluro Polkadot Socks - Mens\n2\n6\n18\nMens\nSocks\nPink Fluro Polkadot\n\n\n\n\n\n\nbalanced_tree.sales contains product level information for all the transactions made for Balanced Tree including quantity, price, percentage discount, member status, a transaction ID and also the transaction timestamp.\n\n\n\n\n\n\n\n\n\n\n\n\nprod_id\nqty\nprice\ndiscount\nmember\ntxn_id\nstart_txn_time\n\n\n\n\nc4a632\n4\n13\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n5d267b\n4\n40\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nb9a74d\n4\n17\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n2feb6b\n2\n29\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nc4a632\n5\n13\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\ne31d39\n2\n10\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n72f5d4\n3\n19\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n2a2353\n3\n57\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nf084eb\n3\n36\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nc4a632\n1\n13\n21\nf\nef648d\n2021-01-27 02:18:17.1648\n\n\n\n\n\n\nThes tables are used only for the bonus question where we will use them to recreate the balanced_tree.product_details table.\nbalanced_tree.product_hierarchy\n\n\n\nid\nparent_id\nlevel_text\nlevel_name\n\n\n\n\n1\n\nWomens\nCategory\n\n\n2\n\nMens\nCategory\n\n\n3\n1\nJeans\nSegment\n\n\n4\n1\nJacket\nSegment\n\n\n5\n2\nShirt\nSegment\n\n\n6\n2\nSocks\nSegment\n\n\n7\n3\nNavy Oversized\nStyle\n\n\n8\n3\nBlack Straight\nStyle\n\n\n9\n3\nCream Relaxed\nStyle\n\n\n10\n4\nKhaki Suit\nStyle\n\n\n11\n4\nIndigo Rain\nStyle\n\n\n12\n4\nGrey Fashion\nStyle\n\n\n13\n5\nWhite Tee\nStyle\n\n\n14\n5\nTeal Button Up\nStyle\n\n\n15\n5\nBlue Polo\nStyle\n\n\n16\n6\nNavy Solid\nStyle\n\n\n17\n6\nWhite Striped\nStyle\n\n\n18\n6\nPink Fluro Polkadot\nStyle\n\n\n\nbalanced_tree.product_prices\n\n\n\nid\nproduct_id\nprice\n\n\n\n\n7\nc4a632\n13\n\n\n8\ne83aa3\n32\n\n\n9\ne31d39\n10\n\n\n10\nd5e9a6\n23\n\n\n11\n72f5d4\n19\n\n\n12\n9ec847\n54\n\n\n13\n5d267b\n40\n\n\n14\nc8d436\n10\n\n\n15\n2a2353\n57\n\n\n16\nf084eb\n36\n\n\n17\nb9a74d\n17\n\n\n18\n2feb6b\n29"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#interactive-sql-session",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#interactive-sql-session",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA balanced_tree;\n\nCREATE TABLE balanced_tree.product_hierarchy (\n  `id` INTEGER,\n  `parent_id` INTEGER,\n  `level_text` VARCHAR(19),\n  `level_name` VARCHAR(8)\n);\n\nINSERT INTO balanced_tree.product_hierarchy\n  (`id`, `parent_id`, `level_text`, `level_name`)\nVALUES\n  ('1', NULL, 'Womens', 'Category'),\n  ('2', NULL, 'Mens', 'Category'),\n  ('3', '1', 'Jeans', 'Segment'),\n  ('4', '1', 'Jacket', 'Segment'),\n  ('5', '2', 'Shirt', 'Segment'),\n  ('6', '2', 'Socks', 'Segment'),\n  ('7', '3', 'Navy Oversized', 'Style'),\n  ('8', '3', 'Black Straight', 'Style'),\n  ('9', '3', 'Cream Relaxed', 'Style'),\n  ('10', '4', 'Khaki Suit', 'Style'),\n  ('11', '4', 'Indigo Rain', 'Style'),\n  ('12', '4', 'Grey Fashion', 'Style'),\n  ('13', '5', 'White Tee', 'Style'),\n  ('14', '5', 'Teal Button Up', 'Style'),\n  ('15', '5', 'Blue Polo', 'Style'),\n  ('16', '6', 'Navy Solid', 'Style'),\n  ('17', '6', 'White Striped', 'Style'),\n  ('18', '6', 'Pink Fluro Polkadot', 'Style');\n\nCREATE TABLE balanced_tree.product_prices (\n  `id` INTEGER,\n  `product_id` VARCHAR(6),\n  `price` INTEGER\n);\n\nINSERT INTO balanced_tree.product_prices\n  (`id`, `product_id`, `price`)\nVALUES\n  ('7', 'c4a632', '13'),\n  ('8', 'e83aa3', '32'),\n  ('9', 'e31d39', '10'),\n  ('10', 'd5e9a6', '23'),\n  ('11', '72f5d4', '19'),\n  ('12', '9ec847', '54'),\n  ('13', '5d267b', '40'),\n  ('14', 'c8d436', '10'),\n  ('15', '2a2353', '57'),\n  ('16', 'f084eb', '36'),\n  ('17', 'b9a74d', '17'),\n  ('18', '2feb6b', '29');\n\nCREATE TABLE balanced_tree.product_details (\n  `product_id` VARCHAR(6),\n  `price` INTEGER,\n  `product_name` VARCHAR(32),\n  `category_id` INTEGER,\n  `segment_id` INTEGER,\n  `style_id` INTEGER,\n  `category_name` VARCHAR(6),\n  `segment_name` VARCHAR(6),\n  `style_name` VARCHAR(19)\n);\n\nINSERT INTO balanced_tree.product_details\n  (`product_id`, `price`, `product_name`, `category_id`, `segment_id`, `style_id`, `category_name`, \n   `segment_name`, `style_name`)\nVALUES\n  ('c4a632', '13', 'Navy Oversized Jeans - Womens', '1', '3', '7', 'Womens', 'Jeans', 'Navy Oversized'),\n  ('e83aa3', '32', 'Black Straight Jeans - Womens', '1', '3', '8', 'Womens', 'Jeans', 'Black Straight'),\n  ('e31d39', '10', 'Cream Relaxed Jeans - Womens', '1', '3', '9', 'Womens', 'Jeans', 'Cream Relaxed'),\n  ('d5e9a6', '23', 'Khaki Suit Jacket - Womens', '1', '4', '10', 'Womens', 'Jacket', 'Khaki Suit'),\n  ('72f5d4', '19', 'Indigo Rain Jacket - Womens', '1', '4', '11', 'Womens', 'Jacket', 'Indigo Rain'),\n  ('9ec847', '54', 'Grey Fashion Jacket - Womens', '1', '4', '12', 'Womens', 'Jacket', 'Grey Fashion'),\n  ('5d267b', '40', 'White Tee Shirt - Mens', '2', '5', '13', 'Mens', 'Shirt', 'White Tee'),\n  ('c8d436', '10', 'Teal Button Up Shirt - Mens', '2', '5', '14', 'Mens', 'Shirt', 'Teal Button Up'),\n  ('2a2353', '57', 'Blue Polo Shirt - Mens', '2', '5', '15', 'Mens', 'Shirt', 'Blue Polo'),\n  ('f084eb', '36', 'Navy Solid Socks - Mens', '2', '6', '16', 'Mens', 'Socks', 'Navy Solid'),\n  ('b9a74d', '17', 'White Striped Socks - Mens', '2', '6', '17', 'Mens', 'Socks', 'White Striped'),\n  ('2feb6b', '29', 'Pink Fluro Polkadot Socks - Mens', '2', '6', '18', 'Mens', 'Socks', 'Pink Fluro Polkadot');\n\nCREATE TABLE balanced_tree.sales (\n  `prod_id` VARCHAR(6),\n  `qty` INTEGER,\n  `price` INTEGER,\n  `discount` INTEGER,\n  `member` VARCHAR(2),\n  `txn_id` VARCHAR(6),\n  `start_txn_time` TIMESTAMP\n);\n\nINSERT INTO balanced_tree.sales\n  (`prod_id`, `qty`, `price`, `discount`, `member`, `txn_id`, `start_txn_time`)\nVALUES\n  ('c4a632', '4', '13', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('5d267b', '4', '40', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('b9a74d', '4', '17', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('2feb6b', '2', '29', '17', 't', '54f307', '2021-02-13 01:59:43.296'),\n  ('c4a632', '5', '13', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('e31d39', '2', '10', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('72f5d4', '3', '19', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('2a2353', '3', '57', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('f084eb', '3', '36', '21', 't', '26cc98', '2021-01-19 01:39:00.3456'),\n  ('c4a632', '1', '13', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('e83aa3', '5', '32', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('d5e9a6', '1', '23', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('72f5d4', '1', '19', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('5d267b', '3', '40', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('f084eb', '4', '36', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('b9a74d', '4', '17', '21', 'f', 'ef648d', '2021-01-27 02:18:17.1648'),\n  ('c4a632', '2', '13', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('e31d39', '5', '10', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('9ec847', '3', '54', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('f084eb', '4', '36', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('2feb6b', '2', '29', '23', 't', 'fba96f', '2021-03-03 00:32:56.0544'),\n  ('c4a632', '5', '13', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('e31d39', '4', '10', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('72f5d4', '5', '19', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('f084eb', '2', '36', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('b9a74d', '2', '17', '11', 't', '4e9268', '2021-01-23 14:18:54.0576'),\n  ('e83aa3', '4', '32', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('d5e9a6', '2', '23', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('c8d436', '4', '10', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('2a2353', '1', '57', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('f084eb', '1', '36', '4', 't', '9717d4', '2021-01-29 07:22:13.2672'),\n  ('72f5d4', '3', '19', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('9ec847', '3', '54', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('2a2353', '4', '57', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('f084eb', '2', '36', '14', 'f', 'e9a1dd', '2021-03-28 20:01:43.1328'),\n  ('c4a632', '3', '13', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792'),\n  ('e31d39', '3', '10', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792'),\n  ('d5e9a6', '3', '23', '6', 'f', '003ea6', '2021-01-20 14:21:00.9792')"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#case-study-questions",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "The following questions can be considered key business questions and metrics that the Balanced Tree team requires for their monthly reports.\nEach question can be answered using a single query - but as you are writing the SQL to solve each individual problem, keep in mind how you would generate all of these metrics in a single SQL script which the Balanced Tree team can run each month.\n\n\n\nWhat was the total quantity sold for all products?\nWhat is the total generated revenue for all products before discounts?\nWhat was the total discount amount for all products?\n\n\n\n\n\nHow many unique transactions were there?\nWhat is the average unique products purchased in each transaction?\nWhat are the 25th, 50th and 75th percentile values for the revenue per transaction?\nWhat is the average discount value per transaction?\nWhat is the percentage split of all transactions for members vs non-members?\nWhat is the average revenue for member transactions and non-member transactions?\n\n\n\n\n\nWhat are the top 3 products by total revenue before discount?\nWhat is the total quantity, revenue and discount for each segment?\nWhat is the top selling product for each segment?\nWhat is the total quantity, revenue and discount for each category?\nWhat is the top selling product for each category?\nWhat is the percentage split of revenue by product for each segment?\nWhat is the percentage split of revenue by segment for each category?\nWhat is the percentage split of total revenue by category?\nWhat is the total transaction “penetration” for each product? (hint: penetration = number of transactions where at least 1 quantity of a product was purchased divided by total number of transactions)\nWhat is the most common combination of at least 1 quantity of any 3 products in a 1 single transaction?\n\n\n\n\nWrite a single SQL script that combines all of the previous questions into a scheduled report that the Balanced Tree team can run at the beginning of each month to calculate the previous month’s values.\nImagine that the Chief Financial Officer (which is also Danny) has asked for all of these questions at the end of every month.\nHe first wants you to generate the data for January only - but then he also wants you to demonstrate that you can easily run the samne analysis for February without many changes (if at all).\nFeel free to split up your final outputs into as many tables as you need - but be sure to explicitly reference which table outputs relate to which question for full marks :)\n\n\n\nUse a single SQL query to transform the product_hierarchy and product_prices datasets to the product_details table.\nHint: you may want to consider using a recursive CTE to solve this problem!"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#a.-high-level-sales-analysis-1",
    "href": "portfolio/8WeeksSQLChallenge-Balanced_Tree/index.html#a.-high-level-sales-analysis-1",
    "title": "Case Study #7 - Balanced Tree.",
    "section": "",
    "text": "Total quantity sold from all the products is given by the following query:\nSELECT\n    SUM(qty) AS total_quantity_sold\nFROM sales;\nOutput:\n\n\n\ntotal_quantity_sold\n\n\n\n\n45216\n\n\n\nTotal quantity sold for all the products is given by the following query:\nSELECT\n    PD.product_name, SUM(S.qty) AS total_quantity_sold\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_quantity_sold DESC;\nOutput:\n\n\n\nproduct_name\ntotal_quantity_sold\n\n\n\n\nGrey Fashion Jacket - Womens\n3876\n\n\nNavy Oversized Jeans - Womens\n3856\n\n\nBlue Polo Shirt - Mens\n3819\n\n\nWhite Tee Shirt - Mens\n3800\n\n\nNavy Solid Socks - Mens\n3792\n\n\nBlack Straight Jeans - Womens\n3786\n\n\nPink Fluro Polkadot Socks - Mens\n3770\n\n\nIndigo Rain Jacket - Womens\n3757\n\n\nKhaki Suit Jacket - Womens\n3752\n\n\nCream Relaxed Jeans - Womens\n3707\n\n\nWhite Striped Socks - Mens\n3655\n\n\nTeal Button Up Shirt - Mens\n3646\n\n\n\n\n\n\nInsights:\n\nThe total quantity sold for all products is 45216 with the Grey Fashion Jacket and Navy Oversized Jeans being the top-selling items.\n\n\n\n\n\n\nTotal revenue before discounts\nSELECT\n    SUM(qty * price) AS total_revenue\nFROM sales;\nOutput:\n\n\n\ntotal_revenue\n\n\n\n\n1289453\n\n\n\nTotal revenue before discounts for all the products\nSELECT\n    PD.product_name, SUM(S.qty * S.price) AS total_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_revenue DESC;\nOutput:\n\n\n\nproduct_name\ntotal_revenue\n\n\n\n\nBlue Polo Shirt - Mens\n217683\n\n\nGrey Fashion Jacket - Womens\n209304\n\n\nWhite Tee Shirt - Mens\n152000\n\n\nNavy Solid Socks - Mens\n136512\n\n\nBlack Straight Jeans - Womens\n121152\n\n\nPink Fluro Polkadot Socks - Mens\n109330\n\n\nKhaki Suit Jacket - Womens\n86296\n\n\nIndigo Rain Jacket - Womens\n71383\n\n\nWhite Striped Socks - Mens\n62135\n\n\nNavy Oversized Jeans - Womens\n50128\n\n\nCream Relaxed Jeans - Womens\n37070\n\n\nTeal Button Up Shirt - Mens\n36460\n\n\n\n\n\n\nInsights:\n\nThe total revenue generated for all products before discounts is $1,289,453.\n\n\n\n\n\n\nTotal discount given is given by\nSELECT\n    ROUND(SUM((qty*price*discount)/100),2) AS total_discount\nFROM sales;\nOutput:\n\n\n\ntotal_discount\n\n\n\n\n156229.14\n\n\n\nTotal discount for all the products is given by\nSELECT\n    PD.product_name, ROUND(SUM((S.qty*S.price*S.discount)/100),2) AS total_discount\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_discount DESC;\nOutput:\n\n\n\nproduct_name\ntotal_discount\n\n\n\n\nBlue Polo Shirt - Mens\n26819.07\n\n\nGrey Fashion Jacket - Womens\n25391.88\n\n\nWhite Tee Shirt - Mens\n18377.60\n\n\nNavy Solid Socks - Mens\n16650.36\n\n\nBlack Straight Jeans - Womens\n14744.96\n\n\nPink Fluro Polkadot Socks - Mens\n12952.27\n\n\nKhaki Suit Jacket - Womens\n10243.05\n\n\nIndigo Rain Jacket - Womens\n8642.53\n\n\nWhite Striped Socks - Mens\n7410.81\n\n\nNavy Oversized Jeans - Womens\n6135.61\n\n\nCream Relaxed Jeans - Womens\n4463.40\n\n\nTeal Button Up Shirt - Mens\n4397.60\n\n\n\n\n\n\nInsights:\n\nThe total discount amount given across all products is $156,229.14.\n\n\n\n\n\n\nSELECT\n    COUNT(DISTINCT txn_id) AS unique_number_of_transactions\nFROM sales;\nOutput:\n\n\n\nunique_number_of_transactions\n\n\n\n\n2500\n\n\n\n\n\n\nInsights:\n\nThe total number of unique transactions recorded is 2,500. Each transaction represents a distinct instance of a purchase made by a customer, providing valuable insights into the volume of sales and customer engagement with Balanced Tree Clothing Company.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(unique_products_purchased),0) AS avg_unique_products_purchased\nFROM (\n    SELECT txn_id,\n        COUNT(DISTINCT prod_id) AS unique_products_purchased\n    FROM sales\n    GROUP BY txn_id\n) AS unique_products_count;\nOutput:\n\n\n\navg_unique_products_purchased\n\n\n\n\n6\n\n\n\n\n\n\nInsights:\n\nThe average number of unique products purchased in each transaction is 6. This metric indicates the diversity of products that customers typically buy in a single transaction, reflecting their preferences and shopping behavior.\n\n\n\n\n\n\nWITH revenue_per_transaction AS (\n    SELECT\n        S.txn_id,\n        SUM(S.qty * S.price) AS total_revenue\n    FROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\n    GROUP BY S.txn_id\n    ORDER BY S.txn_id\n)\nSELECT\n    MAX(CASE WHEN percentile_group = 1 THEN total_revenue END) AS percentile_25,\n    MAX(CASE WHEN percentile_group = 2 THEN total_revenue END) AS percentile_50,\n    MAX(CASE WHEN percentile_group = 3 THEN total_revenue END) AS percentile_75\nFROM (\n    SELECT\n        txn_id,\n        total_revenue,\n        NTILE(4) OVER (ORDER BY total_revenue) as percentile_group\n    FROM revenue_per_transaction\n    ) AS percentile_groups;\nOutput:\n\n\n\npercentile_25\npercentile_50\npercentile_75\n\n\n\n\n375\n509\n647\n\n\n\n\n\n\nInsights:\n\nThese percentile values represent the revenue per transaction at different points in the distribution. For instance, the 25th percentile indicates that 25% of transactions have a revenue of $375 or lower, while the 75th percentile signifies that 75% of transactions have a revenue of $647 or lower. Understanding these values helps in assessing the distribution of revenue and identifying potential areas for improvement or optimization in sales strategies.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(discount_value),1) AS avg_discount_value\nFROM (\n    SELECT\n        txn_id,\n        ROUND(SUM((price * qty * discount)/100),0) AS discount_value\n    FROM sales\n    GROUP BY txn_id\n    ) AS discount_table;\nOutput:\n\n\n\navg_discount_value\n\n\n\n\n62.5\n\n\n\n\n\n\nInsights:\n\nThe Average Discount Value of $62.5, indicates the average discount applied per transaction across all purchases.\n\n\n\n\n\n\nSELECT\n    ROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 't' THEN txn_id ELSE 0 END))\n        /(SELECT COUNT(DISTINCT txn_id)  FROM sales),2) As member_transaction_pct,\n    ROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 'f' THEN txn_id ELSE 0 END))\n        /(SELECT COUNT(DISTINCT txn_id)  FROM sales),2) As non_member_transaction_pct\nFROM sales;\nOutput:\n\n\n\nmember_transaction_pct\nnon_member_transaction_pct\n\n\n\n\n60.24\n39.84\n\n\n\n\n\n\nInsights:\n\nThe data reveals that a significant portion of transactions, approximately 60.24%, is attributed to members, while non-members contribute to about 39.84% of the total transactions. Understanding this split helps in assessing the contribution of membership programs to overall sales and in formulating strategies to attract and retain both member and non-member customers. It highlights the importance of analyzing the behavior and preferences of each segment to tailor marketing and loyalty initiatives effectively.\n\n\n\n\n\n\nWITH member_transactions_cte AS (\n    SELECT\n        member,\n        txn_id,\n        SUM(qty*price) AS avg_revenue\n    FROM sales GROUP BY member,txn_id\n    )\n    SELECT\n        member,\n        ROUND(AVG(avg_revenue),2) AS avg_member_transactions\n    FROM member_transactions_cte\n    GROUP BY member;\nOutput:\n\n\n\nmember\navg_member_transactions\n\n\n\n\nt\n516.27\n\n\nf\n515.04\n\n\n\n\n\n\nInsights:\n\nOn average, member transactions generate slightly higher revenue compared to non-member transactions. This insight underscores the potential value of membership programs in driving higher spending per transaction. It suggests that members may be more inclined to make larger purchases or buy higher-priced items, indicating their engagement and loyalty to the brand. Understanding these differences in average revenue can inform targeted marketing strategies and personalized offerings to enhance customer satisfaction and retention.\n\n\n\n\n\n\nSELECT PD.product_name,\n    SUM(S.price * S.qty) AS total_revenue_before_discount\nFROM product_details AS PD\nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_revenue_before_discount DESC\nLIMIT 3;\nOutput:\n\n\n\nproduct_name\ntotal_revenue_before_discount\n\n\n\n\nBlue Polo Shirt - Mens\n217683\n\n\nGrey Fashion Jacket - Womens\n209304\n\n\nWhite Tee Shirt - Mens\n152000\n\n\n\n\n\n\nInsights:\n\nBlue Polo Shirt - Mens: $217,683\nGrey Fashion Jacket - Womens: $209,304\nWhite Tee Shirt - Mens: $152,000\n\nObservation:\n\nThese top-selling products significantly contribute to the total revenue generated by the company before applying any discounts. Understanding the popularity and revenue performance of these products can help the company focus on effective marketing strategies, optimize inventory management, and identify opportunities for product bundling or upselling to further maximize revenue. Additionally, analyzing customer preferences and purchasing behavior related to these products can provide valuable insights for product development and assortment planning.\n\n\n\n\n\n\nSELECT PD.segment_name,\n    ROUND(SUM(S.qty),2) AS total_quantity,\n    ROUND(SUM(S.qty * S.price),2) AS total_revenue_before_discount,\n    ROUND(SUM((S.qty * S.price * S.discount)/100), 2) AS discount\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.segment_name;\nOutput:\n\n\n\n\n\n\n\n\n\nsegment_name\ntotal_quantity\ntotal_revenue_before_discount\ntotal_revenue_after_discount\n\n\n\n\nJeans\n11349\n208350\n25343.97\n\n\nShirt\n11265\n406143\n49594.27\n\n\nSocks\n11217\n307977\n37013.44\n\n\nJacket\n11385\n366983\n44277.46\n\n\n\n\n\n\nInsights:\n\nJeans Segment:\n\nTotal Quantity: 11,349\nTotal Revenue Before Discount: $208,350\nTotal Discount: $25,343.97\n\nShirt Segment:\n\nTotal Quantity: 11,265\nTotal Revenue Before Discount: $406,143\nTotal Discount: $49,594.27\n\nSocks Segment:\n\nTotal Quantity: 11,217\nTotal Revenue Before Discount: $307,977\nTotal Discount: $37,013.44\n\nJacket Segment:\n\nTotal Quantity: 11,385\nTotal Revenue Before Discount: $366,983\nTotal Discount: $44,277.46\n\n\nObservation:\n\nThese insights provide a breakdown of the total quantity sold, revenue generated before discounts, and total discount offered for each segment of products. Understanding the performance of different segments can help in strategic decision-making, such as allocating resources effectively, optimizing pricing strategies, and identifying areas for improvement or growth within each segment.\n\n\n\n\n\n\nWITH segment_product_qty_sales_cte AS (\n    SELECT\n        PD.segment_name, PD.product_name,\n        SUM(S.qty) AS total_qty_sold\n    FROM product_details AS PD JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.segment_name, PD.product_name\n    ),\n    top_selling_products_cte AS (\n    SELECT\n        segment_product_qty_sales_cte.segment_name,\n        segment_product_qty_sales_cte.product_name,\n        segment_product_qty_sales_cte.total_qty_sold,\n    ROW_NUMBER() OVER (PARTITION BY segment_product_qty_sales_cte.segment_name\n        ORDER BY segment_product_qty_sales_cte.total_qty_sold DESC) AS row_num\n    FROM segment_product_qty_sales_cte\n    )\n    SELECT\n        top_selling_products_cte.segment_name,\n        top_selling_products_cte.product_name,\n        top_selling_products_cte.total_qty_sold\n    FROM top_selling_products_cte\n    WHERE row_num = 1;\nOutput:\n\n\n\nsegment_name\nproduct_name\ntotal_qty_sold\n\n\n\n\nJacket\nGrey Fashion Jacket - Womens\n3876\n\n\nJeans\nNavy Oversized Jeans - Womens\n3856\n\n\nShirt\nBlue Polo Shirt - Mens\n3819\n\n\nSocks\nNavy Solid Socks - Mens\n3792\n\n\n\n\n\n\nInsights:\n\nJacket Segment:\n\nProduct: Grey Fashion Jacket - Womens\nTotal Quantity Sold: 3,876\n\nJeans Segment:\n\nProduct: Navy Oversized Jeans - Womens\nTotal Quantity Sold: 3,856\n\nShirt Segment:\n\nProduct: Blue Polo Shirt - Mens\nTotal Quantity Sold: 3,819\n\nSocks Segment:\n\nNavy Solid Socks - Mens\nTotal Quantity Sold: 3,792\n\n\nObservation:\n\nIdentifying the top-selling products in each segment helps in understanding consumer preferences and market demand within different product categories. This information can be valuable for inventory management, marketing strategies, and product development initiatives to further enhance sales and customer satisfaction.\n\n\n\n\n\n\nSELECT PD.category_name,\n    ROUND(SUM(S.qty),2) AS total_quantity,\n    ROUND(SUM(S.qty * S.price),2) AS total_revenue_before_discount,\n    ROUND(SUM((S.qty * S.price * S.discount)/100), 2) AS discount\nFROM product_details AS PD\nJOIN sales AS S\nON PD.product_id = S.prod_id\nGROUP BY PD.category_name;\nOutput:\n\n\n\n\n\n\n\n\n\ncategory_name\ntotal_quantity\ntotal_revenue_before_discount\ndiscount\n\n\n\n\nWomens\n22734\n575333\n69621.43\n\n\nMens\n22482\n714120\n86607.71\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_product_qty_sales_cte AS (\n    SELECT\n        PD.category_name,\n        PD.product_name,\n        SUM(S.qty) AS total_qty_sold\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name, PD.product_name\n    ),\n    top_selling_products_cte AS (\n    SELECT\n        category_product_qty_sales_cte.category_name,\n        category_product_qty_sales_cte.product_name,\n        category_product_qty_sales_cte.total_qty_sold,\n    ROW_NUMBER() OVER (PARTITION BY category_product_qty_sales_cte.category_name\n        ORDER BY category_product_qty_sales_cte.total_qty_sold DESC) AS row_num\n    FROM category_product_qty_sales_cte\n    )\n    SELECT\n        top_selling_products_cte.category_name,\n        top_selling_products_cte.product_name,\n    top_selling_products_cte.total_qty_sold\n    FROM top_selling_products_cte\n    WHERE row_num = 1;\nOutput:\n\n\n\ncategory_name\nproduct_name\ntotal_qty_sold\n\n\n\n\nMens\nBlue Polo Shirt - Mens\n3819\n\n\nWomens\nGrey Fashion Jacket - Womens\n3876\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH segment_product_revenue_cte AS (\n    SELECT\n        PD.segment_name,\n        PD.product_name,\n        SUM(S.price * S.qty) AS segment_product_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.segment_name, PD.product_name\n    )\n    SELECT\n        segment_name,\n        product_name,\n        ROUND(100.0 * segment_product_revenue / (\n            SUM(segment_product_revenue) OVER\n            (PARTITION BY segment_name)), 2) AS revenue_pct\n    FROM segment_product_revenue_cte\n    ORDER BY segment_name, product_name;\nOutput:\n\n\n\nsegment_name\nproduct_name\nrevenue_pct\n\n\n\n\nJacket\nGrey Fashion Jacket - Womens\n57.03\n\n\nJacket\nIndigo Rain Jacket - Womens\n19.45\n\n\nJacket\nKhaki Suit Jacket - Womens\n23.51\n\n\nJeans\nBlack Straight Jeans - Womens\n58.15\n\n\nJeans\nCream Relaxed Jeans - Womens\n17.79\n\n\nJeans\nNavy Oversized Jeans - Womens\n24.06\n\n\nShirt\nBlue Polo Shirt - Mens\n53.60\n\n\nShirt\nTeal Button Up Shirt - Mens\n8.98\n\n\nShirt\nWhite Tee Shirt - Mens\n37.43\n\n\nSocks\nNavy Solid Socks - Mens\n44.33\n\n\nSocks\nPink Fluro Polkadot Socks - Mens\n35.50\n\n\nSocks\nWhite Striped Socks - Mens\n20.18\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_segment_revenue_cte AS (\n    SELECT\n        PD.category_name,\n        PD.segment_name,\n        SUM(S.price * S.qty) AS category_segment_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name, PD.segment_name\n    )\n    SELECT\n        category_name,\n        segment_name,\n        ROUND(100.0 * category_segment_revenue / (\n            SUM(category_segment_revenue) OVER\n            (PARTITION BY category_name)), 2) AS revenue_pct\n    FROM category_segment_revenue_cte\n    ORDER BY category_name, segment_name;\nOutput:\n\n\n\ncategory_name\nsegment_name\nrevenue_pct\n\n\n\n\nMens\nShirt\n56.87\n\n\nMens\nSocks\n43.13\n\n\nWomens\nJacket\n63.79\n\n\nWomens\nJeans\n36.21\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH category_revenue_cte AS (\n    SELECT\n        PD.category_name,\n        SUM(S.price * S.qty) AS category_revenue\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.category_name\n    )\n    SELECT category_name,\n        ROUND(100.0 * category_revenue / (\n            SUM(category_revenue) OVER()), 2) AS revenue_pct\n    FROM category_revenue_cte\n    GROUP BY category_name\n    ORDER BY category_name;\nOutput:\n\n\n\ncategory_name\nrevenue_pct\n\n\n\n\nMens\n55.38\n\n\nWomens\n44.62\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH product_transactions AS (\n    SELECT\n        PD.product_name,\n        COUNT(DISTINCT S.txn_id) AS product_transactions,\n        (SELECT\n            COUNT(DISTINCT txn_id)\n        FROM sales) AS total_number_of_transactions\n    FROM product_details AS PD\n    JOIN sales AS S\n    ON PD.product_id = S.prod_id\n    GROUP BY PD.product_name\n    )\n    SELECT\n        product_name,\n        ROUND(100.0 * (product_transactions/total_number_of_transactions),2) AS product_penetration\n    FROM product_transactions\n    ORDER BY product_penetration DESC;\nOutput:\n\n\n\nproduct_name\nproduct_penetration\n\n\n\n\nNavy Solid Socks - Mens\n51.24\n\n\nGrey Fashion Jacket - Womens\n51.00\n\n\nNavy Oversized Jeans - Womens\n50.96\n\n\nBlue Polo Shirt - Mens\n50.72\n\n\nWhite Tee Shirt - Mens\n50.72\n\n\nPink Fluro Polkadot Socks - Mens\n50.32\n\n\nIndigo Rain Jacket - Womens\n50.00\n\n\nKhaki Suit Jacket - Womens\n49.88\n\n\nBlack Straight Jeans - Womens\n49.84\n\n\nCream Relaxed Jeans - Womens\n49.72\n\n\nWhite Striped Socks - Mens\n49.72\n\n\nTeal Button Up Shirt - Mens\n49.68\n\n\n\n\n\n\nInsights:\n\n\n\n\n\n\n\n\nWITH products_per_transaction AS (\n    SELECT s.txn_id, pd.product_id, pd.product_name, s.qty,\n        COUNT(pd.product_id) OVER (PARTITION BY txn_id) AS cnt\n    FROM sales s\n    JOIN product_details pd ON s.prod_id = pd.product_id\n), combinations AS (\n    SELECT\n        GROUP_CONCAT(product_id ORDER BY product_id) AS product_ids,\n        GROUP_CONCAT(product_name ORDER BY product_id) AS product_names\n    FROM products_per_transaction\n    WHERE cnt = 3\n    GROUP BY txn_id\n), combination_count AS (\n    SELECT product_ids, product_names, COUNT(*) AS common_combinations\n    FROM combinations\n    GROUP BY product_ids, product_names\n) SELECT product_ids, product_names\nFROM combination_count\nWHERE common_combinations = (SELECT MAX(common_combinations) FROM combination_count);\nOutput:\n\n\n\n\n\n\n\nProduct IDs\nProduct Names\n\n\n\n\n5d267b,c4a632,e31d39\nWhite Tee Shirt - Mens, Navy Oversized Jeans - Womens, Cream Relaxed Jeans - Womens\n\n\nb9a74d,c4a632,d5e9a6\nWhite Striped Socks - Mens, Navy Oversized Jeans - Womens, Khaki Suit Jacket - Womens\n\n\n2a2353,2feb6b,c4a632\nBlue Polo Shirt - Mens, Pink Fluro Polkadot Socks - Mens, Navy Oversized Jeans - Womens\n\n\n5d267b,c4a632,e83aa3\nWhite Tee Shirt - Mens, Navy Oversized Jeans - Womens, Black Straight Jeans - Womens\n\n\nc4a632,c8d436,e83aa3\nNavy Oversized Jeans - Womens, Teal Button Up Shirt - Mens, Black Straight Jeans - Womens\n\n\n\n\n\n\nInsights:"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "Clique Bait is not like your regular online seafood store - the founder and CEO Danny, was also a part of a digital data analytics team and wanted to expand his knowledge into the seafood industry!\nIn this case study - you are required to support Danny’s vision and analyse his dataset and come up with creative solutions to calculate funnel fallout rates for the Clique Bait online store.\n\n\n\nFor this case study there is a total of 5 datasets which you will need to combine to solve all of the questions.\n\n\nCustomers who visit the Clique Bait website are tagged via their cookie_id.\n\n\n\nuser_id\ncookie_id\nstart_date\n\n\n\n\n397\n3759ff\n2020-03-30 00:00:00\n\n\n215\n863329\n2020-01-26 00:00:00\n\n\n191\neefca9\n2020-03-15 00:00:00\n\n\n89\n764796\n2020-01-07 00:00:00\n\n\n127\n17ccc5\n2020-01-22 00:00:00\n\n\n81\nb0b666\n2020-03-01 00:00:00\n\n\n260\na4f236\n2020-01-08 00:00:00\n\n\n203\nd1182f\n2020-04-18 00:00:00\n\n\n23\n12dbc8\n2020-01-18 00:00:00\n\n\n375\nf61d69\n2020-01-03 00:00:00\n\n\n\n\n\n\nCustomer visits are logged in this events table at a cookie_id level and the event_type and page_id values can be used to join onto relevant satellite tables to obtain further information about each event.\nThe sequence_number is used to order the events within each visit.\n\n\n\n\n\n\n\n\n\n\n\nvisit_id\ncookie_id\npage_id\nevent_type\nsequence_number\nevent_time\n\n\n\n\n719fd3\n3d83d3\n5\n1\n4\n2020-03-02 00:29:09.975502\n\n\nfb1eb1\nc5ff25\n5\n2\n8\n2020-01-22 07:59:16.761931\n\n\n23fe81\n1e8c2d\n10\n1\n9\n2020-03-21 13:14:11.745667\n\n\nad91aa\n648115\n6\n1\n3\n2020-04-27 16:28:09.824606\n\n\n5576d7\nac418c\n6\n1\n4\n2020-01-18 04:55:10.149236\n\n\n48308b\nc686c1\n8\n1\n5\n2020-01-29 06:10:38.702163\n\n\n46b17d\n78f9b3\n7\n1\n12\n2020-02-16 09:45:31.926407\n\n\n9fd196\nccf057\n4\n1\n5\n2020-02-14 08:29:12.922164\n\n\nedf853\nf85454\n1\n1\n1\n2020-02-22 12:59:07.652207\n\n\n3c6716\n02e74f\n3\n2\n5\n2020-01-31 17:56:20.777383\n\n\n\n\n\n\nThe event_identifier table shows the types of events which are captured by Clique Bait’s digital data systems.\n\n\n\nevent_type\nevent_name\n\n\n\n\n1\nPage View\n\n\n2\nAdd to Cart\n\n\n3\nPurchase\n\n\n4\nAd Impression\n\n\n5\nAd Click\n\n\n\n\n\n\nThis table shows information for the 3 campaigns that Clique Bait has ran on their website so far in 2020.\n\n\n\n\n\n\n\n\n\n\ncampaign_id\nproducts\ncampaign_name\nstart_date\nend_date\n\n\n\n\n1\n1-3\nBOGOF - Fishing For Compliments\n2020-01-01 00:00:00\n2020-01-14 00:00:00\n\n\n2\n4-5\n25% Off - Living The Lux Life\n2020-01-15 00:00:00\n2020-01-28 00:00:00\n\n\n3\n6-8\nHalf Off - Treat Your Shellf(ish)\n2020-02-01 00:00:00\n2020-03-31 00:00:00\n\n\n\n\n\n\nThis table lists all of the pages on the Clique Bait website which are tagged and have data passing through from user interaction events.\n\n\n\npage_id\npage_name\nproduct_category\nproduct_id\n\n\n\n\n1\nHome Page\nnull\nnull\n\n\n2\nAll Products\nnull\nnull\n\n\n3\nSalmon\nFish\n1\n\n\n4\nKingfish\nFish\n2\n\n\n5\nTuna\nFish\n3\n\n\n6\nRussian Caviar\nLuxury\n4\n\n\n7\nBlack Truffle\nLuxury\n5\n\n\n8\nAbalone\nShellfish\n6\n\n\n9\nLobster\nShellfish\n7\n\n\n10\nCrab\nShellfish\n8\n\n\n11\nOyster\nShellfish\n9\n\n\n12\nCheckout\nnull\nnull\n\n\n13\nConfirmation\nnull\nnull\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA clique_bait;\n\nCREATE TABLE clique_bait.event_identifier (\n  `event_type` INTEGER,\n  `event_name` VARCHAR(13)\n);\n\nINSERT INTO clique_bait.event_identifier\n  (`event_type`, `event_name`)\nVALUES\n  ('1', 'Page View'),\n  ('2', 'Add to Cart'),\n  ('3', 'Purchase'),\n  ('4', 'Ad Impression'),\n  ('5', 'Ad Click');\n\nCREATE TABLE clique_bait.campaign_identifier (\n  `campaign_id` INTEGER,\n  `products` VARCHAR(3),\n  `campaign_name` VARCHAR(33),\n  `start_date` TIMESTAMP,\n  `end_date` TIMESTAMP\n);\n\nINSERT INTO clique_bait.campaign_identifier\n  (`campaign_id`, `products`, `campaign_name`, `start_date`, `end_date`)\nVALUES\n  ('1', '1-3', 'BOGOF - Fishing For Compliments', '2020-01-01', '2020-01-14'),\n  ('2', '4-5', '25% Off - Living The Lux Life', '2020-01-15', '2020-01-28'),\n  ('3', '6-8', 'Half Off - Treat Your Shellf(ish)', '2020-02-01', '2020-03-31');\n\nCREATE TABLE clique_bait.page_hierarchy (\n  `page_id` INTEGER,\n  `page_name` VARCHAR(14),\n  `product_category` VARCHAR(9),\n  `product_id` INTEGER\n);\n\nINSERT INTO clique_bait.page_hierarchy\n  (`page_id`, `page_name`, `product_category`, `product_id`)\nVALUES\n  ('1', 'Home Page', null, null),\n  ('2', 'All Products', null, null),\n  ('3', 'Salmon', 'Fish', '1'),\n  ('4', 'Kingfish', 'Fish', '2'),\n  ('5', 'Tuna', 'Fish', '3'),\n  ('6', 'Russian Caviar', 'Luxury', '4'),\n  ('7', 'Black Truffle', 'Luxury', '5'),\n  ('8', 'Abalone', 'Shellfish', '6'),\n  ('9', 'Lobster', 'Shellfish', '7'),\n  ('10', 'Crab', 'Shellfish', '8'),\n  ('11', 'Oyster', 'Shellfish', '9'),\n  ('12', 'Checkout', null, null),\n  ('13', 'Confirmation', null, null);\n\nCREATE TABLE clique_bait.users (\n  `user_id` INTEGER,\n  `cookie_id` VARCHAR(6),\n  `start_date` TIMESTAMP\n);\n\nINSERT INTO clique_bait.users\n  (`user_id`, `cookie_id`, `start_date`)\nVALUES\n  ('1', 'c4ca42', '2020-02-04'),\n  ('2', 'c81e72', '2020-01-18'),\n  ('3', 'eccbc8', '2020-02-21'),\n  ('4', 'a87ff6', '2020-02-22'),\n  ('5', 'e4da3b', '2020-02-01'),\n  ('6', '167909', '2020-01-25'),\n  ('7', '8f14e4', '2020-02-09'),\n  ('8', 'c9f0f8', '2020-02-12'),\n  ('9', '45c48c', '2020-02-07'),\n  ('10', 'd3d944', '2020-01-23'),\n  ('11', '6512bd', '2020-01-17'),\n  ('12', 'c20ad4', '2020-02-06'),\n  ('13', 'c51ce4', '2020-02-12'),\n  ('14', 'aab323', '2020-01-12'),\n  ('15', '9bf31c', '2020-01-28'),\n  ('16', 'c74d97', '2020-01-06'),\n  ('17', '70efdf', '2020-02-17'),\n  ('18', '6f4922', '2020-02-29'),\n  ('19', '1f0e3d', '2020-02-11'),\n  ('20', '98f137', '2020-02-12'),\n  ('21', '3c59dc', '2020-02-14'),\n  ('22', 'b6d767', '2020-02-08'),\n  ('23', '37693c', '2020-01-16')\n  \nCREATE TABLE clique_bait.events (\n  `visit_id` VARCHAR(6),\n  `cookie_id` VARCHAR(6),\n  `page_id` INTEGER,\n  `event_type` INTEGER,\n  `sequence_number` INTEGER,\n  `event_time` TIMESTAMP\n);\n\nINSERT INTO clique_bait.events\n  (`visit_id`, `cookie_id`, `page_id`, `event_type`, `sequence_number`, `event_time`)\nVALUES\n  ('ccf365', 'c4ca42', '1', '1', '1', '2020-02-04 19:16:09.182546'),\n  ('ccf365', 'c4ca42', '2', '1', '2', '2020-02-04 19:16:17.358191'),\n  ('ccf365', 'c4ca42', '6', '1', '3', '2020-02-04 19:16:58.454669'),\n  ('ccf365', 'c4ca42', '9', '1', '4', '2020-02-04 19:16:58.609142'),\n  ('ccf365', 'c4ca42', '9', '2', '5', '2020-02-04 19:17:51.72942'),\n  ('ccf365', 'c4ca42', '10', '1', '6', '2020-02-04 19:18:11.605815'),\n  ('ccf365', 'c4ca42', '10', '2', '7', '2020-02-04 19:19:10.570786'),\n  ('ccf365', 'c4ca42', '11', '1', '8', '2020-02-04 19:19:46.911728'),\n  ('ccf365', 'c4ca42', '11', '2', '9', '2020-02-04 19:20:45.27469'),\n  ('ccf365', 'c4ca42', '12', '1', '10', '2020-02-04 19:20:52.307244'),\n  ('ccf365', 'c4ca42', '13', '3', '11', '2020-02-04 19:21:26.242563'),\n  ('d58cbd', 'c81e72', '1', '1', '1', '2020-01-18 23:40:54.761906'),\n  ('d58cbd', 'c81e72', '2', '1', '2', '2020-01-18 23:41:06.391027'),\n  ('d58cbd', 'c81e72', '4', '1', '3', '2020-01-18 23:42:02.213001'),\n  ('d58cbd', 'c81e72', '4', '2', '4', '2020-01-18 23:42:02.370046'),\n  ('d58cbd', 'c81e72', '5', '1', '5', '2020-01-18 23:42:44.717024'),\n  ('d58cbd', 'c81e72', '5', '2', '6', '2020-01-18 23:43:11.121855'),\n  ('d58cbd', 'c81e72', '7', '1', '7', '2020-01-18 23:43:25.806239'),\n  ('d58cbd', 'c81e72', '8', '1', '8', '2020-01-18 23:43:40.537995'),\n  ('d58cbd', 'c81e72', '8', '2', '9', '2020-01-18 23:44:14.026393'),\n  ('d58cbd', 'c81e72', '10', '1', '10', '2020-01-18 23:44:22.103768'),\n  ('d58cbd', 'c81e72', '10', '2', '11', '2020-01-18 23:45:00.004781'),\n  ('d58cbd', 'c81e72', '12', '1', '12', '2020-01-18 23:45:38.186554'),\n  ('9a2f24', 'eccbc8', '1', '1', '1', '2020-02-21 03:19:10.032455'),\n  ('9a2f24', 'eccbc8', '4', '1', '2', '2020-02-21 03:19:24.677901'),\n  ('9a2f24', 'eccbc8', '4', '2', '3', '2020-02-21 03:19:48.146489'),\n  ('9a2f24', 'eccbc8', '7', '1', '4', '2020-02-21 03:20:13.39183'),\n  ('9a2f24', 'eccbc8', '7', '2', '5', '2020-02-21 03:20:13.869733'),\n  ('9a2f24', 'eccbc8', '10', '1', '6', '2020-02-21 03:20:45.854556'),\n  ('9a2f24', 'eccbc8', '11', '1', '7', '2020-02-21 03:21:20.335104'),\n  ('9a2f24', 'eccbc8', '12', '1', '8', '2020-02-21 03:21:43.262109'),\n  ('9a2f24', 'eccbc8', '13', '3', '9', '2020-02-21 03:22:22.501245'),\n  ('7caba5', 'a87ff6', '1', '1', '1', '2020-02-22 17:49:37.646174'),\n  ('7caba5', 'a87ff6', '4', '1', '2', '2020-02-22 17:50:23.736729'),\n  ('7caba5', 'a87ff6', '5', '1', '3', '2020-02-22 17:50:26.878153')  \n\n\n\n\n\nUsing the following DDL schema details to create an ERD for all the Clique Bait datasets. Click_Here to access the DB Diagram tool to create the ERD.\n\n\n\n\nUsing the available datasets - answer the following questions using a single query for each one:\n\nHow many users are there?\nHow many cookies does each user have on average?\nWhat is the unique number of visits by all users per month?\nWhat is the number of events for each event type?\nWhat is the percentage of visits which have a purchase event?\nWhat is the percentage of visits which view the checkout page but do not have a purchase event?\nWhat are the top 3 pages by number of views?\nWhat is the number of views and cart adds for each product category?\nWhat are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nUse your 2 new output tables - answer the following questions:\n\nWhich product had the most views, cart adds and purchases?\nWhich product was most likely to be abandoned?\nWhich product had the highest view to purchase percentage?\nWhat is the average conversion rate from view to cart add?\nWhat is the average conversion rate from cart add to purchase?\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?\n\n\n\n\n\n\n\nSELECT\n    COUNT(DISTINCT user_id) AS number_of_unique_users\nFROM users;\nOutput:\n\n\n\nnumber_of_unique_users\n\n\n\n\n500\n\n\n\n\n\n\nInsight:\n\nThe dataset contains information about 500 unique users who have visited the Clique Bait website.\n\n\n\n\n\n\nWITH cookie AS (\nSELECT user_id,\n        COUNT(cookie_id) AS cookie_count\nFROM users GROUP BY user_id\n)\nSELECT ROUND(AVG(cookie_count),0) AS average_cookie_per_user\nFROM cookie;\nOutput:\n\n\n\naverage_cookie_per_user\n\n\n\n\n4\n\n\n\n\n\n\nInsight:\n\nOn average, each user has approximately 4 cookies associated with their user account.\n\n\n\n\n\n\nSELECT\n    MONTH(event_time) AS 'month',\n    COUNT(DISTINCT visit_id) AS customer_count\nFROM events\nGROUP BY MONTH(event_time);\nOutput:\n\n\n\nmonth\ncustomer_count\n\n\n\n\n1\n876\n\n\n2\n1488\n\n\n3\n916\n\n\n4\n248\n\n\n5\n36\n\n\n\n\n\n\nInsights:\n\nSeasonal Trends: The data shows fluctuations in customer visits across different months, indicating potential seasonal patterns or shifts in user behavior.\nPeak Months: February witnessed the highest number of unique visits, suggesting increased activity or interest during that period.\nMonthly Variability: There is variability in customer engagement across months, with some months experiencing higher or lower visit counts compared to others.\n\n\n\n\n\n\nSELECT EI.event_name,\n    COUNT(E.event_time) AS number_of_events\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nGROUP BY EI.event_name\nORDER BY number_of_events DESC;\nOutput:\n\n\n\nevent_name\nnumber_of_events\n\n\n\n\nPage View\n20928\n\n\nAdd to Cart\n8451\n\n\nPurchase\n1777\n\n\nAd Impression\n876\n\n\nAd Click\n702\n\n\n\n\n\n\nInsights:\n\nEvent Distribution: Page views constitute the majority of events, indicating high user engagement with various pages on the website.\nConversion Actions: While page views are common, fewer users proceed to add items to their cart or make purchases, as evidenced by the lower counts for “Add to Cart” and “Purchase” events.\nMarketing Engagement: The number of ad impressions and ad clicks suggests user interaction with advertising content, which can provide insights into the effectiveness of marketing campaigns.\n\n\n\n\n\n\nSELECT\n    ROUND(100.0 * COUNT(DISTINCT E.visit_id) / (SELECT COUNT(DISTINCT visit_id) FROM events),2)\n    AS vists_percentage\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nWHERE EI.event_name = 'Purchase';\nOutput:\n\n\n\nvisits_percentage\n\n\n\n\n49.86\n\n\n\n\n\n\nInsights:\n\nPurchase Rate: Approximately 49.86% of visits result in a purchase event, indicating a moderate conversion rate.\nConversion Performance: Understanding the proportion of visits that lead to purchases provides insights into the effectiveness of the website in driving sales.\nPotential Growth Opportunities: Identifying areas for improvement in the conversion funnel to increase the purchase rate and enhance overall revenue generation.\n\n\n\n\n\n\nWITH view_purchase AS (\nSELECT\n    COUNT(E.visit_id) AS visit_count\nFROM events AS E JOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.page_name = 'Checkout' and EI.event_name = 'Page View')\nSELECT\n    ROUND(100 - (100.0 * COUNT(DISTINCT E.visit_id) /\n        (SELECT visit_count FROM view_purchase)),2) AS pct_of_checkout_visits_not_purchased\nFROM events AS E JOIN event_identifier AS EI ON E.event_type = EI.event_type\nWHERE EI.event_name = 'Purchase';\nOutput:\n\n\n\npct_of_checkout_visits_not_purchased\n\n\n\n\n15.50\n\n\n\n\n\n\nInsights:\n\nCheckout Abandonment: Approximately 15.50% of visits proceed to the checkout page but do not culminate in a purchase, indicating a significant dropout rate.\nPotential Revenue Loss: Identifying and addressing factors contributing to checkout abandonment is crucial to mitigate potential revenue loss and maximize conversion rates.\nUser Experience Evaluation: Analyzing the user experience at the checkout stage, including ease of navigation, payment options, and shipping information, can provide insights into areas for improvement.\n\n\n\n\n\n\nWITH top_3_pages AS\n(SELECT\n    page_id, COUNT(DISTINCT visit_id) AS number_of_views\nFROM events\nWHERE event_type = '1'\nGROUP BY page_id\nORDER BY number_of_views DESC\nLIMIT 3\n)\nSELECT\n    page_name, number_of_views\nFROM top_3_pages\nJOIN page_hierarchy ON top_3_pages.page_id = page_hierarchy.page_id;\nOutput:\n\n\n\npage_name\nnumber_of_views\n\n\n\n\nHome Page\n1782\n\n\nAll Products\n3174\n\n\nCheckout\n2103\n\n\n\n\n\n\nInsights:\n\nUser Engagement: The Home Page, All Products, and Checkout pages are pivotal in user navigation, as evidenced by their high view counts.\nBrowsing Behavior: Users frequently visit the All Products page, indicating a strong interest in exploring available products or services.\nCheckout Process: The significant number of views on the Checkout page underscores its importance in the conversion journey, suggesting a substantial portion of users progress towards completing transactions.\n\n\n\n\n\n\nSELECT PH.product_category,\n       SUM(CASE WHEN E.event_type = '1' THEN 1 ELSE 0 END) AS number_of_views,\n       SUM(CASE WHEN E.event_type = '2' THEN 1 ELSE 0 END) AS number_of_cart_adds\nFROM events as E\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.product_category IS NOT NULL\nGROUP BY PH.product_category\nORDER BY PH.product_category;\nOutput:\n\n\n\nproduct_category\nnumber_of_views\nnumber_of_cart_adds\n\n\n\n\nFish\n4633\n2789\n\n\nLuxury\n3032\n1870\n\n\nShellfish\n6204\n3792\n\n\n\n\n\n\nInsights:\n\nPopular Categories: Shellfish has the highest number of views and cart additions, followed by Fish and Luxury categories.\nEngagement Discrepancy: Despite Fish having fewer views compared to Shellfish, it has a relatively higher cart addition rate, indicating stronger user intent or interest in purchasing Fish products.\nConversion Opportunities: Analyzing user behavior within each category can help identify opportunities to optimize product pages, pricing strategies, or promotional efforts to drive conversions.\n\n\n\n\n\n\nSELECT\n    PH.product_id,\n    PH.page_name,\n    PH.product_category,\n    COUNT(PH.product_id) AS product_count\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON PH.page_id = E.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id IN\n    (SELECT E.visit_id FROM events as E\n     JOIN event_identifier AS EI ON E.event_type = EI.event_type WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_id, PH.page_name, PH.product_category\nORDER BY product_count DESC\nLIMIT 3;\nOutput:\n\n\n\nproduct_id\nproduct_name\nproduct_category\nproduct_count\n\n\n\n\n7\nLobster\nShellfish\n754\n\n\n9\nOyster\nShellfish\n726\n\n\n8\nCrab\nShellfish\n719\n\n\n\n\n\n\nInsights:\n\nShellfish Dominance: All top 3 products belong to the Shellfish category, indicating its popularity among customers.\nHigh Demand Items: Lobster, Oyster, and Crab are evidently high-demand items within the Shellfish category, likely due to factors such as taste, availability, and pricing.\nCross-Promotion Opportunities: Identifying complementary products or bundle offers with these top-selling items can help increase average order value and enhance customer satisfaction.\n\n\n\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\n\n\nCreating a Temporary table view_add_to_cart\nCREATE TEMPORARY TABLE view_add_to_cart AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM\n    events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE\n    PH.product_category IS NOT NULL\nGROUP BY\n    PH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table products_abandoned\nCREATE TEMPORARY TABLE products_abandoned AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    COUNT(*) AS abandoned\nFROM\n    events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE\n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase'\n    )\nGROUP BY\n    PH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table products_purchased\nCREATE TEMPORARY TABLE products_purchased AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    COUNT(*) AS purchased\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase')\nGROUP BY\nPH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table product_information that combines all the above tables created above.\nCREATE TEMPORARY TABLE product_information AS\nSELECT\n    VATC.*,\n    AB.abandoned,\n    PP.purchased\nFROM\nview_add_to_cart AS VATC\nJOIN products_abandoned AS AB ON VATC.product_id = AB.product_id\nJOIN products_purchased AS PP ON VATC.product_id = PP.product_id;\nDropping the created temporary tables, since they are not required anymore.\nDROP TEMPORARY TABLE IF EXISTS view_add_to_cart, products_abandoned, products_purchased;\nDisplaying the Final resulting table product_information records..\nSELECT * FROM product_information\nORDER BY product_id;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n1\nSalmon\nFish\n1559\n938\n227\n711\n\n\n2\nKingfish\nFish\n1559\n920\n213\n707\n\n\n3\nTuna\nFish\n1515\n931\n234\n697\n\n\n4\nRussian Caviar\nLuxury\n1563\n946\n249\n697\n\n\n5\nBlack Truffle\nLuxury\n1469\n924\n217\n707\n\n\n6\nAbalone\nShellfish\n1525\n932\n233\n699\n\n\n7\nLobster\nShellfish\n1547\n968\n214\n754\n\n\n8\nCrab\nShellfish\n1564\n949\n230\n719\n\n\n9\nOyster\nShellfish\n1568\n943\n217\n726\n\n\n\n\n\n\nCreating a Temporary table category_view_add_to_cart\nCREATE TEMPORARY TABLE category_view_add_to_cart AS\nSELECT\n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.product_category IS NOT NULL\nGROUP BY PH.product_category;\nCreating a Temporary table category_products_abandoned\nCREATE TEMPORARY TABLE category_products_abandoned AS\nSELECT\n    PH.product_category,\n    COUNT(*) AS abandoned\n    FROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id NOT IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_category;\nCreating a Temporary table category_products_purchased\nCREATE TEMPORARY TABLE category_products_purchased AS\nSELECT\n    PH.product_category,\n    COUNT(*) AS purchased\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart'\nAND E.visit_id IN (SELECT E.visit_id\n    FROM events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_category;\nCreating a Temporary table category_product_information that combines all the above tables created above.\nCREATE TEMPORARY TABLE category_product_information AS\nSELECT\n    VATC.*, AB.abandoned, PP.purchased\nFROM category_view_add_to_cart AS VATC\nJOIN category_products_abandoned AS AB ON VATC.product_category = AB.product_category\nJOIN category_products_purchased AS PP ON VATC.product_category = PP.product_category;\nDrop the temporary tables, since they are not needed anymore\nDROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart, category_products_abandoned, category_products_purchased;\nDisplaying the final resulting category_product_information table records\nSELECT *\nFROM category_product_information\nORDER BY product_category;\nOutput:\n\n\n\n\n\n\n\n\n\n\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\nLuxury\n3032\n1870\n466\n1404\n\n\nFish\n4633\n2789\n674\n2115\n\n\nShellfish\n6204\n3792\n894\n2898\n\n\n\n\n\n\nSELECT *\nFROM product_information\nORDER BY view_counts DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n9\nOyster\nShellfish\n1568\n943\n217\n726\n\n\n\nSELECT *\nFROM product_information\nORDER BY add_to_cart_counts DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n7\nLobster\nShellfish\n1547\n\n\n\n\n\n\nSELECT *\nFROM product_information\nORDER BY purchased DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n7\nLobster\nShellfish\n1547\n968\n214\n754\n\n\n\n\n\n\nInsights:\n\nShellfish Dominance: Both the product with the most views and the product with the most cart adds and purchases belong to the Shellfish category, indicating its popularity among customers.\nLobster Dominance: The product with the most cart adds and purchases is Lobster, suggesting that it is not only popular but also highly sought-after for purchase among customers.\nOyster Engagement: Although Oyster has the most views, it has a relatively lower number of cart adds and purchases compared to Lobster, indicating potential areas for improvement in conversion rate or marketing strategies.\n\n\n\n\n\n\nSELECT * FROM product_information\nORDER BY abandoned DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n4\nRussian Caviar\nLuxury\n1563\n946\n249\n697\n\n\n\n\n\n\nInsights:\n\nLuxury Product: Russian Caviar falls under the Luxury category, which may imply a higher price point or more exclusive nature compared to other products.\nHigh Abandonment Rate: The relatively high number of abandonments suggests that customers might have shown interest in the product but ultimately decided not to proceed with the purchase.\nPotential Improvements: Analyzing the reasons behind abandonment, such as pricing concerns, shipping costs, or checkout process issues, could provide insights into areas for improvement to reduce abandonment rates and increase conversions. Additionally, targeted marketing or promotional strategies could be employed to encourage customers to complete their purchase of Russian Caviar.\n\n\n\n\n\n\nSELECT product_name,\n    ROUND(100.0 * (purchased/view_counts),2) AS purchase_to_view_pct\nFROM product_information\nORDER BY purchase_to_view_pct DESC\nLIMIT 1;\nOutput:\n\n\n\nproduct_name\npurchase_to_view_pct\n\n\n\n\nLobster\n48.74\n\n\n\n\n\n\nInsights:\n\nHigh Conversion Rate: The high purchase-to-view percentage indicates that a significant portion of customers who viewed the Lobster product ultimately made a purchase.\nAppealing Product: Lobster seems to be particularly appealing to customers, leading to a relatively high conversion rate compared to other products.\nMarket Demand: The high conversion rate may suggest strong market demand for Lobster, potentially due to factors such as its taste, quality, or perceived value.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(100.0 * (add_to_cart_counts/view_counts)),2) AS avg_conversion_rate\nFROM product_information;\nOutput:\n\n\n\navg_conversion_rate\n\n\n\n\n60.95\n\n\n\n\n\n\nInsights:\n\nConversion Funnel Efficiency: The high average conversion rate indicates that a significant proportion of customers who view products proceed to add them to their cart.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(100.0 * (purchased/add_to_cart_counts)),2) AS avg_conversion_rate\nFROM product_information;\nOutput:\n\n\n\navg_conversion_rate\n\n\n\n\n75.93\n\n\n\n\n\n\nInsights:\n\nConversion Funnel Efficiency: This high average conversion rate indicates that a significant proportion of customers who add products to their cart ultimately proceed to make a purchase.\n\n\n\n\n\n\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number).\nSELECT\n    U.user_id, E.visit_id, MIN(E.event_time) AS visit_start_date, C.campaign_name,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS page_view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts,\n    SUM(CASE WHEN EI.event_name = 'Purchase' THEN 1 ELSE 0 END) AS purchased_counts,\n    SUM(CASE WHEN EI.event_name = 'Ad Impression' THEN 1 ELSE 0 END) AS impression_counts,\n    SUM(CASE WHEN EI.event_name = 'Ad Click' THEN 1 ELSE 0 END) AS click_counts\nFROM users AS U\nJOIN events AS E ON U.cookie_id = E.cookie_id\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN campaign_identifier AS C ON E.event_time BETWEEN C.start_date AND C.end_date\nGROUP BY U.user_id, E.visit_id, C.campaign_name;\nOutput:\nFirst 20 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nvisit_id\nvisit_start_date\ncampaign_name\npage_view_counts\nadd_to_cart_counts\npurchased\nimpression_counts\nclick_counts\n\n\n\n\n1\nccf365\n2020-02-04 19:16:09\nHalf Off - Treat Your Shellf(ish)\n7\n3\n1\n0\n0\n\n\n2\nd58cbd\n2020-01-18 23:40:55\n25% Off - Living The Lux Life\n8\n4\n0\n0\n0\n\n\n3\n9a2f24\n2020-02-21 03:19:10\nHalf Off - Treat Your Shellf(ish)\n6\n2\n1\n0\n0\n\n\n4\n7caba5\n2020-02-22 17:49:38\nHalf Off - Treat Your Shellf(ish)\n5\n2\n0\n0\n0\n\n\n5\nf61ed7\n2020-02-01 06:30:40\nHalf Off - Treat Your Shellf(ish)\n8\n2\n1\n0\n0\n\n\n6\ne0ce49\n2020-01-25 22:43:21\n25% Off - Living The Lux Life\n9\n3\n1\n0\n0\n\n\n7\n8479c1\n2020-02-09 17:27:59\nHalf Off - Treat Your Shellf(ish)\n5\n1\n1\n0\n0\n\n\n8\na6c424\n2020-02-12 11:23:55\nHalf Off - Treat Your Shellf(ish)\n7\n2\n0\n0\n0\n\n\n9\n5ef346\n2020-02-07 17:32:45\nHalf Off - Treat Your Shellf(ish)\n7\n0\n0\n0\n0\n\n\n10\nd39d35\n2020-01-23 21:47:04\n25% Off - Living The Lux Life\n7\n3\n1\n0\n0\n\n\n11\n9c2633\n2020-01-17 04:59:43\n25% Off - Living The Lux Life\n8\n2\n0\n0\n0\n\n\n12\nd69e73\n2020-02-06 09:09:06\nHalf Off - Treat Your Shellf(ish)\n5\n1\n0\n0\n0\n\n\n13\nc70085\n2020-02-12 08:26:14\nHalf Off - Treat Your Shellf(ish)\n6\n1\n0\n0\n0\n\n\n14\n6a20a3\n2020-01-12 02:49:32\nBOGOF - Fishing For Compliments\n8\n1\n0\n0\n0\n\n\n16\n69440b\n2020-01-06 21:45:51\nBOGOF - Fishing For Compliments\n4\n1\n1\n0\n0\n\n\n17\ne70fd5\n2020-02-17 10:05:51\nHalf Off - Treat Your Shellf(ish)\n7\n2\n0\n0\n0\n\n\n18\n48810d\n2020-02-29 15:26:41\nHalf Off - Treat Your Shellf(ish)\n4\n0\n0\n0\n0\n\n\n19\nfdf383\n2020-02-11 13:52:24\nHalf Off - Treat Your Shellf(ish)\n7\n1\n1\n0\n0\n\n\n20\n378a75\n2020-02-12 23:33:51\nHalf Off - Treat Your Shellf(ish)\n4\n0\n0\n0\n0"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#introduction",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "Clique Bait is not like your regular online seafood store - the founder and CEO Danny, was also a part of a digital data analytics team and wanted to expand his knowledge into the seafood industry!\nIn this case study - you are required to support Danny’s vision and analyse his dataset and come up with creative solutions to calculate funnel fallout rates for the Clique Bait online store."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#available-data",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#available-data",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "For this case study there is a total of 5 datasets which you will need to combine to solve all of the questions.\n\n\nCustomers who visit the Clique Bait website are tagged via their cookie_id.\n\n\n\nuser_id\ncookie_id\nstart_date\n\n\n\n\n397\n3759ff\n2020-03-30 00:00:00\n\n\n215\n863329\n2020-01-26 00:00:00\n\n\n191\neefca9\n2020-03-15 00:00:00\n\n\n89\n764796\n2020-01-07 00:00:00\n\n\n127\n17ccc5\n2020-01-22 00:00:00\n\n\n81\nb0b666\n2020-03-01 00:00:00\n\n\n260\na4f236\n2020-01-08 00:00:00\n\n\n203\nd1182f\n2020-04-18 00:00:00\n\n\n23\n12dbc8\n2020-01-18 00:00:00\n\n\n375\nf61d69\n2020-01-03 00:00:00\n\n\n\n\n\n\nCustomer visits are logged in this events table at a cookie_id level and the event_type and page_id values can be used to join onto relevant satellite tables to obtain further information about each event.\nThe sequence_number is used to order the events within each visit.\n\n\n\n\n\n\n\n\n\n\n\nvisit_id\ncookie_id\npage_id\nevent_type\nsequence_number\nevent_time\n\n\n\n\n719fd3\n3d83d3\n5\n1\n4\n2020-03-02 00:29:09.975502\n\n\nfb1eb1\nc5ff25\n5\n2\n8\n2020-01-22 07:59:16.761931\n\n\n23fe81\n1e8c2d\n10\n1\n9\n2020-03-21 13:14:11.745667\n\n\nad91aa\n648115\n6\n1\n3\n2020-04-27 16:28:09.824606\n\n\n5576d7\nac418c\n6\n1\n4\n2020-01-18 04:55:10.149236\n\n\n48308b\nc686c1\n8\n1\n5\n2020-01-29 06:10:38.702163\n\n\n46b17d\n78f9b3\n7\n1\n12\n2020-02-16 09:45:31.926407\n\n\n9fd196\nccf057\n4\n1\n5\n2020-02-14 08:29:12.922164\n\n\nedf853\nf85454\n1\n1\n1\n2020-02-22 12:59:07.652207\n\n\n3c6716\n02e74f\n3\n2\n5\n2020-01-31 17:56:20.777383\n\n\n\n\n\n\nThe event_identifier table shows the types of events which are captured by Clique Bait’s digital data systems.\n\n\n\nevent_type\nevent_name\n\n\n\n\n1\nPage View\n\n\n2\nAdd to Cart\n\n\n3\nPurchase\n\n\n4\nAd Impression\n\n\n5\nAd Click\n\n\n\n\n\n\nThis table shows information for the 3 campaigns that Clique Bait has ran on their website so far in 2020.\n\n\n\n\n\n\n\n\n\n\ncampaign_id\nproducts\ncampaign_name\nstart_date\nend_date\n\n\n\n\n1\n1-3\nBOGOF - Fishing For Compliments\n2020-01-01 00:00:00\n2020-01-14 00:00:00\n\n\n2\n4-5\n25% Off - Living The Lux Life\n2020-01-15 00:00:00\n2020-01-28 00:00:00\n\n\n3\n6-8\nHalf Off - Treat Your Shellf(ish)\n2020-02-01 00:00:00\n2020-03-31 00:00:00\n\n\n\n\n\n\nThis table lists all of the pages on the Clique Bait website which are tagged and have data passing through from user interaction events.\n\n\n\npage_id\npage_name\nproduct_category\nproduct_id\n\n\n\n\n1\nHome Page\nnull\nnull\n\n\n2\nAll Products\nnull\nnull\n\n\n3\nSalmon\nFish\n1\n\n\n4\nKingfish\nFish\n2\n\n\n5\nTuna\nFish\n3\n\n\n6\nRussian Caviar\nLuxury\n4\n\n\n7\nBlack Truffle\nLuxury\n5\n\n\n8\nAbalone\nShellfish\n6\n\n\n9\nLobster\nShellfish\n7\n\n\n10\nCrab\nShellfish\n8\n\n\n11\nOyster\nShellfish\n9\n\n\n12\nCheckout\nnull\nnull\n\n\n13\nConfirmation\nnull\nnull"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#interactive-sql-instance",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#interactive-sql-instance",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA clique_bait;\n\nCREATE TABLE clique_bait.event_identifier (\n  `event_type` INTEGER,\n  `event_name` VARCHAR(13)\n);\n\nINSERT INTO clique_bait.event_identifier\n  (`event_type`, `event_name`)\nVALUES\n  ('1', 'Page View'),\n  ('2', 'Add to Cart'),\n  ('3', 'Purchase'),\n  ('4', 'Ad Impression'),\n  ('5', 'Ad Click');\n\nCREATE TABLE clique_bait.campaign_identifier (\n  `campaign_id` INTEGER,\n  `products` VARCHAR(3),\n  `campaign_name` VARCHAR(33),\n  `start_date` TIMESTAMP,\n  `end_date` TIMESTAMP\n);\n\nINSERT INTO clique_bait.campaign_identifier\n  (`campaign_id`, `products`, `campaign_name`, `start_date`, `end_date`)\nVALUES\n  ('1', '1-3', 'BOGOF - Fishing For Compliments', '2020-01-01', '2020-01-14'),\n  ('2', '4-5', '25% Off - Living The Lux Life', '2020-01-15', '2020-01-28'),\n  ('3', '6-8', 'Half Off - Treat Your Shellf(ish)', '2020-02-01', '2020-03-31');\n\nCREATE TABLE clique_bait.page_hierarchy (\n  `page_id` INTEGER,\n  `page_name` VARCHAR(14),\n  `product_category` VARCHAR(9),\n  `product_id` INTEGER\n);\n\nINSERT INTO clique_bait.page_hierarchy\n  (`page_id`, `page_name`, `product_category`, `product_id`)\nVALUES\n  ('1', 'Home Page', null, null),\n  ('2', 'All Products', null, null),\n  ('3', 'Salmon', 'Fish', '1'),\n  ('4', 'Kingfish', 'Fish', '2'),\n  ('5', 'Tuna', 'Fish', '3'),\n  ('6', 'Russian Caviar', 'Luxury', '4'),\n  ('7', 'Black Truffle', 'Luxury', '5'),\n  ('8', 'Abalone', 'Shellfish', '6'),\n  ('9', 'Lobster', 'Shellfish', '7'),\n  ('10', 'Crab', 'Shellfish', '8'),\n  ('11', 'Oyster', 'Shellfish', '9'),\n  ('12', 'Checkout', null, null),\n  ('13', 'Confirmation', null, null);\n\nCREATE TABLE clique_bait.users (\n  `user_id` INTEGER,\n  `cookie_id` VARCHAR(6),\n  `start_date` TIMESTAMP\n);\n\nINSERT INTO clique_bait.users\n  (`user_id`, `cookie_id`, `start_date`)\nVALUES\n  ('1', 'c4ca42', '2020-02-04'),\n  ('2', 'c81e72', '2020-01-18'),\n  ('3', 'eccbc8', '2020-02-21'),\n  ('4', 'a87ff6', '2020-02-22'),\n  ('5', 'e4da3b', '2020-02-01'),\n  ('6', '167909', '2020-01-25'),\n  ('7', '8f14e4', '2020-02-09'),\n  ('8', 'c9f0f8', '2020-02-12'),\n  ('9', '45c48c', '2020-02-07'),\n  ('10', 'd3d944', '2020-01-23'),\n  ('11', '6512bd', '2020-01-17'),\n  ('12', 'c20ad4', '2020-02-06'),\n  ('13', 'c51ce4', '2020-02-12'),\n  ('14', 'aab323', '2020-01-12'),\n  ('15', '9bf31c', '2020-01-28'),\n  ('16', 'c74d97', '2020-01-06'),\n  ('17', '70efdf', '2020-02-17'),\n  ('18', '6f4922', '2020-02-29'),\n  ('19', '1f0e3d', '2020-02-11'),\n  ('20', '98f137', '2020-02-12'),\n  ('21', '3c59dc', '2020-02-14'),\n  ('22', 'b6d767', '2020-02-08'),\n  ('23', '37693c', '2020-01-16')\n  \nCREATE TABLE clique_bait.events (\n  `visit_id` VARCHAR(6),\n  `cookie_id` VARCHAR(6),\n  `page_id` INTEGER,\n  `event_type` INTEGER,\n  `sequence_number` INTEGER,\n  `event_time` TIMESTAMP\n);\n\nINSERT INTO clique_bait.events\n  (`visit_id`, `cookie_id`, `page_id`, `event_type`, `sequence_number`, `event_time`)\nVALUES\n  ('ccf365', 'c4ca42', '1', '1', '1', '2020-02-04 19:16:09.182546'),\n  ('ccf365', 'c4ca42', '2', '1', '2', '2020-02-04 19:16:17.358191'),\n  ('ccf365', 'c4ca42', '6', '1', '3', '2020-02-04 19:16:58.454669'),\n  ('ccf365', 'c4ca42', '9', '1', '4', '2020-02-04 19:16:58.609142'),\n  ('ccf365', 'c4ca42', '9', '2', '5', '2020-02-04 19:17:51.72942'),\n  ('ccf365', 'c4ca42', '10', '1', '6', '2020-02-04 19:18:11.605815'),\n  ('ccf365', 'c4ca42', '10', '2', '7', '2020-02-04 19:19:10.570786'),\n  ('ccf365', 'c4ca42', '11', '1', '8', '2020-02-04 19:19:46.911728'),\n  ('ccf365', 'c4ca42', '11', '2', '9', '2020-02-04 19:20:45.27469'),\n  ('ccf365', 'c4ca42', '12', '1', '10', '2020-02-04 19:20:52.307244'),\n  ('ccf365', 'c4ca42', '13', '3', '11', '2020-02-04 19:21:26.242563'),\n  ('d58cbd', 'c81e72', '1', '1', '1', '2020-01-18 23:40:54.761906'),\n  ('d58cbd', 'c81e72', '2', '1', '2', '2020-01-18 23:41:06.391027'),\n  ('d58cbd', 'c81e72', '4', '1', '3', '2020-01-18 23:42:02.213001'),\n  ('d58cbd', 'c81e72', '4', '2', '4', '2020-01-18 23:42:02.370046'),\n  ('d58cbd', 'c81e72', '5', '1', '5', '2020-01-18 23:42:44.717024'),\n  ('d58cbd', 'c81e72', '5', '2', '6', '2020-01-18 23:43:11.121855'),\n  ('d58cbd', 'c81e72', '7', '1', '7', '2020-01-18 23:43:25.806239'),\n  ('d58cbd', 'c81e72', '8', '1', '8', '2020-01-18 23:43:40.537995'),\n  ('d58cbd', 'c81e72', '8', '2', '9', '2020-01-18 23:44:14.026393'),\n  ('d58cbd', 'c81e72', '10', '1', '10', '2020-01-18 23:44:22.103768'),\n  ('d58cbd', 'c81e72', '10', '2', '11', '2020-01-18 23:45:00.004781'),\n  ('d58cbd', 'c81e72', '12', '1', '12', '2020-01-18 23:45:38.186554'),\n  ('9a2f24', 'eccbc8', '1', '1', '1', '2020-02-21 03:19:10.032455'),\n  ('9a2f24', 'eccbc8', '4', '1', '2', '2020-02-21 03:19:24.677901'),\n  ('9a2f24', 'eccbc8', '4', '2', '3', '2020-02-21 03:19:48.146489'),\n  ('9a2f24', 'eccbc8', '7', '1', '4', '2020-02-21 03:20:13.39183'),\n  ('9a2f24', 'eccbc8', '7', '2', '5', '2020-02-21 03:20:13.869733'),\n  ('9a2f24', 'eccbc8', '10', '1', '6', '2020-02-21 03:20:45.854556'),\n  ('9a2f24', 'eccbc8', '11', '1', '7', '2020-02-21 03:21:20.335104'),\n  ('9a2f24', 'eccbc8', '12', '1', '8', '2020-02-21 03:21:43.262109'),\n  ('9a2f24', 'eccbc8', '13', '3', '9', '2020-02-21 03:22:22.501245'),\n  ('7caba5', 'a87ff6', '1', '1', '1', '2020-02-22 17:49:37.646174'),\n  ('7caba5', 'a87ff6', '4', '1', '2', '2020-02-22 17:50:23.736729'),\n  ('7caba5', 'a87ff6', '5', '1', '3', '2020-02-22 17:50:26.878153')"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#case-study-questions",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "Using the following DDL schema details to create an ERD for all the Clique Bait datasets. Click_Here to access the DB Diagram tool to create the ERD.\n\n\n\n\nUsing the available datasets - answer the following questions using a single query for each one:\n\nHow many users are there?\nHow many cookies does each user have on average?\nWhat is the unique number of visits by all users per month?\nWhat is the number of events for each event type?\nWhat is the percentage of visits which have a purchase event?\nWhat is the percentage of visits which view the checkout page but do not have a purchase event?\nWhat are the top 3 pages by number of views?\nWhat is the number of views and cart adds for each product category?\nWhat are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nUse your 2 new output tables - answer the following questions:\n\nWhich product had the most views, cart adds and purchases?\nWhich product was most likely to be abandoned?\nWhich product had the highest view to purchase percentage?\nWhat is the average conversion rate from view to cart add?\nWhat is the average conversion rate from cart add to purchase?\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#digital-analysis-1",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#digital-analysis-1",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "SELECT\n    COUNT(DISTINCT user_id) AS number_of_unique_users\nFROM users;\nOutput:\n\n\n\nnumber_of_unique_users\n\n\n\n\n500\n\n\n\n\n\n\nInsight:\n\nThe dataset contains information about 500 unique users who have visited the Clique Bait website.\n\n\n\n\n\n\nWITH cookie AS (\nSELECT user_id,\n        COUNT(cookie_id) AS cookie_count\nFROM users GROUP BY user_id\n)\nSELECT ROUND(AVG(cookie_count),0) AS average_cookie_per_user\nFROM cookie;\nOutput:\n\n\n\naverage_cookie_per_user\n\n\n\n\n4\n\n\n\n\n\n\nInsight:\n\nOn average, each user has approximately 4 cookies associated with their user account.\n\n\n\n\n\n\nSELECT\n    MONTH(event_time) AS 'month',\n    COUNT(DISTINCT visit_id) AS customer_count\nFROM events\nGROUP BY MONTH(event_time);\nOutput:\n\n\n\nmonth\ncustomer_count\n\n\n\n\n1\n876\n\n\n2\n1488\n\n\n3\n916\n\n\n4\n248\n\n\n5\n36\n\n\n\n\n\n\nInsights:\n\nSeasonal Trends: The data shows fluctuations in customer visits across different months, indicating potential seasonal patterns or shifts in user behavior.\nPeak Months: February witnessed the highest number of unique visits, suggesting increased activity or interest during that period.\nMonthly Variability: There is variability in customer engagement across months, with some months experiencing higher or lower visit counts compared to others.\n\n\n\n\n\n\nSELECT EI.event_name,\n    COUNT(E.event_time) AS number_of_events\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nGROUP BY EI.event_name\nORDER BY number_of_events DESC;\nOutput:\n\n\n\nevent_name\nnumber_of_events\n\n\n\n\nPage View\n20928\n\n\nAdd to Cart\n8451\n\n\nPurchase\n1777\n\n\nAd Impression\n876\n\n\nAd Click\n702\n\n\n\n\n\n\nInsights:\n\nEvent Distribution: Page views constitute the majority of events, indicating high user engagement with various pages on the website.\nConversion Actions: While page views are common, fewer users proceed to add items to their cart or make purchases, as evidenced by the lower counts for “Add to Cart” and “Purchase” events.\nMarketing Engagement: The number of ad impressions and ad clicks suggests user interaction with advertising content, which can provide insights into the effectiveness of marketing campaigns.\n\n\n\n\n\n\nSELECT\n    ROUND(100.0 * COUNT(DISTINCT E.visit_id) / (SELECT COUNT(DISTINCT visit_id) FROM events),2)\n    AS vists_percentage\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nWHERE EI.event_name = 'Purchase';\nOutput:\n\n\n\nvisits_percentage\n\n\n\n\n49.86\n\n\n\n\n\n\nInsights:\n\nPurchase Rate: Approximately 49.86% of visits result in a purchase event, indicating a moderate conversion rate.\nConversion Performance: Understanding the proportion of visits that lead to purchases provides insights into the effectiveness of the website in driving sales.\nPotential Growth Opportunities: Identifying areas for improvement in the conversion funnel to increase the purchase rate and enhance overall revenue generation.\n\n\n\n\n\n\nWITH view_purchase AS (\nSELECT\n    COUNT(E.visit_id) AS visit_count\nFROM events AS E JOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.page_name = 'Checkout' and EI.event_name = 'Page View')\nSELECT\n    ROUND(100 - (100.0 * COUNT(DISTINCT E.visit_id) /\n        (SELECT visit_count FROM view_purchase)),2) AS pct_of_checkout_visits_not_purchased\nFROM events AS E JOIN event_identifier AS EI ON E.event_type = EI.event_type\nWHERE EI.event_name = 'Purchase';\nOutput:\n\n\n\npct_of_checkout_visits_not_purchased\n\n\n\n\n15.50\n\n\n\n\n\n\nInsights:\n\nCheckout Abandonment: Approximately 15.50% of visits proceed to the checkout page but do not culminate in a purchase, indicating a significant dropout rate.\nPotential Revenue Loss: Identifying and addressing factors contributing to checkout abandonment is crucial to mitigate potential revenue loss and maximize conversion rates.\nUser Experience Evaluation: Analyzing the user experience at the checkout stage, including ease of navigation, payment options, and shipping information, can provide insights into areas for improvement.\n\n\n\n\n\n\nWITH top_3_pages AS\n(SELECT\n    page_id, COUNT(DISTINCT visit_id) AS number_of_views\nFROM events\nWHERE event_type = '1'\nGROUP BY page_id\nORDER BY number_of_views DESC\nLIMIT 3\n)\nSELECT\n    page_name, number_of_views\nFROM top_3_pages\nJOIN page_hierarchy ON top_3_pages.page_id = page_hierarchy.page_id;\nOutput:\n\n\n\npage_name\nnumber_of_views\n\n\n\n\nHome Page\n1782\n\n\nAll Products\n3174\n\n\nCheckout\n2103\n\n\n\n\n\n\nInsights:\n\nUser Engagement: The Home Page, All Products, and Checkout pages are pivotal in user navigation, as evidenced by their high view counts.\nBrowsing Behavior: Users frequently visit the All Products page, indicating a strong interest in exploring available products or services.\nCheckout Process: The significant number of views on the Checkout page underscores its importance in the conversion journey, suggesting a substantial portion of users progress towards completing transactions.\n\n\n\n\n\n\nSELECT PH.product_category,\n       SUM(CASE WHEN E.event_type = '1' THEN 1 ELSE 0 END) AS number_of_views,\n       SUM(CASE WHEN E.event_type = '2' THEN 1 ELSE 0 END) AS number_of_cart_adds\nFROM events as E\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.product_category IS NOT NULL\nGROUP BY PH.product_category\nORDER BY PH.product_category;\nOutput:\n\n\n\nproduct_category\nnumber_of_views\nnumber_of_cart_adds\n\n\n\n\nFish\n4633\n2789\n\n\nLuxury\n3032\n1870\n\n\nShellfish\n6204\n3792\n\n\n\n\n\n\nInsights:\n\nPopular Categories: Shellfish has the highest number of views and cart additions, followed by Fish and Luxury categories.\nEngagement Discrepancy: Despite Fish having fewer views compared to Shellfish, it has a relatively higher cart addition rate, indicating stronger user intent or interest in purchasing Fish products.\nConversion Opportunities: Analyzing user behavior within each category can help identify opportunities to optimize product pages, pricing strategies, or promotional efforts to drive conversions.\n\n\n\n\n\n\nSELECT\n    PH.product_id,\n    PH.page_name,\n    PH.product_category,\n    COUNT(PH.product_id) AS product_count\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON PH.page_id = E.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id IN\n    (SELECT E.visit_id FROM events as E\n     JOIN event_identifier AS EI ON E.event_type = EI.event_type WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_id, PH.page_name, PH.product_category\nORDER BY product_count DESC\nLIMIT 3;\nOutput:\n\n\n\nproduct_id\nproduct_name\nproduct_category\nproduct_count\n\n\n\n\n7\nLobster\nShellfish\n754\n\n\n9\nOyster\nShellfish\n726\n\n\n8\nCrab\nShellfish\n719\n\n\n\n\n\n\nInsights:\n\nShellfish Dominance: All top 3 products belong to the Shellfish category, indicating its popularity among customers.\nHigh Demand Items: Lobster, Oyster, and Crab are evidently high-demand items within the Shellfish category, likely due to factors such as taste, availability, and pricing.\nCross-Promotion Opportunities: Identifying complementary products or bundle offers with these top-selling items can help increase average order value and enhance customer satisfaction."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#product-funnel-analysis-1",
    "href": "portfolio/8WeeksSQLChallenge-Clique_Bait/index.html#product-funnel-analysis-1",
    "title": "Case Study #6 - CliqueBait.",
    "section": "",
    "text": "Using a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\n\n\nCreating a Temporary table view_add_to_cart\nCREATE TEMPORARY TABLE view_add_to_cart AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM\n    events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE\n    PH.product_category IS NOT NULL\nGROUP BY\n    PH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table products_abandoned\nCREATE TEMPORARY TABLE products_abandoned AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    COUNT(*) AS abandoned\nFROM\n    events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE\n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase'\n    )\nGROUP BY\n    PH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table products_purchased\nCREATE TEMPORARY TABLE products_purchased AS\nSELECT\n    PH.product_id,\n    PH.page_name AS product_name,\n    PH.product_category,\n    COUNT(*) AS purchased\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase')\nGROUP BY\nPH.product_id, PH.page_name, PH.product_category;\nCreating a Temporary table product_information that combines all the above tables created above.\nCREATE TEMPORARY TABLE product_information AS\nSELECT\n    VATC.*,\n    AB.abandoned,\n    PP.purchased\nFROM\nview_add_to_cart AS VATC\nJOIN products_abandoned AS AB ON VATC.product_id = AB.product_id\nJOIN products_purchased AS PP ON VATC.product_id = PP.product_id;\nDropping the created temporary tables, since they are not required anymore.\nDROP TEMPORARY TABLE IF EXISTS view_add_to_cart, products_abandoned, products_purchased;\nDisplaying the Final resulting table product_information records..\nSELECT * FROM product_information\nORDER BY product_id;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n1\nSalmon\nFish\n1559\n938\n227\n711\n\n\n2\nKingfish\nFish\n1559\n920\n213\n707\n\n\n3\nTuna\nFish\n1515\n931\n234\n697\n\n\n4\nRussian Caviar\nLuxury\n1563\n946\n249\n697\n\n\n5\nBlack Truffle\nLuxury\n1469\n924\n217\n707\n\n\n6\nAbalone\nShellfish\n1525\n932\n233\n699\n\n\n7\nLobster\nShellfish\n1547\n968\n214\n754\n\n\n8\nCrab\nShellfish\n1564\n949\n230\n719\n\n\n9\nOyster\nShellfish\n1568\n943\n217\n726\n\n\n\n\n\n\nCreating a Temporary table category_view_add_to_cart\nCREATE TEMPORARY TABLE category_view_add_to_cart AS\nSELECT\n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE PH.product_category IS NOT NULL\nGROUP BY PH.product_category;\nCreating a Temporary table category_products_abandoned\nCREATE TEMPORARY TABLE category_products_abandoned AS\nSELECT\n    PH.product_category,\n    COUNT(*) AS abandoned\n    FROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart' AND E.visit_id NOT IN (\n        SELECT E.visit_id\n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type\n        WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_category;\nCreating a Temporary table category_products_purchased\nCREATE TEMPORARY TABLE category_products_purchased AS\nSELECT\n    PH.product_category,\n    COUNT(*) AS purchased\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nWHERE EI.event_name = 'Add to Cart'\nAND E.visit_id IN (SELECT E.visit_id\n    FROM events AS E\n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    WHERE EI.event_name = 'Purchase')\nGROUP BY PH.product_category;\nCreating a Temporary table category_product_information that combines all the above tables created above.\nCREATE TEMPORARY TABLE category_product_information AS\nSELECT\n    VATC.*, AB.abandoned, PP.purchased\nFROM category_view_add_to_cart AS VATC\nJOIN category_products_abandoned AS AB ON VATC.product_category = AB.product_category\nJOIN category_products_purchased AS PP ON VATC.product_category = PP.product_category;\nDrop the temporary tables, since they are not needed anymore\nDROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart, category_products_abandoned, category_products_purchased;\nDisplaying the final resulting category_product_information table records\nSELECT *\nFROM category_product_information\nORDER BY product_category;\nOutput:\n\n\n\n\n\n\n\n\n\n\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\nLuxury\n3032\n1870\n466\n1404\n\n\nFish\n4633\n2789\n674\n2115\n\n\nShellfish\n6204\n3792\n894\n2898\n\n\n\n\n\n\nSELECT *\nFROM product_information\nORDER BY view_counts DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n9\nOyster\nShellfish\n1568\n943\n217\n726\n\n\n\nSELECT *\nFROM product_information\nORDER BY add_to_cart_counts DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n7\nLobster\nShellfish\n1547\n\n\n\n\n\n\nSELECT *\nFROM product_information\nORDER BY purchased DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n7\nLobster\nShellfish\n1547\n968\n214\n754\n\n\n\n\n\n\nInsights:\n\nShellfish Dominance: Both the product with the most views and the product with the most cart adds and purchases belong to the Shellfish category, indicating its popularity among customers.\nLobster Dominance: The product with the most cart adds and purchases is Lobster, suggesting that it is not only popular but also highly sought-after for purchase among customers.\nOyster Engagement: Although Oyster has the most views, it has a relatively lower number of cart adds and purchases compared to Lobster, indicating potential areas for improvement in conversion rate or marketing strategies.\n\n\n\n\n\n\nSELECT * FROM product_information\nORDER BY abandoned DESC\nLIMIT 1;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nproduct_name\nproduct_category\nview_counts\nadd_to_cart_counts\nabandoned\npurchased\n\n\n\n\n4\nRussian Caviar\nLuxury\n1563\n946\n249\n697\n\n\n\n\n\n\nInsights:\n\nLuxury Product: Russian Caviar falls under the Luxury category, which may imply a higher price point or more exclusive nature compared to other products.\nHigh Abandonment Rate: The relatively high number of abandonments suggests that customers might have shown interest in the product but ultimately decided not to proceed with the purchase.\nPotential Improvements: Analyzing the reasons behind abandonment, such as pricing concerns, shipping costs, or checkout process issues, could provide insights into areas for improvement to reduce abandonment rates and increase conversions. Additionally, targeted marketing or promotional strategies could be employed to encourage customers to complete their purchase of Russian Caviar.\n\n\n\n\n\n\nSELECT product_name,\n    ROUND(100.0 * (purchased/view_counts),2) AS purchase_to_view_pct\nFROM product_information\nORDER BY purchase_to_view_pct DESC\nLIMIT 1;\nOutput:\n\n\n\nproduct_name\npurchase_to_view_pct\n\n\n\n\nLobster\n48.74\n\n\n\n\n\n\nInsights:\n\nHigh Conversion Rate: The high purchase-to-view percentage indicates that a significant portion of customers who viewed the Lobster product ultimately made a purchase.\nAppealing Product: Lobster seems to be particularly appealing to customers, leading to a relatively high conversion rate compared to other products.\nMarket Demand: The high conversion rate may suggest strong market demand for Lobster, potentially due to factors such as its taste, quality, or perceived value.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(100.0 * (add_to_cart_counts/view_counts)),2) AS avg_conversion_rate\nFROM product_information;\nOutput:\n\n\n\navg_conversion_rate\n\n\n\n\n60.95\n\n\n\n\n\n\nInsights:\n\nConversion Funnel Efficiency: The high average conversion rate indicates that a significant proportion of customers who view products proceed to add them to their cart.\n\n\n\n\n\n\nSELECT\n    ROUND(AVG(100.0 * (purchased/add_to_cart_counts)),2) AS avg_conversion_rate\nFROM product_information;\nOutput:\n\n\n\navg_conversion_rate\n\n\n\n\n75.93\n\n\n\n\n\n\nInsights:\n\nConversion Funnel Efficiency: This high average conversion rate indicates that a significant proportion of customers who add products to their cart ultimately proceed to make a purchase.\n\n\n\n\n\n\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number).\nSELECT\n    U.user_id, E.visit_id, MIN(E.event_time) AS visit_start_date, C.campaign_name,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS page_view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts,\n    SUM(CASE WHEN EI.event_name = 'Purchase' THEN 1 ELSE 0 END) AS purchased_counts,\n    SUM(CASE WHEN EI.event_name = 'Ad Impression' THEN 1 ELSE 0 END) AS impression_counts,\n    SUM(CASE WHEN EI.event_name = 'Ad Click' THEN 1 ELSE 0 END) AS click_counts\nFROM users AS U\nJOIN events AS E ON U.cookie_id = E.cookie_id\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN campaign_identifier AS C ON E.event_time BETWEEN C.start_date AND C.end_date\nGROUP BY U.user_id, E.visit_id, C.campaign_name;\nOutput:\nFirst 20 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nvisit_id\nvisit_start_date\ncampaign_name\npage_view_counts\nadd_to_cart_counts\npurchased\nimpression_counts\nclick_counts\n\n\n\n\n1\nccf365\n2020-02-04 19:16:09\nHalf Off - Treat Your Shellf(ish)\n7\n3\n1\n0\n0\n\n\n2\nd58cbd\n2020-01-18 23:40:55\n25% Off - Living The Lux Life\n8\n4\n0\n0\n0\n\n\n3\n9a2f24\n2020-02-21 03:19:10\nHalf Off - Treat Your Shellf(ish)\n6\n2\n1\n0\n0\n\n\n4\n7caba5\n2020-02-22 17:49:38\nHalf Off - Treat Your Shellf(ish)\n5\n2\n0\n0\n0\n\n\n5\nf61ed7\n2020-02-01 06:30:40\nHalf Off - Treat Your Shellf(ish)\n8\n2\n1\n0\n0\n\n\n6\ne0ce49\n2020-01-25 22:43:21\n25% Off - Living The Lux Life\n9\n3\n1\n0\n0\n\n\n7\n8479c1\n2020-02-09 17:27:59\nHalf Off - Treat Your Shellf(ish)\n5\n1\n1\n0\n0\n\n\n8\na6c424\n2020-02-12 11:23:55\nHalf Off - Treat Your Shellf(ish)\n7\n2\n0\n0\n0\n\n\n9\n5ef346\n2020-02-07 17:32:45\nHalf Off - Treat Your Shellf(ish)\n7\n0\n0\n0\n0\n\n\n10\nd39d35\n2020-01-23 21:47:04\n25% Off - Living The Lux Life\n7\n3\n1\n0\n0\n\n\n11\n9c2633\n2020-01-17 04:59:43\n25% Off - Living The Lux Life\n8\n2\n0\n0\n0\n\n\n12\nd69e73\n2020-02-06 09:09:06\nHalf Off - Treat Your Shellf(ish)\n5\n1\n0\n0\n0\n\n\n13\nc70085\n2020-02-12 08:26:14\nHalf Off - Treat Your Shellf(ish)\n6\n1\n0\n0\n0\n\n\n14\n6a20a3\n2020-01-12 02:49:32\nBOGOF - Fishing For Compliments\n8\n1\n0\n0\n0\n\n\n16\n69440b\n2020-01-06 21:45:51\nBOGOF - Fishing For Compliments\n4\n1\n1\n0\n0\n\n\n17\ne70fd5\n2020-02-17 10:05:51\nHalf Off - Treat Your Shellf(ish)\n7\n2\n0\n0\n0\n\n\n18\n48810d\n2020-02-29 15:26:41\nHalf Off - Treat Your Shellf(ish)\n4\n0\n0\n0\n0\n\n\n19\nfdf383\n2020-02-11 13:52:24\nHalf Off - Treat Your Shellf(ish)\n7\n1\n1\n0\n0\n\n\n20\n378a75\n2020-02-12 23:33:51\nHalf Off - Treat Your Shellf(ish)\n4\n0\n0\n0\n0"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "Danny seriously loves Japanese food so in the beginning of 2021, he decides to embark upon a risky venture and opens up a cute little restaurant that sells his 3 favourite foods: sushi, curry and ramen.\nDanny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\n\n\n\nDanny wants to use the data to answer a few simple questions about his customers, especially about their visiting patterns, how much money they’ve spent and also which menu items are their favourite. Having this deeper connection with his customers will help him deliver a better and more personalised experience for his loyal customers.\nHe plans on using these insights to help him decide whether he should expand the existing customer loyalty program - additionally he needs help to generate some basic datasets so his team can easily inspect the data without needing to use SQL.\nDanny has provided you with a sample of his overall customer data due to privacy issues - but he hopes that these examples are enough for you to write fully functioning SQL queries to help him answer his questions!\nDanny has shared with you 3 key datasets for this case study:\n\nsales\nmenu\nmembers\n\nYou can inspect the entity relationship diagram and example data below.\n\n\n\n\n\n\n\n\nAll datasets exist within the dannys_diner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\n\ncustomer_id\norder_date\nproduct_id\n\n\n\n\nA\n2021-01-01\n1\n\n\nA\n2021-01-01\n2\n\n\nA\n2021-01-07\n2\n\n\nA\n2021-01-10\n3\n\n\nA\n2021-01-11\n3\n\n\nA\n2021-01-11\n3\n\n\nB\n2021-01-01\n2\n\n\nB\n2021-01-02\n2\n\n\nB\n2021-01-04\n1\n\n\nB\n2021-01-11\n1\n\n\nB\n2021-01-16\n3\n\n\nB\n2021-02-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-07\n3\n\n\n\n\n\n\nThe menu table maps the product_id to the actual product_name and price of each menu item.\n\n\n\nproduct_id\nproduct_name\nprice\n\n\n\n\n1\nsushi\n10\n\n\n2\ncurry\n15\n\n\n3\nramen\n12\n\n\n\n\n\n\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\n\ncustomer_id\njoin_date\n\n\n\n\nA\n2021-01-07\n\n\nB\n2021-01-09\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n-- Creating the Database, Tables and Injecting the data into those tables --\nCREATE DATABASE dannys_diner;\nUSE dannys_diner;\n\nCREATE TABLE sales (\n  `customer_id` VARCHAR(1),\n  `order_date` DATE,\n  `product_id` INTEGER\n);\n\nINSERT INTO sales\n  (`customer_id`, `order_date`, `product_id`)\nVALUES\n  ('A', '2021-01-01', '1'),\n  ('A', '2021-01-01', '2'),\n  ('A', '2021-01-07', '2'),\n  ('A', '2021-01-10', '3'),\n  ('A', '2021-01-11', '3'),\n  ('A', '2021-01-11', '3'),\n  ('B', '2021-01-01', '2'),\n  ('B', '2021-01-02', '2'),\n  ('B', '2021-01-04', '1'),\n  ('B', '2021-01-11', '1'),\n  ('B', '2021-01-16', '3'),\n  ('B', '2021-02-01', '3'),\n  ('C', '2021-01-01', '3'),\n  ('C', '2021-01-01', '3'),\n  ('C', '2021-01-07', '3');\n \n\nCREATE TABLE menu (\n  `product_id` INTEGER,\n  `product_name` VARCHAR(5),\n  `price` INTEGER);\n\nINSERT INTO menu\n  (`product_id`, `product_name`, `price`)\nVALUES\n  ('1', 'sushi', '10'),\n  ('2', 'curry', '15'),\n  ('3', 'ramen', '12');\n  \n\nCREATE TABLE members (\n  `customer_id` VARCHAR(1),\n  `join_date` DATE\n);\n\nINSERT INTO members\n  (`customer_id`, `join_date`)\nVALUES\n  ('A', '2021-01-07'),\n  ('B', '2021-01-09');\n\n\n\nEach of the following case study questions can be answered using a single SQL statement:\n\n\nSELECT SALES.customer_id, SUM(MENU.price) as Total_amt_spend\nFROM SALES\nLEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nGROUP BY SALES.customer_id\nORDER BY SUM(SALES.customer_id) ASC;\nOutput:\n\n\n\ncustomer_id\nTotal_amt_spend\n\n\n\n\nA\n76\n\n\nB\n74\n\n\nC\n36\n\n\n\n\n\n\nCustomer Spending Overview:\n\nCustomer A has spent a total of $76 at the restaurant, making them the highest spender among all customers.\nCustomer B follows closely behind, with a total expenditure of $74.\nCustomer C has spent $36 in total, which is notably lower compared to customers A and B.\n\nIdentifying High-Value Customers:\n\nCustomers A and B emerge as high-value customers, contributing significantly to the restaurant’s revenue.\nFocusing on retaining and incentivizing these customers through loyalty programs or personalized offers could enhance customer retention and overall profitability.\n\nOpportunities for Targeted Marketing:\n\nTargeted marketing campaigns can be designed to attract new customers or encourage existing ones to increase their spending.\nBy analyzing the preferences and behavior of high-spending customers, the restaurant can refine its menu offerings and promotional activities to cater to their preferences effectively.\n\n\n\n\n\n\nSELECT SALES.customer_id, SUM(MENU.price) as Total_amt_spend\nFROM SALES\nLEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nGROUP BY SALES.customer_id\nORDER BY SUM(SALES.customer_id) ASC;\nOutput:\n\n\n\ncustomer_id\nnumber_of_visits\n\n\n\n\nA\n4\n\n\nB\n6\n\n\nC\n2\n\n\n\n\n\n\nFrequency of Visits:\n\nCustomer A has visited the restaurant on 4 different days, indicating moderate visitation frequency.\nCustomer B has visited the restaurant on 6 days, suggesting a higher frequency of visits compared to other customers.\nCustomer C has visited the restaurant on 2 days, indicating relatively infrequent visits.\n\nLeveraging Personalized Marketing for Customer Retention and Revenue Growth:\n\nImplementing personalized marketing strategies tailored to customers like Customer C presents an opportunity for enhanced customer retention and revenue growth. By offering customized plans based on individual preferences, the restaurant can foster loyalty, encourage repeat visits, and ultimately drive increased spending. This targeted approach not only strengthens customer relationships but also maximizes profitability, contributing to long-term sustainability and success.\n\n\n\n\n\n\nWITH item_purchased_by_customer\nas (SELECT\n    SALES.customer_id as customer_id,\n    SALES.order_date as order_date,\n    MENU.product_name as product_name,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY SALES.order_date) as `Dense_Rank`\n    FROM SALES\n    LEFT JOIN MENU ON\n    SALES.product_id = MENU.product_id\n    )\nSELECT customer_id,\n        product_name\n        FROM item_purchased_by_customer\nWHERE `Dense_Rank` = 1\nGROUP BY customer_id, product_name;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nsushi\n\n\nA\ncurry\n\n\nB\ncurry\n\n\nC\nramen\n\n\n\n\n\n\nInitial Menu Preferences:\n\nCustomer A’s first purchase was sushi, indicating a preference for this menu item from the outset.\nCustomer B’s initial purchase was curry, suggesting a different menu preference compared to Customer A.\nCustomer C’s first purchase was ramen, indicating yet another distinct menu preference.\n\n\n\n\n\n\nSELECT MENU.product_name,\n    COUNT(SALES.customer_id) as item_count\nFROM MENU\nINNER JOIN SALES ON\nMENU.product_id = SALES.product_id\nGROUP BY MENU.product_name\nORDER BY item_count DESC\nLIMIT 1;\nOutput:\n\n\n\nproduct_name\nitem_count\n\n\n\n\nramen\n8\n\n\n\n\n\n\nMost Popular Menu Item:\n\nThe analysis reveals that “ramen” is the most purchased item on the menu, with a total of 8 purchases by all customers.\nThis indicates a high level of popularity and demand for ramen among the restaurant’s clientele.\n\n\n\n\n\n\nWITH most_popular\nAS (SELECT SALES.customer_id,\n        MENU.product_name,\n        COUNT(MENU.product_name) AS order_cnt,\n        DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY COUNT(SALES.customer_id) DESC) as `Dense_Rank`\n        FROM SALES JOIN MENU ON\n        MENU.product_id = SALES.product_id\n        GROUP BY SALES.customer_id, MENU.product_name\n    )\nSELECT customer_id, product_name, order_cnt\nFROM most_popular\nWHERE `Dense_Rank` = 1;\nOutput:\n\n\n\ncustomer_id\nproduct_name\norder_cnt\n\n\n\n\nA\nramen\n3\n\n\nB\ncurry\n2\n\n\nB\nsushi\n2\n\n\nB\nramen\n2\n\n\nC\nramen\n3\n\n\n\n\n\n\nCustomer-Specific Preferences:\n\nCustomer A’s most popular item is “ramen,” with a total of 3 orders, indicating a preference for this menu item.\nCustomer B has multiple most popular items, including “curry,” “sushi,” and “ramen,” each with 2 orders, suggesting diverse preferences.\nCustomer C’s most popular item is also “ramen,” with 3 orders, indicating a consistent preference for this menu item.\n\n\n\n\n\n\nWITH first_item_purchased AS (\nSELECT\n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date ASC) AS DRank\n    FROM MEMBERS\n    JOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &gt; MEMBERS.join_date\n)\nSELECT\n    first_item_purchased.customer_id AS customer_id,\n    MENU.product_name\nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nramen\n\n\nB\nsushi\n\n\n\n\n\n\nMembership Engagement:\n\nCustomer A’s first purchase after becoming a member was “ramen,” indicating their choice of menu item soon after joining the loyalty program.\nCustomer B’s initial purchase post-membership was “sushi,” suggesting a different menu preference compared to Customer A.\n\n\n\n\n\n\nWITH first_item_purchased AS (\nSELECT\n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date DESC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date\n)\nSELECT\n    first_item_purchased.customer_id AS customer_id,\n    MENU.product_name\nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nsushi\n\n\nA\ncurry\n\n\nB\nsushi\n\n\n\n\n\n\nPre-Membership Purchases:\n\nCustomer A’s last purchases before becoming a member were “sushi” and “curry,” indicating their menu preferences just before joining the loyalty program.\nCustomer B’s final purchase before membership was also “sushi,” suggesting their menu choice before becoming a member.\n\n\n\n\n\n\nSELECT SALES.customer_id,\n        COUNT(SALES.product_id) AS total_items_purchased,\n        SUM(MENU.price) AS amount_spent\nFROM SALES\nINNER JOIN MEMBERS\nON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date\nJOIN MENU\nON MENU.product_id = SALES.product_id\nGROUP BY SALES.customer_id\nORDER BY SALES.customer_id;\nOutput:\n\n\n\ncustomer_id\ntotal_items_purchased\namount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n3\n40\n\n\n\n\n\n\nPre-Membership Purchases Overview:\n\nCustomer A made a total of 2 purchases before becoming a member, with a corresponding total amount spent of $25.\nCustomer B made 3 purchases before joining the loyalty program, amounting to a total expenditure of $40.\n\n\n\n\n\n\nWITH cte AS (\nSELECT\n    MENU.product_id,\n    (CASE WHEN MENU.product_name = 'sushi' THEN 20 * MENU.price\n    ELSE 10 * MENU.price END) AS Points\n    FROM MENU)\nSELECT SALES.customer_id AS Customer_Id,\n    SUM(cte.points) AS Total_Points\nFROM SALES LEFT JOIN cte ON\nSALES.product_id = cte.product_id\nGROUP BY SALES.customer_id\nORDER BY SALES.customer_id;\nOutput:\n\n\n\ncustomer_id\nTotal_Points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\nC\n360\n\n\n\n\n\n\nTotal Loyalty Points Earned:\n\nCustomer A has earned a total of 860 loyalty points based on their purchases.\nCustomer B has accumulated 940 loyalty points, indicating a higher level of engagement with the loyalty program.\nCustomer C has earned 360 loyalty points, reflecting their lower spending and engagement compared to other customers.\n\n\n\n\n\n\nWITH dates_cte AS (\nSELECT customer_id, join_date,\n    DATE_ADD(join_date, INTERVAL 6 DAY) AS valid_date,\n    LAST_DAY('2021-01-01') AS month_end_date\n    FROM members\n) SELECT DC.customer_id,\n    SUM(CASE WHEN S.order_date BETWEEN DC.join_date AND DC.valid_date THEN M.price * 20\n             WHEN M.product_name = 'sushi' THEN M.price * 20\n             ELSE M.price * 10 END) AS total_points\nFROM dates_cte AS DC\nJOIN sales AS S\nON DC.customer_id = S.customer_id\nJOIN menu AS M\nON M.product_id = S.product_id\nWHERE S.order_date &lt;= DC.month_end_date\nGROUP BY DC.customer_id;\nOutput:\n\n\n\ncustomer_id\ntotal_points\n\n\n\n\nB\n820\n\n\nA\n1370\n\n\n\n\n\n\nTotal Points Earned in January:\n\nCustomer A has earned a total of 1370 loyalty points by the end of January, taking advantage of the 2x points promotion in the first week after joining the program.\nCustomer B has accumulated 820 loyalty points by the end of January, also benefiting from the promotional offer during the first week.\n\n\n\n\n\n\n\n\n\nDesired Table:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\nSELECT\n    SALES.customer_id,\n    SALES.order_date,\n    MENU.product_name,\n    MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES LEFT JOIN MEMBERS ON\nSALES.customer_id = MEMBERS.customer_id\nJOIN MENU ON\nSALES.product_id = MENU.product_id\nORDER BY SALES.customer_id ASC, SALES.order_date ASC;\nOutput:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\n\n\n\nDesired Table:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\nranking\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-07\nramen\n12\nN\nNULL\n\n\n\nWITH all_data AS (\nSELECT\n    SALES.customer_id, SALES.order_date, MENU.product_name, MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES LEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nLEFT JOIN MEMBERS ON\nSALES.customer_id = MEMBERS.customer_id)\n\nSELECT all_data.*,\n    CASE WHEN all_data.Member = 'Y' THEN RANK() OVER (PARTITION BY all_data.customer_id, all_data.member\n            ORDER BY all_data.order_date) ELSE NULL\n    END AS 'ranking'\nFROM all_data;\nOutput:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\nranking\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-07\nramen\n12\nN\nNULL"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#introduction",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "Danny seriously loves Japanese food so in the beginning of 2021, he decides to embark upon a risky venture and opens up a cute little restaurant that sells his 3 favourite foods: sushi, curry and ramen.\nDanny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#problem-statement",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#problem-statement",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "Danny wants to use the data to answer a few simple questions about his customers, especially about their visiting patterns, how much money they’ve spent and also which menu items are their favourite. Having this deeper connection with his customers will help him deliver a better and more personalised experience for his loyal customers.\nHe plans on using these insights to help him decide whether he should expand the existing customer loyalty program - additionally he needs help to generate some basic datasets so his team can easily inspect the data without needing to use SQL.\nDanny has provided you with a sample of his overall customer data due to privacy issues - but he hopes that these examples are enough for you to write fully functioning SQL queries to help him answer his questions!\nDanny has shared with you 3 key datasets for this case study:\n\nsales\nmenu\nmembers\n\nYou can inspect the entity relationship diagram and example data below."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#example-datasets",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#example-datasets",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "All datasets exist within the dannys_diner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\n\ncustomer_id\norder_date\nproduct_id\n\n\n\n\nA\n2021-01-01\n1\n\n\nA\n2021-01-01\n2\n\n\nA\n2021-01-07\n2\n\n\nA\n2021-01-10\n3\n\n\nA\n2021-01-11\n3\n\n\nA\n2021-01-11\n3\n\n\nB\n2021-01-01\n2\n\n\nB\n2021-01-02\n2\n\n\nB\n2021-01-04\n1\n\n\nB\n2021-01-11\n1\n\n\nB\n2021-01-16\n3\n\n\nB\n2021-02-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-07\n3\n\n\n\n\n\n\nThe menu table maps the product_id to the actual product_name and price of each menu item.\n\n\n\nproduct_id\nproduct_name\nprice\n\n\n\n\n1\nsushi\n10\n\n\n2\ncurry\n15\n\n\n3\nramen\n12\n\n\n\n\n\n\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\n\ncustomer_id\njoin_date\n\n\n\n\nA\n2021-01-07\n\n\nB\n2021-01-09"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#interactive-sql-session",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#interactive-sql-session",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n-- Creating the Database, Tables and Injecting the data into those tables --\nCREATE DATABASE dannys_diner;\nUSE dannys_diner;\n\nCREATE TABLE sales (\n  `customer_id` VARCHAR(1),\n  `order_date` DATE,\n  `product_id` INTEGER\n);\n\nINSERT INTO sales\n  (`customer_id`, `order_date`, `product_id`)\nVALUES\n  ('A', '2021-01-01', '1'),\n  ('A', '2021-01-01', '2'),\n  ('A', '2021-01-07', '2'),\n  ('A', '2021-01-10', '3'),\n  ('A', '2021-01-11', '3'),\n  ('A', '2021-01-11', '3'),\n  ('B', '2021-01-01', '2'),\n  ('B', '2021-01-02', '2'),\n  ('B', '2021-01-04', '1'),\n  ('B', '2021-01-11', '1'),\n  ('B', '2021-01-16', '3'),\n  ('B', '2021-02-01', '3'),\n  ('C', '2021-01-01', '3'),\n  ('C', '2021-01-01', '3'),\n  ('C', '2021-01-07', '3');\n \n\nCREATE TABLE menu (\n  `product_id` INTEGER,\n  `product_name` VARCHAR(5),\n  `price` INTEGER);\n\nINSERT INTO menu\n  (`product_id`, `product_name`, `price`)\nVALUES\n  ('1', 'sushi', '10'),\n  ('2', 'curry', '15'),\n  ('3', 'ramen', '12');\n  \n\nCREATE TABLE members (\n  `customer_id` VARCHAR(1),\n  `join_date` DATE\n);\n\nINSERT INTO members\n  (`customer_id`, `join_date`)\nVALUES\n  ('A', '2021-01-07'),\n  ('B', '2021-01-09');"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#case-study-questions",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "Each of the following case study questions can be answered using a single SQL statement:\n\n\nSELECT SALES.customer_id, SUM(MENU.price) as Total_amt_spend\nFROM SALES\nLEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nGROUP BY SALES.customer_id\nORDER BY SUM(SALES.customer_id) ASC;\nOutput:\n\n\n\ncustomer_id\nTotal_amt_spend\n\n\n\n\nA\n76\n\n\nB\n74\n\n\nC\n36\n\n\n\n\n\n\nCustomer Spending Overview:\n\nCustomer A has spent a total of $76 at the restaurant, making them the highest spender among all customers.\nCustomer B follows closely behind, with a total expenditure of $74.\nCustomer C has spent $36 in total, which is notably lower compared to customers A and B.\n\nIdentifying High-Value Customers:\n\nCustomers A and B emerge as high-value customers, contributing significantly to the restaurant’s revenue.\nFocusing on retaining and incentivizing these customers through loyalty programs or personalized offers could enhance customer retention and overall profitability.\n\nOpportunities for Targeted Marketing:\n\nTargeted marketing campaigns can be designed to attract new customers or encourage existing ones to increase their spending.\nBy analyzing the preferences and behavior of high-spending customers, the restaurant can refine its menu offerings and promotional activities to cater to their preferences effectively.\n\n\n\n\n\n\nSELECT SALES.customer_id, SUM(MENU.price) as Total_amt_spend\nFROM SALES\nLEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nGROUP BY SALES.customer_id\nORDER BY SUM(SALES.customer_id) ASC;\nOutput:\n\n\n\ncustomer_id\nnumber_of_visits\n\n\n\n\nA\n4\n\n\nB\n6\n\n\nC\n2\n\n\n\n\n\n\nFrequency of Visits:\n\nCustomer A has visited the restaurant on 4 different days, indicating moderate visitation frequency.\nCustomer B has visited the restaurant on 6 days, suggesting a higher frequency of visits compared to other customers.\nCustomer C has visited the restaurant on 2 days, indicating relatively infrequent visits.\n\nLeveraging Personalized Marketing for Customer Retention and Revenue Growth:\n\nImplementing personalized marketing strategies tailored to customers like Customer C presents an opportunity for enhanced customer retention and revenue growth. By offering customized plans based on individual preferences, the restaurant can foster loyalty, encourage repeat visits, and ultimately drive increased spending. This targeted approach not only strengthens customer relationships but also maximizes profitability, contributing to long-term sustainability and success.\n\n\n\n\n\n\nWITH item_purchased_by_customer\nas (SELECT\n    SALES.customer_id as customer_id,\n    SALES.order_date as order_date,\n    MENU.product_name as product_name,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY SALES.order_date) as `Dense_Rank`\n    FROM SALES\n    LEFT JOIN MENU ON\n    SALES.product_id = MENU.product_id\n    )\nSELECT customer_id,\n        product_name\n        FROM item_purchased_by_customer\nWHERE `Dense_Rank` = 1\nGROUP BY customer_id, product_name;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nsushi\n\n\nA\ncurry\n\n\nB\ncurry\n\n\nC\nramen\n\n\n\n\n\n\nInitial Menu Preferences:\n\nCustomer A’s first purchase was sushi, indicating a preference for this menu item from the outset.\nCustomer B’s initial purchase was curry, suggesting a different menu preference compared to Customer A.\nCustomer C’s first purchase was ramen, indicating yet another distinct menu preference.\n\n\n\n\n\n\nSELECT MENU.product_name,\n    COUNT(SALES.customer_id) as item_count\nFROM MENU\nINNER JOIN SALES ON\nMENU.product_id = SALES.product_id\nGROUP BY MENU.product_name\nORDER BY item_count DESC\nLIMIT 1;\nOutput:\n\n\n\nproduct_name\nitem_count\n\n\n\n\nramen\n8\n\n\n\n\n\n\nMost Popular Menu Item:\n\nThe analysis reveals that “ramen” is the most purchased item on the menu, with a total of 8 purchases by all customers.\nThis indicates a high level of popularity and demand for ramen among the restaurant’s clientele.\n\n\n\n\n\n\nWITH most_popular\nAS (SELECT SALES.customer_id,\n        MENU.product_name,\n        COUNT(MENU.product_name) AS order_cnt,\n        DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY COUNT(SALES.customer_id) DESC) as `Dense_Rank`\n        FROM SALES JOIN MENU ON\n        MENU.product_id = SALES.product_id\n        GROUP BY SALES.customer_id, MENU.product_name\n    )\nSELECT customer_id, product_name, order_cnt\nFROM most_popular\nWHERE `Dense_Rank` = 1;\nOutput:\n\n\n\ncustomer_id\nproduct_name\norder_cnt\n\n\n\n\nA\nramen\n3\n\n\nB\ncurry\n2\n\n\nB\nsushi\n2\n\n\nB\nramen\n2\n\n\nC\nramen\n3\n\n\n\n\n\n\nCustomer-Specific Preferences:\n\nCustomer A’s most popular item is “ramen,” with a total of 3 orders, indicating a preference for this menu item.\nCustomer B has multiple most popular items, including “curry,” “sushi,” and “ramen,” each with 2 orders, suggesting diverse preferences.\nCustomer C’s most popular item is also “ramen,” with 3 orders, indicating a consistent preference for this menu item.\n\n\n\n\n\n\nWITH first_item_purchased AS (\nSELECT\n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date ASC) AS DRank\n    FROM MEMBERS\n    JOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &gt; MEMBERS.join_date\n)\nSELECT\n    first_item_purchased.customer_id AS customer_id,\n    MENU.product_name\nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nramen\n\n\nB\nsushi\n\n\n\n\n\n\nMembership Engagement:\n\nCustomer A’s first purchase after becoming a member was “ramen,” indicating their choice of menu item soon after joining the loyalty program.\nCustomer B’s initial purchase post-membership was “sushi,” suggesting a different menu preference compared to Customer A.\n\n\n\n\n\n\nWITH first_item_purchased AS (\nSELECT\n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date DESC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date\n)\nSELECT\n    first_item_purchased.customer_id AS customer_id,\n    MENU.product_name\nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nOutput:\n\n\n\ncustomer_id\nproduct_name\n\n\n\n\nA\nsushi\n\n\nA\ncurry\n\n\nB\nsushi\n\n\n\n\n\n\nPre-Membership Purchases:\n\nCustomer A’s last purchases before becoming a member were “sushi” and “curry,” indicating their menu preferences just before joining the loyalty program.\nCustomer B’s final purchase before membership was also “sushi,” suggesting their menu choice before becoming a member.\n\n\n\n\n\n\nSELECT SALES.customer_id,\n        COUNT(SALES.product_id) AS total_items_purchased,\n        SUM(MENU.price) AS amount_spent\nFROM SALES\nINNER JOIN MEMBERS\nON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date\nJOIN MENU\nON MENU.product_id = SALES.product_id\nGROUP BY SALES.customer_id\nORDER BY SALES.customer_id;\nOutput:\n\n\n\ncustomer_id\ntotal_items_purchased\namount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n3\n40\n\n\n\n\n\n\nPre-Membership Purchases Overview:\n\nCustomer A made a total of 2 purchases before becoming a member, with a corresponding total amount spent of $25.\nCustomer B made 3 purchases before joining the loyalty program, amounting to a total expenditure of $40.\n\n\n\n\n\n\nWITH cte AS (\nSELECT\n    MENU.product_id,\n    (CASE WHEN MENU.product_name = 'sushi' THEN 20 * MENU.price\n    ELSE 10 * MENU.price END) AS Points\n    FROM MENU)\nSELECT SALES.customer_id AS Customer_Id,\n    SUM(cte.points) AS Total_Points\nFROM SALES LEFT JOIN cte ON\nSALES.product_id = cte.product_id\nGROUP BY SALES.customer_id\nORDER BY SALES.customer_id;\nOutput:\n\n\n\ncustomer_id\nTotal_Points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\nC\n360\n\n\n\n\n\n\nTotal Loyalty Points Earned:\n\nCustomer A has earned a total of 860 loyalty points based on their purchases.\nCustomer B has accumulated 940 loyalty points, indicating a higher level of engagement with the loyalty program.\nCustomer C has earned 360 loyalty points, reflecting their lower spending and engagement compared to other customers.\n\n\n\n\n\n\nWITH dates_cte AS (\nSELECT customer_id, join_date,\n    DATE_ADD(join_date, INTERVAL 6 DAY) AS valid_date,\n    LAST_DAY('2021-01-01') AS month_end_date\n    FROM members\n) SELECT DC.customer_id,\n    SUM(CASE WHEN S.order_date BETWEEN DC.join_date AND DC.valid_date THEN M.price * 20\n             WHEN M.product_name = 'sushi' THEN M.price * 20\n             ELSE M.price * 10 END) AS total_points\nFROM dates_cte AS DC\nJOIN sales AS S\nON DC.customer_id = S.customer_id\nJOIN menu AS M\nON M.product_id = S.product_id\nWHERE S.order_date &lt;= DC.month_end_date\nGROUP BY DC.customer_id;\nOutput:\n\n\n\ncustomer_id\ntotal_points\n\n\n\n\nB\n820\n\n\nA\n1370\n\n\n\n\n\n\nTotal Points Earned in January:\n\nCustomer A has earned a total of 1370 loyalty points by the end of January, taking advantage of the 2x points promotion in the first week after joining the program.\nCustomer B has accumulated 820 loyalty points by the end of January, also benefiting from the promotional offer during the first week."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#bonus-questions",
    "href": "portfolio/8WeeksSQLChallenge-Danny's_Diner/index.html#bonus-questions",
    "title": "Case Study #1 - Danny’s Diner.",
    "section": "",
    "text": "Desired Table:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\nSELECT\n    SALES.customer_id,\n    SALES.order_date,\n    MENU.product_name,\n    MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES LEFT JOIN MEMBERS ON\nSALES.customer_id = MEMBERS.customer_id\nJOIN MENU ON\nSALES.product_id = MENU.product_id\nORDER BY SALES.customer_id ASC, SALES.order_date ASC;\nOutput:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\n\n\n\nDesired Table:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\nranking\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-07\nramen\n12\nN\nNULL\n\n\n\nWITH all_data AS (\nSELECT\n    SALES.customer_id, SALES.order_date, MENU.product_name, MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES LEFT JOIN MENU ON\nSALES.product_id = MENU.product_id\nLEFT JOIN MEMBERS ON\nSALES.customer_id = MEMBERS.customer_id)\n\nSELECT all_data.*,\n    CASE WHEN all_data.Member = 'Y' THEN RANK() OVER (PARTITION BY all_data.customer_id, all_data.member\n            ORDER BY all_data.order_date) ELSE NULL\n    END AS 'ranking'\nFROM all_data;\nOutput:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nMember\nranking\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-01\nramen\n12\nN\nNULL\n\n\nC\n2021-01-07\nramen\n12\nN\nNULL"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "Data Mart is Danny’s latest venture and after running international operations for his online supermarket that specialises in fresh produce - Danny is asking for your support to analyse his sales performance.\nIn June 2020 - large scale supply changes were made at Data Mart. All Data Mart products now use sustainable packaging methods in every single step from the farm all the way to the customer.\nDanny needs your help to quantify the impact of this change on the sales performance for Data Mart and it’s separate business areas.\nThe key business question he wants you to help him answer are the following:\n\nWhat was the quantifiable impact of the changes introduced in June 2020?\nWhich platform, region, segment and customer types were the most impacted by this change?\nWhat can we do about future introduction of similar sustainability updates to the business to minimise impact on sales?\n\n\n\n\nFor this case study there is only a single table: data_mart.weekly_sales\nThe Entity Relationship Diagram is shown below with the data types made clear, please note that there is only this one table - hence why it looks a little bit lonely!\n\n\n\nThe columns are pretty self-explanatory based on the column names but here are some further details about the dataset:\n\nData Mart has international operations using a multi-region strategy\nData Mart has both, a retail and online platform in the form of a Shopify store front to serve their customers\nCustomer segment and customer_type data relates to personal age and demographics information that is shared with Data Mart.\ntransactions is the count of unique purchases made through Data Mart and sales is the actual dollar amount of purchases\n\nEach record in the dataset is related to a specific aggregated slice of the underlying sales data rolled up into a week_date value which represents the start of the sales week.\n\n\n\n10 random rows are shown in the table output below from data_mart.weekly_sales:\n\n\n\n\n\n\n\n\n\n\n\n\nweek_date\nregion\nplatform\nsegment\ncustomer_type\ntransactions\nsales\n\n\n\n\n9/9/20\nOCEANIA\nShopify\nC3\nNew\n610\n110033.89\n\n\n29/7/20\nAFRICA\nRetail\nC1\nNew\n110692\n3053771.19\n\n\n22/7/20\nEUROPE\nShopify\nC4\nExisting\n24\n8101.54\n\n\n13/5/20\nAFRICA\nShopify\nnull\nGuest\n5287\n1003301.37\n\n\n24/7/19\nASIA\nRetail\nC1\nNew\n127342\n3151780.41\n\n\n10/7/19\nCANADA\nShopify\nF3\nNew\n51\n8844.93\n\n\n26/6/19\nOCEANIA\nRetail\nC3\nNew\n152921\n5551385.36\n\n\n29/5/19\nSOUTH AMERICA\nShopify\nnull\nNew\n53\n10056.2\n\n\n22/8/18\nAFRICA\nRetail\nnull\nExisting\n31721\n1718863.58\n\n\n25/7/18\nSOUTH AMERICA\nRetail\nnull\nNew\n2136\n81757.91\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA data_mart;\nUSE data_mart;\n\nDROP TABLE IF EXISTS data_mart.weekly_sales;\nCREATE TABLE data_mart.weekly_sales (\n  `week_date` VARCHAR(7),\n  `region` VARCHAR(13),\n  `platform` VARCHAR(7),\n  `segment` VARCHAR(4),\n  `customer_type` VARCHAR(8),\n  `transactions` INTEGER,\n  `sales` INTEGER\n);\n\nINSERT INTO data_mart.weekly_sales\n  (`week_date`, `region`, `platform`, `segment`, `customer_type`, `transactions`, `sales`)\nVALUES\n  ('31/8/20', 'ASIA', 'Retail', 'C3', 'New', '120631', '3656163'),\n  ('31/8/20', 'ASIA', 'Retail', 'F1', 'New', '31574', '996575'),\n  ('31/8/20', 'USA', 'Retail', 'null', 'Guest', '529151', '16509610'),\n  ('31/8/20', 'EUROPE', 'Retail', 'C1', 'New', '4517', '141942'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C2', 'New', '58046', '1758388'),\n  ('31/8/20', 'CANADA', 'Shopify', 'F2', 'Existing', '1336', '243878'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F3', 'Existing', '2514', '519502'),\n  ('31/8/20', 'ASIA', 'Shopify', 'F1', 'Existing', '2158', '371417'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F2', 'New', '318', '49557'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C3', 'New', '111032', '3888162'),\n  ('31/8/20', 'USA', 'Shopify', 'F1', 'Existing', '1398', '260773'),\n  ('31/8/20', 'OCEANIA', 'Shopify', 'C2', 'Existing', '4661', '882690'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'C2', 'Existing', '1029', '38762'),\n  ('31/8/20', 'SOUTH AMERICA', 'Shopify', 'C4', 'New', '6', '917'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'F3', 'Existing', '115', '35215'),\n  ('31/8/20', 'OCEANIA', 'Retail', 'F3', 'Existing', '551905', '30371770'),\n  ('31/8/20', 'ASIA', 'Shopify', 'C3', 'Existing', '1969', '374327'),\n  ('31/8/20', 'AFRICA', 'Retail', 'F1', 'Existing', '97604', '5185233'),\n  ('31/8/20', 'OCEANIA', 'Retail', 'C2', 'New', '111219', '2980673'),\n  ('31/8/20', 'USA', 'Retail', 'F1', 'New', '11820', '463738'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'F3', 'Existing', '1363', '65730'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C3', 'Existing', '284971', '14430196'),\n  ('31/8/20', 'ASIA', 'Retail', 'F2', 'New', '70496', '2176980'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F1', 'Existing', '2678', '478756'),\n  ('31/8/20', 'USA', 'Shopify', 'C4', 'New', '22', '3319'),\n  ('31/8/20', 'CANADA', 'Retail', 'F3', 'Existing', '94274', '5306746'),\n  ('31/8/20', 'ASIA', 'Retail', 'F1', 'Existing', '94287', '4511841'),\n  ('31/8/20', 'EUROPE', 'Retail', 'null', 'New', '3064', '134249'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'F1', 'New', '7', '1579'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'C4', 'New', '329', '11451'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'F1', 'Existing', '854', '31589'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'C2', 'Existing', '180', '53567')\n\n\n\nThe following case study questions require some data cleaning steps before we start to unpack Danny’s key business questions in more depth.\n\n\nIn a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record\n\n\n\n\n\nWhat day of the week is used for each week_date value?\nWhat range of week numbers are missing from the dataset?\nHow many total transactions were there for each year in the dataset?\nWhat is the total sales for each region for each month?\nWhat is the total count of transactions for each platform\nWhat is the percentage of sales for Retail vs Shopify for each month?\nWhat is the percentage of sales by demographic for each year in the dataset?\nWhich age_band and demographic values contribute the most to Retail sales?\nCan we use the avg_transaction column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?\n\n\n\n\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n\nWhat is the total sales for the 4 weeks before and after 2020-06-15? What is the growth or reduction rate in actual values and percentage of sales?\nWhat about the entire 12 weeks before and after?\nHow do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019?\n\n\n\n\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?\n\n\n\n\n\n\nIn a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record\n\n\n\n\n\n\n\nDROP TABLE IF EXISTS data_mart.clean_weekly_sales\n\nCREATE TABLE data_mart.clean_weekly_sales AS (\nSELECT\n  STR_TO_DATE(week_date, '%d/%m/%y') AS week_date,\n  WEEK(STR_TO_DATE(week_date, '%d/%m/%y')) AS week_number,\n  MONTH(STR_TO_DATE(week_date, '%d/%m/%y')) AS month_number,\n  YEAR(STR_TO_DATE(week_date, '%d/%m/%y')) AS calendar_year,\n  region,\n  platform,\n  CASE\n    WHEN RIGHT(segment, 1) = '1' THEN 'Young Adults'\n    WHEN RIGHT(segment, 1) = '2' THEN 'Middle Aged'\n    WHEN RIGHT(segment, 1) IN ('3', '4') THEN 'Retirees'\n    ELSE 'unknown' END AS age_band,\n  CASE\n    WHEN LEFT(segment, 1) = 'C' THEN 'Couples'\n    WHEN LEFT(segment, 1) = 'F' THEN 'Families'\n    ELSE 'unknown' END AS demographic,\n  COALESCE(NULLIF(segment, ''), 'unknown') AS segment,\n  transactions,\n  ROUND((sales / transactions), 2) AS avg_transaction,\n  sales\nFROM data_mart.weekly_sales\n);\n\nUPDATE data_mart.clean_weekly_sales\nSET age_band = 'unknown'\nWHERE age_band IS NULL;\n\nUPDATE data_mart.clean_weekly_sales\nSET demographic = 'unknown'\nWHERE demographic IS NULL;\n\nUPDATE data_mart.clean_weekly_sales\nSET segment = 'unknown'\nWHERE segment IS NULL;\n\n\nSELECT * FROM clean_weekly_sales\nLIMIT 15;\nOutput:\nThe first 15 records are shown as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek_date\nweek_number\nmonth_number\ncalendar_year\nregion\nplatform\nage_band\ndemographic_segment\ntransactions\navg_transaction\nsales\n\n\n\n\n2020-08-31\n35\n8\n2020\nASIA\nRetail\nRetirees\nCouples\n120631\n30.31\n3656163\n\n\n2020-08-31\n35\n8\n2020\nASIA\nRetail\nYoung Adults\nFamilies\n31574\n31.56\n996575\n\n\n2020-08-31\n35\n8\n2020\nUSA\nRetail\nunknown\nunknown\nnull\n529151\n31.20\n\n\n2020-08-31\n35\n8\n2020\nEUROPE\nRetail\nYoung Adults\nCouples\n4517\n31.42\n141942\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nRetail\nMiddle Aged\nCouples\n58046\n30.29\n1758388\n\n\n2020-08-31\n35\n8\n2020\nCANADA\nShopify\nMiddle Aged\nFamilies\n1336\n182.54\n243878\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nShopify\nRetirees\nFamilies\n2514\n206.64\n519502\n\n\n2020-08-31\n35\n8\n2020\nASIA\nShopify\nYoung Adults\nFamilies\n2158\n172.11\n371417\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nShopify\nMiddle Aged\nFamilies\n318\n155.84\n49557\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nRetail\nRetirees\nCouples\n111032\n35.02\n3888162\n\n\n2020-08-31\n35\n8\n2020\nUSA\nShopify\nYoung Adults\nFamilies\n1398\n186.53\n260773\n\n\n2020-08-31\n35\n8\n2020\nOCEANIA\nShopify\nMiddle Aged\nCouples\n4661\n189.38\n882690\n\n\n2020-08-31\n35\n8\n2020\nSOUTH AMERICA\nRetail\nMiddle Aged\nCouples\n1029\n37.67\n38762\n\n\n2020-08-31\n35\n8\n2020\nSOUTH AMERICA\nShopify\nRetirees\nCouples\n6\n152.83\n917\n\n\n2020-08-31\n35\n8\n2020\nEUROPE\nShopify\nRetirees\nFamilies\n115\n306.22\n35215\n\n\n\n\n\n\n\n\n\nSELECT\n    DISTINCT DAYNAME(week_date) AS Day_Name\nFROM clean_weekly_sales;\nOutput:\n\n\n\nDay_name\n\n\n\n\nMonday\n\n\n\n\n\n\nInsight:\n\nThe day of the week associated with each week_date value in the weekly_sales dataset is consistently Monday.\n\n\n\n\n\n\nWITH RECURSIVE NumbersSeries AS (\n    SELECT 1 AS week_number\n    UNION ALL\n    SELECT week_number + 1\n    FROM NumbersSeries\n    WHERE week_number &lt; 52\n)\nSELECT NumbersSeries.week_number\nFROM NumbersSeries\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM clean_weekly_sales\n    WHERE week_number = NumbersSeries.week_number\n);\nOutput:\n\n\n\nweek_number\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n\n\n\n\nInsight:\n\nSeveral week numbers are missing from the dataset, ranging from week 1 to week 11 and week 36 to week 52.\n\n\n\n\n\n\nSELECT calendar_year,\n    SUM(transactions) as total_transactions\nFROM clean_weekly_sales\nGROUP BY calendar_year\nORDER BY calendar_year;\nOutput:\n\n\n\ncalendar_year\ntotal_transactions\n\n\n\n\n2018\n346,406,460\n\n\n2019\n365,639,285\n\n\n2020\n375,813,651\n\n\n\n\n\n\nInsight:\n\nThe total number of transactions has shown a consistent upward trend over the years, with 2018 starting at a lower value compared to subsequent years.\n\n\n\n\n\n\nSELECT region, month_number,\n    SUM(sales) as total_sales\nFROM clean_weekly_sales\nGROUP BY region, month_number\nORDER BY region, month_number;\nOutput:\n\n\n\nregion\nmonth_number\ntotal_sales\n\n\n\n\nAFRICA\n3\n567,767,480\n\n\nAFRICA\n4\n1,911,783,504\n\n\nAFRICA\n5\n1,647,244,738\n\n\nAFRICA\n6\n1,767,559,760\n\n\nAFRICA\n7\n1,960,219,710\n\n\nAFRICA\n8\n1,809,596,890\n\n\nAFRICA\n9\n276,320,987\n\n\nASIA\n3\n529,770,793\n\n\nASIA\n4\n1,804,628,707\n\n\nASIA\n5\n1,526,285,399\n\n\nASIA\n6\n1,619,482,889\n\n\nASIA\n7\n1,768,844,756\n\n\nASIA\n8\n1,663,320,609\n\n\nASIA\n9\n252,836,807\n\n\nCANADA\n3\n144,634,329\n\n\nCANADA\n4\n484,552,594\n\n\nCANADA\n5\n412,378,365\n\n\nCANADA\n6\n443,846,698\n\n\nCANADA\n7\n477,134,947\n\n\nCANADA\n8\n447,073,019\n\n\nCANADA\n9\n69,067,959\n\n\nEUROPE\n3\n35,337,093\n\n\nEUROPE\n4\n127,334,255\n\n\nEUROPE\n5\n109,338,389\n\n\nEUROPE\n6\n122,813,826\n\n\nEUROPE\n7\n136,757,466\n\n\nEUROPE\n8\n122,102,995\n\n\nEUROPE\n9\n18,877,433\n\n\nOCEANIA\n3\n783,282,888\n\n\nOCEANIA\n4\n2,599,767,620\n\n\nOCEANIA\n5\n2,215,657,304\n\n\nOCEANIA\n6\n2,371,884,744\n\n\nOCEANIA\n7\n2,563,459,400\n\n\nOCEANIA\n8\n2,432,313,652\n\n\nOCEANIA\n9\n372,465,518\n\n\nSOUTH AMERICA\n3\n71,023,109\n\n\nSOUTH AMERICA\n4\n238,451,531\n\n\nSOUTH AMERICA\n5\n201,391,809\n\n\nSOUTH AMERICA\n6\n218,247,455\n\n\nSOUTH AMERICA\n7\n235,582,776\n\n\nSOUTH AMERICA\n8\n221,166,052\n\n\nSOUTH AMERICA\n9\n34,175,583\n\n\nUSA\n3\n225,353,043\n\n\nUSA\n4\n759,786,323\n\n\nUSA\n5\n655,967,121\n\n\nUSA\n6\n703,878,990\n\n\nUSA\n7\n760,331,754\n\n\nUSA\n8\n712,002,790\n\n\nUSA\n9\n110,532,368\n\n\n\n\n\n\nInsight:\n\nSales performance varies across regions and months, with some regions consistently outperforming others.\n\nOverview:\n\nAfrica: Shows a consistent increase in sales from March to July before a slight decline in August and September.\nAsia: Exhibits a similar trend to Africa, with increasing sales until July followed by a slight decrease in August and September.\nCanada: Sales follow a pattern similar to Africa and Asia, with a peak in July followed by a decline in August and September.\nEurope: Shows a steady increase in sales from March to July, followed by a slight decrease in August and September.\nOceania: Shows the highest sales among all regions, with a peak in July followed by a decline in August and September.\nSouth America: Sales follow a similar pattern to other regions, with a peak in July followed by a decline in August and September.\nUSA: Shows a consistent increase in sales from March to July before a slight decline in August and September.\n\nRegional Variations:\n\nOceania: consistently records the highest sales, indicating strong demand or market presence in that region.\nEurope: Reports the lowest sales compared to other regions, suggesting potential opportunities for growth or market penetration strategies.\n\nSeasonal Trends:\n\nThe months of March to July generally witness higher sales across all regions, suggesting potential seasonal factors or marketing campaigns driving increased consumer spending during this period.\nThe decline in sales observed in August and September across most regions could be attributed to seasonal factors, economic conditions, or specific market dynamics.\n\nRecommendations:\n\nData Mart should analyze the factors contributing to the peak sales months to identify successful strategies and replicate them in other periods or regions.\nTargeted marketing campaigns or promotions could be implemented during periods of lower sales to stimulate demand and boost revenue.\nUnderstanding regional preferences and consumer behavior can help tailor marketing strategies and product offerings to maximize sales potential in each market.\n\n\n\n\n\n\nSELECT platform,\n    SUM(transactions) as total_transactions\nFROM clean_weekly_sales\nGROUP BY platform\nORDER BY platform;\nOutput:\n\n\n\nplatform\ntotal_transactions\n\n\n\n\nRetail\n1,081,934,227\n\n\nShopify\n5,925,169\n\n\n\n\n\n\nInsight:\n\nThe majority of transactions occur through the Retail platform compared to the Shopify platform.\n\nObservations:\n\nThe higher transaction count on the Retail platform suggests a strong presence in physical retail locations or a larger customer base utilizing traditional retail channels.\nThe lower transaction count on the Shopify platform may indicate a smaller but growing segment of customers preferring online shopping experiences.\n\nOpportunities:\n\nData Mart could focus on enhancing its online platform to capture a larger share of the digital market and compete more effectively with traditional retail channels.\nLeveraging data analytics and customer insights from both platforms can help optimize marketing strategies and product offerings to target specific customer segments more effectively.\n\n\n\n\n\n\nWITH platform_sales AS\n(SELECT calendar_year, month_number,\n    SUM(CASE WHEN platform = 'Retail' THEN sales ELSE 0 END) AS retail_sales,\n    SUM(CASE WHEN platform = 'Shopify' THEN sales ELSE 0 END) AS shopify_sales,\n    SUM(sales) AS total_sales\nFROM clean_weekly_sales\nGROUP BY calendar_year, month_number\nORDER BY calendar_year, month_number)\nSELECT calendar_year, month_number,\n    ROUND(100.0 * (retail_sales/total_sales), 2) AS retail_sales_pct,\n    ROUND(100.0 * (shopify_sales/total_sales), 2) AS shopify_sales_pct\nFROM platform_sales;\nOutput:\n\n\n\n\n\n\n\n\n\ncalendar_year\nmonth_number\nretail_sales_pct\nshopify_sales_pct\n\n\n\n\n2018\n3\n97.92\n2.08\n\n\n2018\n4\n97.93\n2.07\n\n\n2018\n5\n97.73\n2.27\n\n\n2018\n6\n97.76\n2.24\n\n\n2018\n7\n97.75\n2.25\n\n\n2018\n8\n97.71\n2.29\n\n\n2018\n9\n97.68\n2.32\n\n\n2019\n3\n97.71\n2.29\n\n\n2019\n4\n97.80\n2.20\n\n\n2019\n5\n97.52\n2.48\n\n\n2019\n6\n97.42\n2.58\n\n\n2019\n7\n97.35\n2.65\n\n\n2019\n8\n97.21\n2.79\n\n\n2019\n9\n97.09\n2.91\n\n\n2020\n3\n97.30\n2.70\n\n\n2020\n4\n96.96\n3.04\n\n\n2020\n5\n96.71\n3.29\n\n\n2020\n6\n96.80\n3.20\n\n\n2020\n7\n96.67\n3.33\n\n\n2020\n8\n96.51\n3.49\n\n\n\n\n\n\nInsight:\n\nThe percentage of sales from the Retail platform consistently dominates over Shopify platform across monthsand years,indicatingthe continued siginificance of Physical stores. However, Shopify’s contributionto total sales has been steadily increasing overtime, suggesting an opportunity to grow online sales.\n\n\n\n\n\n\nWITH demographic_sales AS\n(SELECT calendar_year,\n    SUM(CASE WHEN demographic = 'Couples' THEN sales ELSE 0 END) AS couples_sales,\n    SUM(CASE WHEN demographic = 'Families' THEN sales ELSE 0 END) AS families_sales,\n    SUM(CASE WHEN demographic = 'unknown' THEN sales ELSE 0 END) AS unknown_sales,\n    SUM(sales) AS total_sales\nFROM clean_weekly_sales\nGROUP BY calendar_year\nORDER BY calendar_year)\nSELECT calendar_year,\n    ROUND(100.0 * (couples_sales/total_sales), 2) AS couples_sales_pct,\n    ROUND(100.0 * (families_sales/total_sales), 2) AS families_sales_pct,\n    ROUND(100.0 * (unknown_sales/total_sales), 2) AS unknown_sales_pct\nFROM demographic_sales;\nOutput:\n\n\n\n\n\n\n\n\n\ncalendar_year\ncouples_sales_pct\nfamilies_sales_pct\nunknown_sales_pct\n\n\n\n\n2018\n26.38\n31.99\n41.63\n\n\n2019\n27.28\n32.47\n40.25\n\n\n2020\n28.72\n32.73\n38.55\n\n\n\n\n\n\nInsight:\n\nCouples consistently contribute the least to total sales across all years, with families and unknown demographics making up the majority.\n\n\n\n\n\n\nSELECT age_band, demographic,\n    ROUND(SUM(CASE WHEN platform = 'Retail' THEN sales ELSE 0 END),2) AS retail_sales\nFROM clean_weekly_sales\nGROUP BY 1,2\nORDER BY retail_sales DESC;\nOutput:\n\n\n\nage_band\ndemographic\nretail_sales\n\n\n\n\nunknown\nunknown\n16067285533\n\n\nRetirees\nFamilies\n6634686916\n\n\nRetirees\nCouples\n6370580014\n\n\nMiddle Aged\nFamilies\n4354091554\n\n\nYoung Adults\nCouples\n2602922797\n\n\nMiddle Aged\nCouples\n1854160330\n\n\nYoung Adults\nFamilies\n1770889293\n\n\n\n\n\n\nInsight:\n\nThe demographic category “Unknown” contributes significantly more to retail sales compared to other age bands and demographics, indicating that a large portion of sales data lacks detailed demographic information.\nAmong known demographics, retirees, especially those in families, contribute significantly to retail sales, followed by middle-aged customers, primarily in families and couples.\nYoung adults, both in couples and families, contribute less compared to retirees and middle-aged individuals.\n\n\n\n\n\n\nSELECT calendar_year, platform,\n        ROUND(AVG(avg_transaction), 2) AS avg_transactions_1,\n        ROUND(SUM(sales)/SUM(transactions), 2) AS avg_transactions_2\nFROM clean_weekly_sales\nGROUP BY calendar_year, platform\nORDER BY calendar_year, platform;\nOutput:\n\n\n\ncalendar_year\nplatform\navg_transactions_1\navg_transactions_2\n\n\n\n\n2018\nRetail\n42.91\n36.56\n\n\n2018\nShopify\n188.28\n192.48\n\n\n2019\nRetail\n41.97\n36.83\n\n\n2019\nShopify\n177.56\n183.36\n\n\n2020\nRetail\n40.64\n36.56\n\n\n2020\nShopify\n174.87\n179.03\n\n\n\n\n\n\nInsights:\n\nAvg_Transactions_1: Represents the average value of the avg_transaction column directly. This approach yields different results for each platform, reflecting the variability in average transaction sizes within each platform.\nAvg_Transactions_2: Calculated by dividing the total sales by the total number of transactions. This method provides a more accurate reflection of the average transaction size, considering the actual sales amounts and transaction counts. It ensures consistency in calculation across platforms, making it a more reliable metric for comparison.\n\n\n\n\n\n\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n\n\n\nSELECT DISTINCT(week_number) AS baseline_week_number FROM clean_weekly_sales\nWHERE week_date = '2020-06-15' AND calendar_year = '2020';\n\nWITH before_and_after_data AS\n(SELECT week_date,\n        week_number,\n        SUM(sales) AS total_sales\n        FROM clean_weekly_sales\n    WHERE (week_number BETWEEN 20 AND 27) AND (calendar_year = '2020')\n    GROUP BY 1,2\n    ORDER BY 1,2\n),\nsales_calculation_table_before_and_after AS (\nSELECT\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n        FROM before_and_after_data\n        )\n    SELECT\n        sales_before_baseline_date_value, sales_after_baseline_date_value,\n        (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n        ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/sales_before_baseline_date_value,2) AS pct\n    FROM sales_calculation_table_before_and_after;\nOutput:\nThe Baseline week_number is 24.\n\n\n\n\n\n\n\n\n\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nInsights:\n\nThe total sales decreased by approximately $26.88 million after the baseline date compared to the four weeks before.\nThis corresponds to a reduction of approximately 1.15% in sales during the four weeks after the baseline date compared to the four weeks before.\n\n\n\n\n\n\nWITH before_and_after_data AS\n(SELECT week_date, week_number, SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE calendar_year = '2020'\nGROUP BY week_date, week_number\nORDER BY week_date, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT\n        SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data)\nSELECT\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n7126273147\n6973947753\n-152325394\n-2.14\n\n\n\n\n\n\nInsights:\n\nThe total sales decreased by approximately $152.33 million over the entire 12 weeks after the baseline date compared to the 12 weeks before.\nThis corresponds to a reduction of approximately 2.14% in sales during the 12 weeks after the baseline date compared to the 12 weeks before.\n\n\n\n\n\n\nPart 3.1: How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019 for 4 weeks period?\nWITH before_and_after_data AS\n(SELECT calendar_year, week_number,\n    SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE (week_number BETWEEN 20 AND 27)\nGROUP BY calendar_year, week_number\nORDER BY calendar_year, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT calendar_year,\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data\n    GROUP BY calendar_year\n    )\nSELECT calendar_year,\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)\n    /sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n2125140809\n2129242914\n4102105\n0.19\n\n\n2019\n2249989796\n2252326390\n2336594\n0.10\n\n\n2020\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nOverall Trend:\n\nWhile there were slight increases in sales in the 4 weeks after compared to before for both 2018 and 2019, there was a noticeable decrease in sales in 2020, indicating a negative impact on sales during this period compared to previous years.\n\n\n\n\n\n\nPart 3.1: How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019 for 4 weeks period?\nWITH before_and_after_data AS\n(SELECT calendar_year, week_number,\n    SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE (week_number BETWEEN 20 AND 27)\nGROUP BY calendar_year, week_number\nORDER BY calendar_year, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT calendar_year,\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data\n    GROUP BY calendar_year\n    )\nSELECT calendar_year,\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)\n    /sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n2125140809\n2129242914\n4102105\n0.19\n\n\n2019\n2249989796\n2252326390\n2336594\n0.10\n\n\n2020\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nOverall Trend:\n\nWhile there were slight increases in sales in the 4 weeks after compared to before for both 2018 and 2019, there was a noticeable decrease in sales in 2020, indicating a negative impact on sales during this period compared to previous years.\n\n\n\n\n\n\nWITH before_and_after_data AS\n    (SELECT calendar_year, week_number,\n        SUM(sales) AS total_sales\n        FROM clean_weekly_sales\n    GROUP BY calendar_year, week_number\n    ORDER BY calendar_year, week_number\n    ), sales_calculation_table_before_and_after AS (\n        SELECT calendar_year,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n        FROM before_and_after_data\n        GROUP BY calendar_year\n        )\n    SELECT calendar_year,\n        sales_before_baseline_date_value, sales_after_baseline_date_value,\n        (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n        ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n            sales_before_baseline_date_value,2) AS pct\n    FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n6396562317\n6500818510\n104256193\n1.63\n\n\n2019\n6883386397\n6862646103\n-20740294\n-0.30\n\n\n2020\n7126273147\n6973947753\n-152325394\n-2.14\n\n\n\n\n\n\nOverall Trend:\n\nWhile there was a significant increase in sales in 2018 and a slight decrease in 2019, 2020 saw a notable decrease in sales during this period compared to previous years. This suggests a significant negative impact on sales in 2020 compared to earlier years.\n\n\n\n\n\n\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?\n\n\n\n-- region\n-- platform\n-- age_band\n-- demographic\n-- customer_type\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?\nAnswer:\nSales metric performance across region.\nWITH before_and_after_data AS\n       (SELECT region,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT region,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY region\n            )\n        SELECT region,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nRegion\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nAFRICA\n4942976910\n4997516159\n54539249\n1.10\n\n\nASIA\n4613242689\n4551927271\n-61315418\n-1.33\n\n\nCANADA\n1244662705\n1234025206\n-10637499\n-0.85\n\n\nEUROPE\n328141414\n344420043\n16278629\n4.96\n\n\nOCEANIA\n6698586333\n6640244793\n-58341540\n-0.87\n\n\nSOUTH AMERICA\n611056923\n608981392\n-2075531\n-0.34\n\n\nUSA\n1967554887\n1960297502\n-7257385\n-0.37\n\n\n\nSales metric performance across platform.\nWITH before_and_after_data AS\n       (SELECT platform,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT platform,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY platform\n            )\n        SELECT platform,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nPlatform\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nRetail\n19886040272\n19768576165\n-117464107\n-0.59\n\n\nShopify\n520181589\n568836201\n48654612\n9.35\n\n\n\nSales metric performance across age_band.\nWITH before_and_after_data AS\n       (SELECT age_band,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT age_band,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY age_band\n            )\n        SELECT age_band,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nAge Band\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nMiddle Aged\n3276892347\n3269748622\n-7143725\n-0.22\n\n\nRetirees\n6646865322\n6634706880\n-12158442\n-0.18\n\n\nUnknown\n8191628826\n8146983408\n-44645418\n-0.55\n\n\nYoung Adults\n2290835366\n2285973456\n-4861910\n-0.21\n\n\n\nSales metric performance across demographic.\nWITH before_and_after_data AS\n       (SELECT demographic,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT demographic,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY demographic\n            )\n        SELECT demographic,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nDemographic\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nCouples\n5608866131\n5592341420\n-16524711\n-0.29\n\n\nFamilies\n6605726904\n6598087538\n-7639366\n-0.12\n\n\nUnknown\n8191628826\n8146983408\n-44645418\n-0.55\n\n\n\nSales metric performance across customer_type.\nWITH before_and_after_data AS\n       (SELECT customer_type,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT customer_type,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY customer_type\n            )\n        SELECT customer_type,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nCustomer Type\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nExisting\n10168877642\n10117367239\n-51510403\n-0.51\n\n\nGuest\n7630353739\n7595150744\n-35202995\n-0.46\n\n\nNew\n2606990480\n2624894383\n17903903\n0.69\n\n\n\n\n\n\nSales Metric Performance Across Regions:\n\nAfrica: Experienced a slight increase in sales (1.10%) after the baseline date.\nAsia: Witnessed a decrease in sales (-1.33%) after the baseline date, indicating a negative impact.\nCanada: Saw a minor decrease in sales (-0.85%) after the baseline date.\nEurope: Showed a significant increase in sales (4.96%) after the baseline date, indicating positive performance.\nOceania: Experienced a decrease in sales (-0.87%) after the baseline date.\nSouth America: Saw a slight decrease in sales (-0.34%) after the baseline date.\nUSA: Witnessed a decrease in sales (-0.37%) after the baseline date.\n\nSales Metric Performance Across Platforms:\n\nRetail: Experienced a decrease in sales (-0.59%) after the baseline date.\nShopify: Witnessed a significant increase in sales (9.35%) after the baseline date, indicating positive performance.\n\nSales Metric Performance Across Age Bands:\n\nMiddle Aged: Experienced a minor decrease in sales (-0.22%) after the baseline date.\nRetirees: Saw a slight decrease in sales (-0.18%) after the baseline date.\nUnknown: Witnessed a notable decrease in sales (-0.55%) after the baseline date.\nYoung Adults: Experienced a minor decrease in sales (-0.21%) after the baseline date.\n\nSales Metric Performance Across Demographics:\n\nCouples: Experienced a slight decrease in sales (-0.29%) after the baseline date.\nFamilies: Saw a minor decrease in sales (-0.12%) after the baseline date.\nUnknown: Witnessed a notable decrease in sales (-0.55%) after the baseline date.\n\nSales Metric Performance Across Customer Types:\n\nExisting Customers: Experienced a decrease in sales (-0.51%) after the baseline date.\nGuest Customers: Witnessed a decrease in sales (-0.46%) after the baseline date.\nNew Customers: Saw an increase in sales (0.69%) after the baseline date, indicating positive performance.\n\n\n\n\n\n- Overall Sales Performance: The analysis reveals mixed performance across different regions, platforms, age bands, demographics, and customer types.\n- Positive Trends: Europe and Shopify platform showed significant increases in sales, suggesting areas of potential growth.\n- Negative Trends: Asia, Oceania, and certain demographic segments experienced decreases in sales, requiring further investigation into underlying factors.\n\n- Recommendations: Danny’s team should focus on leveraging positive trends, such as expanding operations in regions with growing sales and optimizing sales strategies for specific customer segments. Additionally, they should address issues contributing to sales declines, such as exploring new marketing strategies or improving product offerings. Regular monitoring and analysis of sales metrics across different business areas will be crucial for identifying opportunities and addressing challenges in the future."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#introduction",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "Data Mart is Danny’s latest venture and after running international operations for his online supermarket that specialises in fresh produce - Danny is asking for your support to analyse his sales performance.\nIn June 2020 - large scale supply changes were made at Data Mart. All Data Mart products now use sustainable packaging methods in every single step from the farm all the way to the customer.\nDanny needs your help to quantify the impact of this change on the sales performance for Data Mart and it’s separate business areas.\nThe key business question he wants you to help him answer are the following:\n\nWhat was the quantifiable impact of the changes introduced in June 2020?\nWhich platform, region, segment and customer types were the most impacted by this change?\nWhat can we do about future introduction of similar sustainability updates to the business to minimise impact on sales?"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#available-data",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#available-data",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "For this case study there is only a single table: data_mart.weekly_sales\nThe Entity Relationship Diagram is shown below with the data types made clear, please note that there is only this one table - hence why it looks a little bit lonely!\n\n\n\nThe columns are pretty self-explanatory based on the column names but here are some further details about the dataset:\n\nData Mart has international operations using a multi-region strategy\nData Mart has both, a retail and online platform in the form of a Shopify store front to serve their customers\nCustomer segment and customer_type data relates to personal age and demographics information that is shared with Data Mart.\ntransactions is the count of unique purchases made through Data Mart and sales is the actual dollar amount of purchases\n\nEach record in the dataset is related to a specific aggregated slice of the underlying sales data rolled up into a week_date value which represents the start of the sales week.\n\n\n\n10 random rows are shown in the table output below from data_mart.weekly_sales:\n\n\n\n\n\n\n\n\n\n\n\n\nweek_date\nregion\nplatform\nsegment\ncustomer_type\ntransactions\nsales\n\n\n\n\n9/9/20\nOCEANIA\nShopify\nC3\nNew\n610\n110033.89\n\n\n29/7/20\nAFRICA\nRetail\nC1\nNew\n110692\n3053771.19\n\n\n22/7/20\nEUROPE\nShopify\nC4\nExisting\n24\n8101.54\n\n\n13/5/20\nAFRICA\nShopify\nnull\nGuest\n5287\n1003301.37\n\n\n24/7/19\nASIA\nRetail\nC1\nNew\n127342\n3151780.41\n\n\n10/7/19\nCANADA\nShopify\nF3\nNew\n51\n8844.93\n\n\n26/6/19\nOCEANIA\nRetail\nC3\nNew\n152921\n5551385.36\n\n\n29/5/19\nSOUTH AMERICA\nShopify\nnull\nNew\n53\n10056.2\n\n\n22/8/18\nAFRICA\nRetail\nnull\nExisting\n31721\n1718863.58\n\n\n25/7/18\nSOUTH AMERICA\nRetail\nnull\nNew\n2136\n81757.91"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#interactive-sql-session",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#interactive-sql-session",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA data_mart;\nUSE data_mart;\n\nDROP TABLE IF EXISTS data_mart.weekly_sales;\nCREATE TABLE data_mart.weekly_sales (\n  `week_date` VARCHAR(7),\n  `region` VARCHAR(13),\n  `platform` VARCHAR(7),\n  `segment` VARCHAR(4),\n  `customer_type` VARCHAR(8),\n  `transactions` INTEGER,\n  `sales` INTEGER\n);\n\nINSERT INTO data_mart.weekly_sales\n  (`week_date`, `region`, `platform`, `segment`, `customer_type`, `transactions`, `sales`)\nVALUES\n  ('31/8/20', 'ASIA', 'Retail', 'C3', 'New', '120631', '3656163'),\n  ('31/8/20', 'ASIA', 'Retail', 'F1', 'New', '31574', '996575'),\n  ('31/8/20', 'USA', 'Retail', 'null', 'Guest', '529151', '16509610'),\n  ('31/8/20', 'EUROPE', 'Retail', 'C1', 'New', '4517', '141942'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C2', 'New', '58046', '1758388'),\n  ('31/8/20', 'CANADA', 'Shopify', 'F2', 'Existing', '1336', '243878'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F3', 'Existing', '2514', '519502'),\n  ('31/8/20', 'ASIA', 'Shopify', 'F1', 'Existing', '2158', '371417'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F2', 'New', '318', '49557'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C3', 'New', '111032', '3888162'),\n  ('31/8/20', 'USA', 'Shopify', 'F1', 'Existing', '1398', '260773'),\n  ('31/8/20', 'OCEANIA', 'Shopify', 'C2', 'Existing', '4661', '882690'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'C2', 'Existing', '1029', '38762'),\n  ('31/8/20', 'SOUTH AMERICA', 'Shopify', 'C4', 'New', '6', '917'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'F3', 'Existing', '115', '35215'),\n  ('31/8/20', 'OCEANIA', 'Retail', 'F3', 'Existing', '551905', '30371770'),\n  ('31/8/20', 'ASIA', 'Shopify', 'C3', 'Existing', '1969', '374327'),\n  ('31/8/20', 'AFRICA', 'Retail', 'F1', 'Existing', '97604', '5185233'),\n  ('31/8/20', 'OCEANIA', 'Retail', 'C2', 'New', '111219', '2980673'),\n  ('31/8/20', 'USA', 'Retail', 'F1', 'New', '11820', '463738'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'F3', 'Existing', '1363', '65730'),\n  ('31/8/20', 'AFRICA', 'Retail', 'C3', 'Existing', '284971', '14430196'),\n  ('31/8/20', 'ASIA', 'Retail', 'F2', 'New', '70496', '2176980'),\n  ('31/8/20', 'AFRICA', 'Shopify', 'F1', 'Existing', '2678', '478756'),\n  ('31/8/20', 'USA', 'Shopify', 'C4', 'New', '22', '3319'),\n  ('31/8/20', 'CANADA', 'Retail', 'F3', 'Existing', '94274', '5306746'),\n  ('31/8/20', 'ASIA', 'Retail', 'F1', 'Existing', '94287', '4511841'),\n  ('31/8/20', 'EUROPE', 'Retail', 'null', 'New', '3064', '134249'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'F1', 'New', '7', '1579'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'C4', 'New', '329', '11451'),\n  ('31/8/20', 'SOUTH AMERICA', 'Retail', 'F1', 'Existing', '854', '31589'),\n  ('31/8/20', 'EUROPE', 'Shopify', 'C2', 'Existing', '180', '53567')"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#case-study-questions",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "The following case study questions require some data cleaning steps before we start to unpack Danny’s key business questions in more depth.\n\n\nIn a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record\n\n\n\n\n\nWhat day of the week is used for each week_date value?\nWhat range of week numbers are missing from the dataset?\nHow many total transactions were there for each year in the dataset?\nWhat is the total sales for each region for each month?\nWhat is the total count of transactions for each platform\nWhat is the percentage of sales for Retail vs Shopify for each month?\nWhat is the percentage of sales by demographic for each year in the dataset?\nWhich age_band and demographic values contribute the most to Retail sales?\nCan we use the avg_transaction column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?\n\n\n\n\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n\nWhat is the total sales for the 4 weeks before and after 2020-06-15? What is the growth or reduction rate in actual values and percentage of sales?\nWhat about the entire 12 weeks before and after?\nHow do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019?\n\n\n\n\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#lets-start-solving-them.",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#lets-start-solving-them.",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "In a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#data-cleaning",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#data-cleaning",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "DROP TABLE IF EXISTS data_mart.clean_weekly_sales\n\nCREATE TABLE data_mart.clean_weekly_sales AS (\nSELECT\n  STR_TO_DATE(week_date, '%d/%m/%y') AS week_date,\n  WEEK(STR_TO_DATE(week_date, '%d/%m/%y')) AS week_number,\n  MONTH(STR_TO_DATE(week_date, '%d/%m/%y')) AS month_number,\n  YEAR(STR_TO_DATE(week_date, '%d/%m/%y')) AS calendar_year,\n  region,\n  platform,\n  CASE\n    WHEN RIGHT(segment, 1) = '1' THEN 'Young Adults'\n    WHEN RIGHT(segment, 1) = '2' THEN 'Middle Aged'\n    WHEN RIGHT(segment, 1) IN ('3', '4') THEN 'Retirees'\n    ELSE 'unknown' END AS age_band,\n  CASE\n    WHEN LEFT(segment, 1) = 'C' THEN 'Couples'\n    WHEN LEFT(segment, 1) = 'F' THEN 'Families'\n    ELSE 'unknown' END AS demographic,\n  COALESCE(NULLIF(segment, ''), 'unknown') AS segment,\n  transactions,\n  ROUND((sales / transactions), 2) AS avg_transaction,\n  sales\nFROM data_mart.weekly_sales\n);\n\nUPDATE data_mart.clean_weekly_sales\nSET age_band = 'unknown'\nWHERE age_band IS NULL;\n\nUPDATE data_mart.clean_weekly_sales\nSET demographic = 'unknown'\nWHERE demographic IS NULL;\n\nUPDATE data_mart.clean_weekly_sales\nSET segment = 'unknown'\nWHERE segment IS NULL;\n\n\nSELECT * FROM clean_weekly_sales\nLIMIT 15;\nOutput:\nThe first 15 records are shown as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek_date\nweek_number\nmonth_number\ncalendar_year\nregion\nplatform\nage_band\ndemographic_segment\ntransactions\navg_transaction\nsales\n\n\n\n\n2020-08-31\n35\n8\n2020\nASIA\nRetail\nRetirees\nCouples\n120631\n30.31\n3656163\n\n\n2020-08-31\n35\n8\n2020\nASIA\nRetail\nYoung Adults\nFamilies\n31574\n31.56\n996575\n\n\n2020-08-31\n35\n8\n2020\nUSA\nRetail\nunknown\nunknown\nnull\n529151\n31.20\n\n\n2020-08-31\n35\n8\n2020\nEUROPE\nRetail\nYoung Adults\nCouples\n4517\n31.42\n141942\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nRetail\nMiddle Aged\nCouples\n58046\n30.29\n1758388\n\n\n2020-08-31\n35\n8\n2020\nCANADA\nShopify\nMiddle Aged\nFamilies\n1336\n182.54\n243878\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nShopify\nRetirees\nFamilies\n2514\n206.64\n519502\n\n\n2020-08-31\n35\n8\n2020\nASIA\nShopify\nYoung Adults\nFamilies\n2158\n172.11\n371417\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nShopify\nMiddle Aged\nFamilies\n318\n155.84\n49557\n\n\n2020-08-31\n35\n8\n2020\nAFRICA\nRetail\nRetirees\nCouples\n111032\n35.02\n3888162\n\n\n2020-08-31\n35\n8\n2020\nUSA\nShopify\nYoung Adults\nFamilies\n1398\n186.53\n260773\n\n\n2020-08-31\n35\n8\n2020\nOCEANIA\nShopify\nMiddle Aged\nCouples\n4661\n189.38\n882690\n\n\n2020-08-31\n35\n8\n2020\nSOUTH AMERICA\nRetail\nMiddle Aged\nCouples\n1029\n37.67\n38762\n\n\n2020-08-31\n35\n8\n2020\nSOUTH AMERICA\nShopify\nRetirees\nCouples\n6\n152.83\n917\n\n\n2020-08-31\n35\n8\n2020\nEUROPE\nShopify\nRetirees\nFamilies\n115\n306.22\n35215"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#data-exploration-1",
    "href": "portfolio/8WeeksSQLChallenge-Data_Mart/index.html#data-exploration-1",
    "title": "Case Study #5 - Data Mart.",
    "section": "",
    "text": "SELECT\n    DISTINCT DAYNAME(week_date) AS Day_Name\nFROM clean_weekly_sales;\nOutput:\n\n\n\nDay_name\n\n\n\n\nMonday\n\n\n\n\n\n\nInsight:\n\nThe day of the week associated with each week_date value in the weekly_sales dataset is consistently Monday.\n\n\n\n\n\n\nWITH RECURSIVE NumbersSeries AS (\n    SELECT 1 AS week_number\n    UNION ALL\n    SELECT week_number + 1\n    FROM NumbersSeries\n    WHERE week_number &lt; 52\n)\nSELECT NumbersSeries.week_number\nFROM NumbersSeries\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM clean_weekly_sales\n    WHERE week_number = NumbersSeries.week_number\n);\nOutput:\n\n\n\nweek_number\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n\n\n\n\nInsight:\n\nSeveral week numbers are missing from the dataset, ranging from week 1 to week 11 and week 36 to week 52.\n\n\n\n\n\n\nSELECT calendar_year,\n    SUM(transactions) as total_transactions\nFROM clean_weekly_sales\nGROUP BY calendar_year\nORDER BY calendar_year;\nOutput:\n\n\n\ncalendar_year\ntotal_transactions\n\n\n\n\n2018\n346,406,460\n\n\n2019\n365,639,285\n\n\n2020\n375,813,651\n\n\n\n\n\n\nInsight:\n\nThe total number of transactions has shown a consistent upward trend over the years, with 2018 starting at a lower value compared to subsequent years.\n\n\n\n\n\n\nSELECT region, month_number,\n    SUM(sales) as total_sales\nFROM clean_weekly_sales\nGROUP BY region, month_number\nORDER BY region, month_number;\nOutput:\n\n\n\nregion\nmonth_number\ntotal_sales\n\n\n\n\nAFRICA\n3\n567,767,480\n\n\nAFRICA\n4\n1,911,783,504\n\n\nAFRICA\n5\n1,647,244,738\n\n\nAFRICA\n6\n1,767,559,760\n\n\nAFRICA\n7\n1,960,219,710\n\n\nAFRICA\n8\n1,809,596,890\n\n\nAFRICA\n9\n276,320,987\n\n\nASIA\n3\n529,770,793\n\n\nASIA\n4\n1,804,628,707\n\n\nASIA\n5\n1,526,285,399\n\n\nASIA\n6\n1,619,482,889\n\n\nASIA\n7\n1,768,844,756\n\n\nASIA\n8\n1,663,320,609\n\n\nASIA\n9\n252,836,807\n\n\nCANADA\n3\n144,634,329\n\n\nCANADA\n4\n484,552,594\n\n\nCANADA\n5\n412,378,365\n\n\nCANADA\n6\n443,846,698\n\n\nCANADA\n7\n477,134,947\n\n\nCANADA\n8\n447,073,019\n\n\nCANADA\n9\n69,067,959\n\n\nEUROPE\n3\n35,337,093\n\n\nEUROPE\n4\n127,334,255\n\n\nEUROPE\n5\n109,338,389\n\n\nEUROPE\n6\n122,813,826\n\n\nEUROPE\n7\n136,757,466\n\n\nEUROPE\n8\n122,102,995\n\n\nEUROPE\n9\n18,877,433\n\n\nOCEANIA\n3\n783,282,888\n\n\nOCEANIA\n4\n2,599,767,620\n\n\nOCEANIA\n5\n2,215,657,304\n\n\nOCEANIA\n6\n2,371,884,744\n\n\nOCEANIA\n7\n2,563,459,400\n\n\nOCEANIA\n8\n2,432,313,652\n\n\nOCEANIA\n9\n372,465,518\n\n\nSOUTH AMERICA\n3\n71,023,109\n\n\nSOUTH AMERICA\n4\n238,451,531\n\n\nSOUTH AMERICA\n5\n201,391,809\n\n\nSOUTH AMERICA\n6\n218,247,455\n\n\nSOUTH AMERICA\n7\n235,582,776\n\n\nSOUTH AMERICA\n8\n221,166,052\n\n\nSOUTH AMERICA\n9\n34,175,583\n\n\nUSA\n3\n225,353,043\n\n\nUSA\n4\n759,786,323\n\n\nUSA\n5\n655,967,121\n\n\nUSA\n6\n703,878,990\n\n\nUSA\n7\n760,331,754\n\n\nUSA\n8\n712,002,790\n\n\nUSA\n9\n110,532,368\n\n\n\n\n\n\nInsight:\n\nSales performance varies across regions and months, with some regions consistently outperforming others.\n\nOverview:\n\nAfrica: Shows a consistent increase in sales from March to July before a slight decline in August and September.\nAsia: Exhibits a similar trend to Africa, with increasing sales until July followed by a slight decrease in August and September.\nCanada: Sales follow a pattern similar to Africa and Asia, with a peak in July followed by a decline in August and September.\nEurope: Shows a steady increase in sales from March to July, followed by a slight decrease in August and September.\nOceania: Shows the highest sales among all regions, with a peak in July followed by a decline in August and September.\nSouth America: Sales follow a similar pattern to other regions, with a peak in July followed by a decline in August and September.\nUSA: Shows a consistent increase in sales from March to July before a slight decline in August and September.\n\nRegional Variations:\n\nOceania: consistently records the highest sales, indicating strong demand or market presence in that region.\nEurope: Reports the lowest sales compared to other regions, suggesting potential opportunities for growth or market penetration strategies.\n\nSeasonal Trends:\n\nThe months of March to July generally witness higher sales across all regions, suggesting potential seasonal factors or marketing campaigns driving increased consumer spending during this period.\nThe decline in sales observed in August and September across most regions could be attributed to seasonal factors, economic conditions, or specific market dynamics.\n\nRecommendations:\n\nData Mart should analyze the factors contributing to the peak sales months to identify successful strategies and replicate them in other periods or regions.\nTargeted marketing campaigns or promotions could be implemented during periods of lower sales to stimulate demand and boost revenue.\nUnderstanding regional preferences and consumer behavior can help tailor marketing strategies and product offerings to maximize sales potential in each market.\n\n\n\n\n\n\nSELECT platform,\n    SUM(transactions) as total_transactions\nFROM clean_weekly_sales\nGROUP BY platform\nORDER BY platform;\nOutput:\n\n\n\nplatform\ntotal_transactions\n\n\n\n\nRetail\n1,081,934,227\n\n\nShopify\n5,925,169\n\n\n\n\n\n\nInsight:\n\nThe majority of transactions occur through the Retail platform compared to the Shopify platform.\n\nObservations:\n\nThe higher transaction count on the Retail platform suggests a strong presence in physical retail locations or a larger customer base utilizing traditional retail channels.\nThe lower transaction count on the Shopify platform may indicate a smaller but growing segment of customers preferring online shopping experiences.\n\nOpportunities:\n\nData Mart could focus on enhancing its online platform to capture a larger share of the digital market and compete more effectively with traditional retail channels.\nLeveraging data analytics and customer insights from both platforms can help optimize marketing strategies and product offerings to target specific customer segments more effectively.\n\n\n\n\n\n\nWITH platform_sales AS\n(SELECT calendar_year, month_number,\n    SUM(CASE WHEN platform = 'Retail' THEN sales ELSE 0 END) AS retail_sales,\n    SUM(CASE WHEN platform = 'Shopify' THEN sales ELSE 0 END) AS shopify_sales,\n    SUM(sales) AS total_sales\nFROM clean_weekly_sales\nGROUP BY calendar_year, month_number\nORDER BY calendar_year, month_number)\nSELECT calendar_year, month_number,\n    ROUND(100.0 * (retail_sales/total_sales), 2) AS retail_sales_pct,\n    ROUND(100.0 * (shopify_sales/total_sales), 2) AS shopify_sales_pct\nFROM platform_sales;\nOutput:\n\n\n\n\n\n\n\n\n\ncalendar_year\nmonth_number\nretail_sales_pct\nshopify_sales_pct\n\n\n\n\n2018\n3\n97.92\n2.08\n\n\n2018\n4\n97.93\n2.07\n\n\n2018\n5\n97.73\n2.27\n\n\n2018\n6\n97.76\n2.24\n\n\n2018\n7\n97.75\n2.25\n\n\n2018\n8\n97.71\n2.29\n\n\n2018\n9\n97.68\n2.32\n\n\n2019\n3\n97.71\n2.29\n\n\n2019\n4\n97.80\n2.20\n\n\n2019\n5\n97.52\n2.48\n\n\n2019\n6\n97.42\n2.58\n\n\n2019\n7\n97.35\n2.65\n\n\n2019\n8\n97.21\n2.79\n\n\n2019\n9\n97.09\n2.91\n\n\n2020\n3\n97.30\n2.70\n\n\n2020\n4\n96.96\n3.04\n\n\n2020\n5\n96.71\n3.29\n\n\n2020\n6\n96.80\n3.20\n\n\n2020\n7\n96.67\n3.33\n\n\n2020\n8\n96.51\n3.49\n\n\n\n\n\n\nInsight:\n\nThe percentage of sales from the Retail platform consistently dominates over Shopify platform across monthsand years,indicatingthe continued siginificance of Physical stores. However, Shopify’s contributionto total sales has been steadily increasing overtime, suggesting an opportunity to grow online sales.\n\n\n\n\n\n\nWITH demographic_sales AS\n(SELECT calendar_year,\n    SUM(CASE WHEN demographic = 'Couples' THEN sales ELSE 0 END) AS couples_sales,\n    SUM(CASE WHEN demographic = 'Families' THEN sales ELSE 0 END) AS families_sales,\n    SUM(CASE WHEN demographic = 'unknown' THEN sales ELSE 0 END) AS unknown_sales,\n    SUM(sales) AS total_sales\nFROM clean_weekly_sales\nGROUP BY calendar_year\nORDER BY calendar_year)\nSELECT calendar_year,\n    ROUND(100.0 * (couples_sales/total_sales), 2) AS couples_sales_pct,\n    ROUND(100.0 * (families_sales/total_sales), 2) AS families_sales_pct,\n    ROUND(100.0 * (unknown_sales/total_sales), 2) AS unknown_sales_pct\nFROM demographic_sales;\nOutput:\n\n\n\n\n\n\n\n\n\ncalendar_year\ncouples_sales_pct\nfamilies_sales_pct\nunknown_sales_pct\n\n\n\n\n2018\n26.38\n31.99\n41.63\n\n\n2019\n27.28\n32.47\n40.25\n\n\n2020\n28.72\n32.73\n38.55\n\n\n\n\n\n\nInsight:\n\nCouples consistently contribute the least to total sales across all years, with families and unknown demographics making up the majority.\n\n\n\n\n\n\nSELECT age_band, demographic,\n    ROUND(SUM(CASE WHEN platform = 'Retail' THEN sales ELSE 0 END),2) AS retail_sales\nFROM clean_weekly_sales\nGROUP BY 1,2\nORDER BY retail_sales DESC;\nOutput:\n\n\n\nage_band\ndemographic\nretail_sales\n\n\n\n\nunknown\nunknown\n16067285533\n\n\nRetirees\nFamilies\n6634686916\n\n\nRetirees\nCouples\n6370580014\n\n\nMiddle Aged\nFamilies\n4354091554\n\n\nYoung Adults\nCouples\n2602922797\n\n\nMiddle Aged\nCouples\n1854160330\n\n\nYoung Adults\nFamilies\n1770889293\n\n\n\n\n\n\nInsight:\n\nThe demographic category “Unknown” contributes significantly more to retail sales compared to other age bands and demographics, indicating that a large portion of sales data lacks detailed demographic information.\nAmong known demographics, retirees, especially those in families, contribute significantly to retail sales, followed by middle-aged customers, primarily in families and couples.\nYoung adults, both in couples and families, contribute less compared to retirees and middle-aged individuals.\n\n\n\n\n\n\nSELECT calendar_year, platform,\n        ROUND(AVG(avg_transaction), 2) AS avg_transactions_1,\n        ROUND(SUM(sales)/SUM(transactions), 2) AS avg_transactions_2\nFROM clean_weekly_sales\nGROUP BY calendar_year, platform\nORDER BY calendar_year, platform;\nOutput:\n\n\n\ncalendar_year\nplatform\navg_transactions_1\navg_transactions_2\n\n\n\n\n2018\nRetail\n42.91\n36.56\n\n\n2018\nShopify\n188.28\n192.48\n\n\n2019\nRetail\n41.97\n36.83\n\n\n2019\nShopify\n177.56\n183.36\n\n\n2020\nRetail\n40.64\n36.56\n\n\n2020\nShopify\n174.87\n179.03\n\n\n\n\n\n\nInsights:\n\nAvg_Transactions_1: Represents the average value of the avg_transaction column directly. This approach yields different results for each platform, reflecting the variability in average transaction sizes within each platform.\nAvg_Transactions_2: Calculated by dividing the total sales by the total number of transactions. This method provides a more accurate reflection of the average transaction size, considering the actual sales amounts and transaction counts. It ensures consistency in calculation across platforms, making it a more reliable metric for comparison.\n\n\n\n\n\n\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n\n\n\nSELECT DISTINCT(week_number) AS baseline_week_number FROM clean_weekly_sales\nWHERE week_date = '2020-06-15' AND calendar_year = '2020';\n\nWITH before_and_after_data AS\n(SELECT week_date,\n        week_number,\n        SUM(sales) AS total_sales\n        FROM clean_weekly_sales\n    WHERE (week_number BETWEEN 20 AND 27) AND (calendar_year = '2020')\n    GROUP BY 1,2\n    ORDER BY 1,2\n),\nsales_calculation_table_before_and_after AS (\nSELECT\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n        FROM before_and_after_data\n        )\n    SELECT\n        sales_before_baseline_date_value, sales_after_baseline_date_value,\n        (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n        ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/sales_before_baseline_date_value,2) AS pct\n    FROM sales_calculation_table_before_and_after;\nOutput:\nThe Baseline week_number is 24.\n\n\n\n\n\n\n\n\n\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nInsights:\n\nThe total sales decreased by approximately $26.88 million after the baseline date compared to the four weeks before.\nThis corresponds to a reduction of approximately 1.15% in sales during the four weeks after the baseline date compared to the four weeks before.\n\n\n\n\n\n\nWITH before_and_after_data AS\n(SELECT week_date, week_number, SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE calendar_year = '2020'\nGROUP BY week_date, week_number\nORDER BY week_date, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT\n        SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data)\nSELECT\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n7126273147\n6973947753\n-152325394\n-2.14\n\n\n\n\n\n\nInsights:\n\nThe total sales decreased by approximately $152.33 million over the entire 12 weeks after the baseline date compared to the 12 weeks before.\nThis corresponds to a reduction of approximately 2.14% in sales during the 12 weeks after the baseline date compared to the 12 weeks before.\n\n\n\n\n\n\nPart 3.1: How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019 for 4 weeks period?\nWITH before_and_after_data AS\n(SELECT calendar_year, week_number,\n    SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE (week_number BETWEEN 20 AND 27)\nGROUP BY calendar_year, week_number\nORDER BY calendar_year, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT calendar_year,\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data\n    GROUP BY calendar_year\n    )\nSELECT calendar_year,\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)\n    /sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n2125140809\n2129242914\n4102105\n0.19\n\n\n2019\n2249989796\n2252326390\n2336594\n0.10\n\n\n2020\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nOverall Trend:\n\nWhile there were slight increases in sales in the 4 weeks after compared to before for both 2018 and 2019, there was a noticeable decrease in sales in 2020, indicating a negative impact on sales during this period compared to previous years.\n\n\n\n\n\n\nPart 3.1: How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019 for 4 weeks period?\nWITH before_and_after_data AS\n(SELECT calendar_year, week_number,\n    SUM(sales) AS total_sales\n    FROM clean_weekly_sales\nWHERE (week_number BETWEEN 20 AND 27)\nGROUP BY calendar_year, week_number\nORDER BY calendar_year, week_number\n),\nsales_calculation_table_before_and_after AS (\n    SELECT calendar_year,\n        SUM(CASE WHEN week_number IN (20,21,22,23) THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n        SUM(CASE WHEN week_number IN (24,25,26,27) THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n    FROM before_and_after_data\n    GROUP BY calendar_year\n    )\nSELECT calendar_year,\n    sales_before_baseline_date_value, sales_after_baseline_date_value,\n    (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n    ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)\n    /sales_before_baseline_date_value,2) AS pct\nFROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n2125140809\n2129242914\n4102105\n0.19\n\n\n2019\n2249989796\n2252326390\n2336594\n0.10\n\n\n2020\n2345878357\n2318994169\n-26884188\n-1.15\n\n\n\n\n\n\nOverall Trend:\n\nWhile there were slight increases in sales in the 4 weeks after compared to before for both 2018 and 2019, there was a noticeable decrease in sales in 2020, indicating a negative impact on sales during this period compared to previous years.\n\n\n\n\n\n\nWITH before_and_after_data AS\n    (SELECT calendar_year, week_number,\n        SUM(sales) AS total_sales\n        FROM clean_weekly_sales\n    GROUP BY calendar_year, week_number\n    ORDER BY calendar_year, week_number\n    ), sales_calculation_table_before_and_after AS (\n        SELECT calendar_year,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n        FROM before_and_after_data\n        GROUP BY calendar_year\n        )\n    SELECT calendar_year,\n        sales_before_baseline_date_value, sales_after_baseline_date_value,\n        (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n        ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n            sales_before_baseline_date_value,2) AS pct\n    FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\ncalendar_year\nsales_before_baseline_date_value\nsales_after_baseline_date_value\ndifference\npct\n\n\n\n\n2018\n6396562317\n6500818510\n104256193\n1.63\n\n\n2019\n6883386397\n6862646103\n-20740294\n-0.30\n\n\n2020\n7126273147\n6973947753\n-152325394\n-2.14\n\n\n\n\n\n\nOverall Trend:\n\nWhile there was a significant increase in sales in 2018 and a slight decrease in 2019, 2020 saw a notable decrease in sales during this period compared to previous years. This suggests a significant negative impact on sales in 2020 compared to earlier years.\n\n\n\n\n\n\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?\n\n\n\n-- region\n-- platform\n-- age_band\n-- demographic\n-- customer_type\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?\nAnswer:\nSales metric performance across region.\nWITH before_and_after_data AS\n       (SELECT region,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT region,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY region\n            )\n        SELECT region,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nRegion\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nAFRICA\n4942976910\n4997516159\n54539249\n1.10\n\n\nASIA\n4613242689\n4551927271\n-61315418\n-1.33\n\n\nCANADA\n1244662705\n1234025206\n-10637499\n-0.85\n\n\nEUROPE\n328141414\n344420043\n16278629\n4.96\n\n\nOCEANIA\n6698586333\n6640244793\n-58341540\n-0.87\n\n\nSOUTH AMERICA\n611056923\n608981392\n-2075531\n-0.34\n\n\nUSA\n1967554887\n1960297502\n-7257385\n-0.37\n\n\n\nSales metric performance across platform.\nWITH before_and_after_data AS\n       (SELECT platform,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT platform,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY platform\n            )\n        SELECT platform,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nPlatform\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nRetail\n19886040272\n19768576165\n-117464107\n-0.59\n\n\nShopify\n520181589\n568836201\n48654612\n9.35\n\n\n\nSales metric performance across age_band.\nWITH before_and_after_data AS\n       (SELECT age_band,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT age_band,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY age_band\n            )\n        SELECT age_band,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nAge Band\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nMiddle Aged\n3276892347\n3269748622\n-7143725\n-0.22\n\n\nRetirees\n6646865322\n6634706880\n-12158442\n-0.18\n\n\nUnknown\n8191628826\n8146983408\n-44645418\n-0.55\n\n\nYoung Adults\n2290835366\n2285973456\n-4861910\n-0.21\n\n\n\nSales metric performance across demographic.\nWITH before_and_after_data AS\n       (SELECT demographic,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT demographic,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY demographic\n            )\n        SELECT demographic,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nDemographic\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nCouples\n5608866131\n5592341420\n-16524711\n-0.29\n\n\nFamilies\n6605726904\n6598087538\n-7639366\n-0.12\n\n\nUnknown\n8191628826\n8146983408\n-44645418\n-0.55\n\n\n\nSales metric performance across customer_type.\nWITH before_and_after_data AS\n       (SELECT customer_type,\n            week_number,\n            SUM(sales) AS total_sales\n            FROM clean_weekly_sales\n        GROUP BY 1,2\n        ORDER BY 1,2\n       ),\n       sales_calculation_table_before_and_after AS (\n       SELECT customer_type,\n            SUM(CASE WHEN week_number &lt; 24 THEN total_sales ELSE 0 END) AS sales_before_baseline_date_value,\n            SUM(CASE WHEN week_number &gt;= 24 THEN total_sales ELSE 0 END) AS sales_after_baseline_date_value\n            FROM before_and_after_data\n            GROUP BY customer_type\n            )\n        SELECT customer_type,\n            sales_before_baseline_date_value, sales_after_baseline_date_value,\n            (sales_after_baseline_date_value - sales_before_baseline_date_value) AS difference,\n            ROUND(100.0 * (sales_after_baseline_date_value - sales_before_baseline_date_value)/\n                sales_before_baseline_date_value,2) AS pct\n        FROM sales_calculation_table_before_and_after;\nOutput:\n\n\n\n\n\n\n\n\n\n\nCustomer Type\nSales Before Baseline Date\nSales After Baseline Date\nDifference\nPercent Change\n\n\n\n\nExisting\n10168877642\n10117367239\n-51510403\n-0.51\n\n\nGuest\n7630353739\n7595150744\n-35202995\n-0.46\n\n\nNew\n2606990480\n2624894383\n17903903\n0.69\n\n\n\n\n\n\nSales Metric Performance Across Regions:\n\nAfrica: Experienced a slight increase in sales (1.10%) after the baseline date.\nAsia: Witnessed a decrease in sales (-1.33%) after the baseline date, indicating a negative impact.\nCanada: Saw a minor decrease in sales (-0.85%) after the baseline date.\nEurope: Showed a significant increase in sales (4.96%) after the baseline date, indicating positive performance.\nOceania: Experienced a decrease in sales (-0.87%) after the baseline date.\nSouth America: Saw a slight decrease in sales (-0.34%) after the baseline date.\nUSA: Witnessed a decrease in sales (-0.37%) after the baseline date.\n\nSales Metric Performance Across Platforms:\n\nRetail: Experienced a decrease in sales (-0.59%) after the baseline date.\nShopify: Witnessed a significant increase in sales (9.35%) after the baseline date, indicating positive performance.\n\nSales Metric Performance Across Age Bands:\n\nMiddle Aged: Experienced a minor decrease in sales (-0.22%) after the baseline date.\nRetirees: Saw a slight decrease in sales (-0.18%) after the baseline date.\nUnknown: Witnessed a notable decrease in sales (-0.55%) after the baseline date.\nYoung Adults: Experienced a minor decrease in sales (-0.21%) after the baseline date.\n\nSales Metric Performance Across Demographics:\n\nCouples: Experienced a slight decrease in sales (-0.29%) after the baseline date.\nFamilies: Saw a minor decrease in sales (-0.12%) after the baseline date.\nUnknown: Witnessed a notable decrease in sales (-0.55%) after the baseline date.\n\nSales Metric Performance Across Customer Types:\n\nExisting Customers: Experienced a decrease in sales (-0.51%) after the baseline date.\nGuest Customers: Witnessed a decrease in sales (-0.46%) after the baseline date.\nNew Customers: Saw an increase in sales (0.69%) after the baseline date, indicating positive performance.\n\n\n\n\n\n- Overall Sales Performance: The analysis reveals mixed performance across different regions, platforms, age bands, demographics, and customer types.\n- Positive Trends: Europe and Shopify platform showed significant increases in sales, suggesting areas of potential growth.\n- Negative Trends: Asia, Oceania, and certain demographic segments experienced decreases in sales, requiring further investigation into underlying factors.\n\n- Recommendations: Danny’s team should focus on leveraging positive trends, such as expanding operations in regions with growing sales and optimizing sales strategies for specific customer segments. Additionally, they should address issues contributing to sales declines, such as exploring new marketing strategies or improving product offerings. Regular monitoring and analysis of sales metrics across different business areas will be crucial for identifying opportunities and addressing challenges in the future."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "Subscription based businesses are super popular and Danny realised that there was a large gap in the market - he wanted to create a new streaming service that only had food related content - something like Netflix but with only cooking shows!\nDanny finds a few smart friends to launch his new startup Foodie-Fi in 2020 and started selling monthly and annual subscriptions, giving their customers unlimited on-demand access to exclusive food videos from around the world!\nDanny created Foodie-Fi with a data driven mindset and wanted to ensure all future investment decisions and new features were decided using data. This case study focuses on using subscription style digital data to answer important business questions.\n\n\n\nDanny has shared the data design for Foodie-Fi and also short descriptions on each of the database tables - our case study focuses on only 2 tables but there will be a challenge to create a new table for the Foodie-Fi team.\nAll datasets exist within the foodie_fi database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\n\n\n\n\nCustomers can choose which plans to join Foodie-Fi when they first sign up.\nBasic plan customers have limited access and can only stream their videos and is only available monthly at $9.90\nPro plan customers have no watch time limits and are able to download videos for offline viewing. Pro plans start at $19.90 a month or $199 for an annual subscription.\nCustomers can sign up to an initial 7 day free trial will automatically continue with the pro monthly subscription plan unless they cancel, downgrade to basic or upgrade to an annual pro plan at any point during the trial.\nWhen customers cancel their Foodie-Fi service - they will have a churn plan record with a null price but their plan will continue until the end of the billing period.\n\n\n\nplan_id\nplan_name\nprice\n\n\n\n\n0\ntrial\n0\n\n\n1\nbasic monthly\n9.90\n\n\n2\npro monthly\n19.90\n\n\n3\npro annual\n199\n\n\n4\nchurn\nnull\n\n\n\n\n\n\nCustomer subscriptions show the exact date where their specific plan_id starts.\nIf customers downgrade from a pro plan or cancel their subscription - the higher plan will remain in place until the period is over - the start_date in the subscriptions table will reflect the date that the actual plan changes.\nWhen customers upgrade their account from a basic plan to a pro or annual pro plan - the higher plan will take effect straightaway.\nWhen customers churn - they will keep their access until the end of their current billing period but the start_date will be technically the day they decided to cancel their service.\n\n\n\ncustomer_id\nplan_id\nstart_date\n\n\n\n\n1\n0\n2020-08-01\n\n\n1\n1\n2020-08-08\n\n\n2\n0\n2020-09-20\n\n\n2\n3\n2020-09-27\n\n\n11\n0\n2020-11-19\n\n\n11\n4\n2020-11-26\n\n\n13\n0\n2020-12-15\n\n\n13\n1\n2020-12-22\n\n\n13\n2\n2021-03-29\n\n\n15\n0\n2020-03-17\n\n\n15\n2\n2020-03-24\n\n\n15\n4\n2020-04-29\n\n\n16\n0\n2020-05-31\n\n\n16\n1\n2020-06-07\n\n\n16\n3\n2020-10-21\n\n\n18\n0\n2020-07-06\n\n\n18\n2\n2020-07-13\n\n\n19\n0\n2020-06-22\n\n\n19\n2\n2020-06-29\n\n\n19\n3\n2020-08-29\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA foodie_fi;\nUSE foodie_fi;\n\nCREATE TABLE plans (\n  plan_id INTEGER,\n  plan_name VARCHAR(13),\n  price DECIMAL(5,2)\n);\n\nINSERT INTO plans\n  (plan_id, plan_name, price)\nVALUES\n  ('0', 'trial', '0'),\n  ('1', 'basic monthly', '9.90'),\n  ('2', 'pro monthly', '19.90'),\n  ('3', 'pro annual', '199'),\n  ('4', 'churn', null);\n\nCREATE TABLE subscriptions (\n  customer_id INTEGER,\n  plan_id INTEGER,\n  start_date DATE\n);\n\nINSERT INTO subscriptions\n  (customer_id, plan_id, start_date)\nVALUES\n  ('1', '0', '2020-08-01'),\n  ('1', '1', '2020-08-08'),\n  ('2', '0', '2020-09-20'),\n  ('2', '3', '2020-09-27'),\n  ('3', '0', '2020-01-13'),\n  ('3', '1', '2020-01-20'),\n  ('4', '0', '2020-01-17'),\n  ('4', '1', '2020-01-24'),\n  ('4', '4', '2020-04-21'),\n  ('5', '0', '2020-08-03'),\n  ('5', '1', '2020-08-10'),\n  ('6', '0', '2020-12-23'),\n  ('6', '1', '2020-12-30'),\n  ('6', '4', '2021-02-26'),\n  ('7', '0', '2020-02-05'),\n  ('7', '1', '2020-02-12'),\n  ('7', '2', '2020-05-22'),\n  ('8', '0', '2020-06-11'),\n  ('8', '1', '2020-06-18'),\n  ('8', '2', '2020-08-03'),\n  ('9', '0', '2020-12-07'),\n  ('9', '3', '2020-12-14'),\n  ('10', '0', '2020-09-19'),\n  ('10', '2', '2020-09-26'),\n  ('11', '0', '2020-11-19'),\n  ('11', '4', '2020-11-26'),\n  ('12', '0', '2020-09-22'),\n  ('12', '1', '2020-09-29'),\n  ('13', '0', '2020-12-15'),\n  ('13', '1', '2020-12-22')\n\n\n\nThis case study is split into an initial data understanding question before diving straight into data analysis questions before finishing with 1 single extension challenge.\n\n\nBased off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\n\n\n\n\nHow many customers has Foodie-Fi ever had?\nWhat is the monthly distribution of trial plan start_date values for our dataset - use the start of the month as the group by value.\nWhat plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name.\nWhat is the customer count and percentage of customers who have churned rounded to 1 decimal place?\nHow many customers have churned straight after their initial free trial - what percentage is this rounded to the nearest whole number?\nWhat is the number and percentage of customer plans after their initial free trial?\nWhat is the customer count and percentage breakdown of all 5 plan_name values at 2020-12-31?\nHow many customers have upgraded to an annual plan in 2020?\nHow many days on average does it take for a customer to an annual plan from the day they join Foodie-Fi?\nCan you further breakdown this average value into 30 day periods (i.e. 0-30 days, 31-60 days etc)\nHow many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\n\n\n\nThe Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n\n\n\nThe following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\nHow would you calculate the rate of growth for Foodie-Fi?\nWhat key metrics would you recommend Foodie-Fi management to track over time to assess performance of their overall business?\nWhat are some key customer journeys or experiences that you would analyse further to improve customer retention?\nIf the Foodie-Fi team were to create an exit survey shown to customers who wish to cancel their subscription, what questions would you include in the survey?\nWhat business levers could the Foodie-Fi team use to reduce the customer churn rate? How would you validate the effectiveness of your ideas?\n\n\n\n\n\n\n\nBased off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\nSQL Query:\nSELECT\n    S.customer_id, P.plan_name, P.price, S.start_date\nFROM Subscriptions AS S\nJOIN Plans AS P ON P.plan_id = S.plan_id\nWHERE customer_id IN (1,2,3,4,5,6,7,8);\nOutput:\n\n\n\ncustomer_id\nplan_name\nprice\nstart_date\n\n\n\n\n1\ntrial\n0.00\n2020-08-01\n\n\n1\nbasic monthly\n9.90\n2020-08-08\n\n\n2\ntrial\n0.00\n2020-09-20\n\n\n2\npro annual\n199.00\n2020-09-27\n\n\n3\ntrial\n0.00\n2020-01-13\n\n\n3\nbasic monthly\n9.90\n2020-01-20\n\n\n4\ntrial\n0.00\n2020-01-17\n\n\n4\nbasic monthly\n9.90\n2020-01-24\n\n\n4\nchurn\n\n2020-04-21\n\n\n5\ntrial\n0.00\n2020-08-03\n\n\n5\nbasic monthly\n9.90\n2020-08-10\n\n\n6\ntrial\n0.00\n2020-12-23\n\n\n6\nbasic monthly\n9.90\n2020-12-30\n\n\n6\nchurn\n\n2021-02-26\n\n\n7\ntrial\n0.00\n2020-02-05\n\n\n7\nbasic monthly\n9.90\n2020-02-12\n\n\n7\npro monthly\n19.90\n2020-05-22\n\n\n8\ntrial\n0.00\n2020-06-11\n\n\n8\nbasic monthly\n9.90\n2020-06-18\n\n\n8\npro monthly\n19.90\n2020-08-03\n\n\n\nCustomer 1 signed up on ‘2020-08-01’ for free-trial and on ‘2020-08-08’ customer took the basic monthly plan as the system automatically upgrades to pro monthly plan.\nCustomer 2 signed up on ‘2020-09-20’ for free trial and on ‘2020-09-27’ customer upgraded to pro annual subscription.\nCustomer 3 signed up on ‘2020-01-13’ for free trial and on ‘2020-01-20’ customer took the basic monthly plan instead of going for the pro monthly plan as what system automatically upgrades to.\nCustomer 4 signed up on ‘2020-01-17’ for free trial, on ‘2020-01-24’ customer took the basic monthly plan and then churned out on ‘2020-0421’ (after 3 months of free-trial).\nCustomer 5 signed up on ‘2020-08-03’ for free-trial and on ‘2020-08-10’ customer took the basic monthly plan instead of going for the pro monthly plan as what system automatically upgrades to.\nCustomer 6 signed up on ‘2020-12-23’ for free trial, on ‘2020-12-30’ customer took the basic monthly plan and then churned out on ‘2021-02-26’ (after 2 months of free-trial).\nCustomer 7 signed up on ‘2020-02-05’ for free-trial, on ‘2020-02-12’ customer took the basic monthly plan and then using the basic monthly plan for 3 months upgraded his plan to pro monthly on ‘2020-05-22’.\nSame goes to customer 8, customer signed up on ‘2020-06-11’ for free-trial, on ‘2020-06-18’ customer took the basic monthly plan and then using the basic monthly plan for 2 months upgraded his plan to pro monthly on ‘2020-08-03’.\n\n\n\n\n\n\nSELECT COUNT(DISTINCT customer_id) AS num_customers\nFROM SUBSCRIPTIONS;\nOutput:\n\n\n\nnum_customers\n\n\n\n\n1000\n\n\n\n\n\n\nCustomer Acquisition/Total Number of Customers:\n\nThe platform has successfully onboarded 1000 customers, indicating a healthy level of customer acquisition.\n\n\n\n\n\n\nSELECT\n    MONTH(S.start_date) AS month_number,\n    MONTHNAME(S.start_date) AS month_name,\n    COUNT(S.customer_id) AS customer_cnt\nFROM subscriptions AS S\nJOIN plans AS P ON S.plan_id = P.plan_id\nWHERE P.plan_name = 'trial'\nGROUP BY month_number, month_name\nORDER BY MONTH(S.start_date);\nOutput:\n\n\n\nmonth_number\nmonth_name\ncustomer_cnt\n\n\n\n\n1\nJanuary\n88\n\n\n2\nFebruary\n68\n\n\n3\nMarch\n94\n\n\n4\nApril\n81\n\n\n5\nMay\n88\n\n\n6\nJune\n79\n\n\n7\nJuly\n89\n\n\n8\nAugust\n88\n\n\n9\nSeptember\n87\n\n\n10\nOctober\n79\n\n\n11\nNovember\n75\n\n\n12\nDecember\n84\n\n\n\n\n\n\nMonthly Breakdown:\n\nThe number of customers starting trial plans shows variations across different months.\nMonths like March, July, and August witnessed relatively higher numbers of trial plan initiations.\nThe distribution highlights potential seasonal patterns or promotional activities that may have influenced trial plan sign-ups.\nUnderstanding the monthly distribution helps in identifying trends and seasonality in customer acquisition.\n\n\n\n\n\n\nSELECT\n    P.plan_name,\n    COUNT(S.customer_id) AS customer_cnt\nFROM subscriptions AS S\nJOIN plans AS P ON S.plan_id = P.plan_id\nWHERE YEAR(S.start_date) &gt; 2020\nGROUP BY P.plan_name\nORDER BY P.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_cnt\n\n\n\n\nbasic monthly\n8\n\n\nchurn\n71\n\n\npro annual\n63\n\n\npro monthly\n60\n\n\n\n\n\n\nBreakdown by Plan Name:\n\nBasic Monthly: 8 events started after 2020 for the basic monthly plan.\nChurn: There are 71 churn events recorded after 2020.\nPro Annual: 63 customers started their pro annual plans after 2020.\nPro Monthly: 60 customers initiated their pro monthly plans after 2020.\n\nObservations:\n\nThe majority of events after 2020 are churn events, indicating customers canceling their subscriptions.\nPro annual and pro monthly plans also have significant post-2020 start dates, suggesting continued subscription renewals and new sign-ups.\nMonitoring churn rates and understanding the reasons behind churn events is crucial for retaining customers.\n\n\n\n\n\n\nSELECT\n    COUNT(DISTINCT S.customer_id) AS churn_cnt,\n    ROUND(100.0 * COUNT(DISTINCT S.customer_id)\n        /(SELECT COUNT(DISTINCT subscriptions.customer_id)\n            FROM foodie_fi.subscriptions),1) AS churn_percentage\nFROM subscriptions AS S\nJOIN plans AS P\nON S.plan_id = P.plan_id\nWHERE P.plan_name = 'churn';\nOutput:\n\n\n\nchurn_cnt\nchurn_percentage\n\n\n\n\n307\n30.7\n\n\n\n\n\n\nChurn Count and Percentage:\n\nChurn Count: The total number of customers who have churned is 307.\nChurn Percentage: The churn rate, rounded to one decimal place, is 30.7%.\n\nObservations:\n\nChurn is a significant factor impacting Foodie-Fi’s customer base, with almost one-third of customers canceling their subscriptions.\nUnderstanding the reasons behind churn and implementing strategies to reduce it is essential for maintaining a stable and growing subscriber base.\n\nImplications:\n\nMonitoring and analyzing churn metrics regularly is crucial for identifying trends and implementing proactive measures to mitigate churn.\nImplementing retention strategies such as personalized offers, improved customer support, and content recommendations can help reduce churn and improve customer satisfaction.\nContinuous evaluation of churn metrics and adjustment of retention strategies based on insights gained will be essential for long-term business success.\n\n\n\n\n\n\nWITH next_plan_cte AS\n    (SELECT *, LEAD(plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\n        FROM subscriptions),\n    churners AS\n        (SELECT * FROM next_plan_cte\n            WHERE next_plan=4 AND plan_id = 0)\n    SELECT COUNT(customer_id) AS 'churners_after_trail',\n        ROUND(100.0 * COUNT(customer_id)/(SELECT COUNT(DISTINCT customer_id) AS 'distinct_customers'\n                FROM subscriptions),2) AS 'churn_percentage_after_trial'\nFROM churners;\nOutput:\n\n\n\nchurners_cnt_after_trial\nchurners_percentage_after_trial\n\n\n\n\n92\n9.20\n\n\n\n\n\n\nChurners after Free Trial:\n\nChurners Count after Trial: There are 92 customers who have churned immediately after their initial free trial period.\nChurn Percentage after Trial: The percentage of customers who churned after their free trial, rounded to the nearest whole number, is 9%.\n\nObservations:\n\nThe churn rate after the free trial period is a critical metric for understanding the effectiveness of the trial in converting users to paid subscribers.\nA churn rate of 9% suggests that a significant portion of customers are not converting to paid plans after their trial period.\n\nImplications:\n\nAnalyzing the reasons why customers churn after the trial period and addressing any pain points or barriers to subscription conversion is essential. -Implementing strategies to improve the trial experience, provide value during the trial period, and incentivize conversion to paid plans can help reduce churn after the trial.\nContinuous monitoring of churn metrics post-trial and iterating on trial offerings based on customer feedback and behavior will be crucial for improving conversion rates and overall subscriber retention.\n\n\n\n\n\n\nWITH next_plan_cte AS\n    (SELECT subscriptions.customer_id,\n        subscriptions.plan_id, plans.plan_name, start_date,\n        LEAD(subscriptions.plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\n        FROM subscriptions\n        JOIN plans ON subscriptions.plan_id = plans.plan_id)\nSELECT next_plan_cte.plan_name,\n    COUNT(next_plan_cte.customer_id) AS customer_cnt,  -- Specify the table for clarity\n    ROUND(100.0 * COUNT(next_plan_cte.customer_id) / (SELECT COUNT(DISTINCT customer_id) AS distinct_customers FROM subscriptions), 2) AS \"percentage\"  -- Use double quotes for aliases\nFROM next_plan_cte\nWHERE next_plan_cte.plan_name != 'trial'\nGROUP BY next_plan_cte.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_cnt\npercentage\n\n\n\n\nbasic monthly\n546\n54.60\n\n\npro annual\n258\n25.80\n\n\nchurn\n307\n30.70\n\n\npro monthly\n539\n53.90\n\n\n\n\n\n\nCustomer Plan Distribution:\n\nBasic Monthly: 546 customers (54.60%) opted for the Basic Monthly plan after their free trial.\nPro Annual: 258 customers (25.80%) chose the Pro Annual plan after their free trial.\nChurn: 307 customers (30.70%) churned after their free trial, indicating that they did not continue with any paid subscription.\nPro Monthly: 539 customers (53.90%) selected the Pro Monthly plan after their free trial.\n\nObservations:\n\nThe majority of customers opt for either the Basic Monthly or Pro Monthly plans after the free trial, accounting for approximately 54.60% and 53.90%, respectively.\nA significant portion of customers (25.80%) choose the Pro Annual plan, indicating a preference for longer-term commitments.\nThe churn rate after the free trial is notably high, with 30.70% of customers opting out of any paid subscription.\n\nImplications:\n\nOffering a variety of subscription plans caters to different customer preferences and financial capabilities.\nUnderstanding the reasons behind churn after the trial period is crucial for improving retention and conversion rates.\nImplementing targeted marketing strategies, personalized offers, and enhancing the value proposition for paid plans can help reduce churn and increase subscription conversions post-trial.\n\n\n\n\n\n\nWITH activeSubscriptions AS (\nSELECT S.customer_id, S.start_date, P.plan_name,\n    LEAD(S.start_date) OVER (PARTITION BY S.customer_id ORDER BY S.start_date) AS next_date\nFROM Plans AS P JOIN Subscriptions AS S\nON P.plan_id = S.plan_id)\nSELECT A.plan_name, COUNT(A.customer_id) AS customer_count,\n    ROUND(100.0 * (COUNT(A.customer_id))/(SELECT COUNT(DISTINCT customer_id) FROM Subscriptions),2) AS customer_percentage\nFROM activeSubscriptions AS A\nWHERE (next_date IS NOT NULL AND (A.start_date &lt; '2020-12-31' AND A.next_date &gt; '2020-12-31')\n    OR (A.start_date &lt; '2020-12-31' AND A.next_date IS NULL))\nGROUP BY A.plan_name\nORDER BY A.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_count\ncustomer_percentage\n\n\n\n\nbasic monthly\n224\n22.40\n\n\nchurn\n235\n23.50\n\n\npro annual\n195\n19.50\n\n\npro monthly\n326\n32.60\n\n\ntrial\n19\n1.90\n\n\n\n\n\n\nCustomer Distribution:\n\nBasic Monthly: There are 224 customers (22.40%) subscribed to the Basic Monthly plan.\nChurn: 235 customers (23.50%) have churned, meaning they no longer have an active subscription.\nPro Annual: 195 customers (19.50%) are subscribed to the Pro Annual plan.\nPro Monthly: 326 customers (32.60%) have opted for the Pro Monthly plan.\nTrial: A small portion of customers, 19 (1.90%), are still in the trial phase as of December 31, 2020.\n\nObservations:\n\nThe Pro Monthly plan has the highest customer count at 32.60%, indicating its popularity among subscribers.\nBasic Monthly and Churn plans have relatively similar customer counts, with 22.40% and 23.50%, respectively.\nThe Pro Annual plan has a lower customer count compared to Pro Monthly but still maintains a significant portion at 19.50%.\nThe Trial phase has a minimal impact on the customer base, with only 1.90% of customers still in the trial period.\n\nImplications:\n\nUnderstanding the distribution of customers across different plans helps in evaluating the effectiveness of pricing strategies and plan offerings.\nAnalyzing churn rates alongside active subscriptions provides insights into customer retention and satisfaction levels.\nTargeted marketing and retention efforts can be tailored based on plan preferences and customer behavior to maximize subscription revenue and minimize churn.\n\n\n\n\n\n\nSELECT\n    COUNT(S.customer_id) AS num_of_customers\nFROM subscriptions AS S\nWHERE plan_id = 3 AND YEAR(start_date) = '2020';\nOutput:\n\n\n\nnum_of_customers\n\n\n\n\n195\n\n\n\n\n\n\nNumber of Upgrades:\n\nAnnual Plan Upgrades: In 2020, a total of 195 customers upgraded to the annual subscription plan.\n\nObservations:\n\nThe annual subscription plan attracted a considerable number of customers, indicating its appeal and value proposition.\n\nImplications:\n\nThe popularity of the annual plan upgrade suggests that customers are interested in committing to Foodie-Fi for a longer duration, possibly due to cost savings or enhanced benefits offered by the annual subscription.\n\n\n\n\n\n\nWITH trial_cte AS\n(SELECT customer_id, start_date AS trial_date FROM subscriptions WHERE plan_id = 0),\n    annual_cte AS\n (SELECT customer_id, start_date AS annual_date FROM subscriptions WHERE plan_id = 3)\nSELECT ROUND(AVG(DATEDIFF(annual_date,trial_date)),0) AS 'avg_num_of_days'\nFROM trial_cte JOIN annual_cte ON trial_cte.customer_id = annual_cte.customer_id;\nOutput:\n\n\n\navg_num_of_days\n\n\n\n\n105\n\n\n\n\n\n\nAverage Time to Upgrade:\n\nAverage Duration: On average, it takes approximately 105 days for customers to upgrade from the trial plan to an annual subscription plan.\n\nObservations:\n\nCustomers typically take over three months to transition from the trial plan to the annual subscription, indicating a deliberative decision-making process or possibly a trial period evaluation.\n\nImplications:\n\nFoodie-Fi can implement targeted campaigns or incentives to prompt trial users to upgrade sooner, thereby increasing conversion rates and revenue. Additionally, personalized communication or offers tailored to users’ preferences can expedite the upgrade process.\n\n\n\n\n\n\nWITH trial_plan AS (\n  SELECT customer_id, start_date AS trial_date\n  FROM foodie_fi.subscriptions WHERE plan_id = 0\n), annual_plan AS (\n  SELECT\n    customer_id, start_date AS annual_date\n  FROM foodie_fi.subscriptions WHERE plan_id = 3\n), bins AS (\n  -- bins CTE: Put customers in 30-day buckets based on the average number of days taken to upgrade to a pro annual plan.\n  SELECT\n    FLOOR((DATEDIFF(annual.annual_date, trial.trial_date) - 1) / 30) + 1 AS avg_days_to_upgrade\n  FROM trial_plan AS trial\n  JOIN annual_plan AS annual\n    ON trial.customer_id = annual.customer_id\n)\nSELECT\n  CONCAT(((avg_days_to_upgrade - 1) * 30 + 1), ' - ', (avg_days_to_upgrade * 30), ' days') AS day_period_bucket,\n  COUNT(*) AS num_of_customers\nFROM bins\nGROUP BY avg_days_to_upgrade\nORDER BY avg_days_to_upgrade;\nOutput:\n\n\n\nday_period_bucket\nnum_of_customers\n\n\n\n\n1 - 30 days\n49\n\n\n31 - 60 days\n24\n\n\n61 - 90 days\n34\n\n\n91 - 120 days\n35\n\n\n121 - 150 days\n42\n\n\n151 - 180 days\n36\n\n\n181 - 210 days\n26\n\n\n211 - 240 days\n4\n\n\n241 - 270 days\n5\n\n\n271 - 300 days\n1\n\n\n301 - 330 days\n1\n\n\n331 - 360 days\n1\n\n\n\n\n\n\nKey Findings:\n\nInitial Upgrade Activity: The majority of customers (49) upgrade within the first month (1 - 30 days) after their trial period ends, indicating prompt conversion for a significant portion of users.\nGradual Adoption: A notable number of customers continue to upgrade gradually over time, with 24 customers upgrading between 31 to 60 days, and 34 between 61 to 90 days.\nSteady Conversion: Conversion remains consistent over time, with a relatively even distribution of customers upgrading across subsequent 30-day intervals, until a decline observed after 180 days.\n\nObservations:\n\nQuick Conversions: The early spike in upgrades suggests that a substantial portion of customers are convinced of the value proposition shortly after their trial ends.\nLonger Adoption Period: Some customers take longer to convert, possibly indicating a longer evaluation or decision-making process, necessitating ongoing engagement strategies during this period.\nLate Adopters: A few customers upgrade much later, highlighting the importance of persistent engagement efforts to encourage conversion even beyond the initial months.\n\n\n\n\n\n\nWITH downgraded_cte AS\n(SELECT customer_id, plan_id, start_date,\n    LEAD(plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\nFROM subscriptions)\nSELECT COUNT(customer_id)\nFROM downgraded_cte\nWHERE plan_id = '2' and next_plan = '1' AND YEAR(start_date);\nOutput:\n\n\n\ncustomer_cnt\n\n\n\n\n0\n\n\n\n\n\n\nKey Finding:\n\nNo Downgrades Detected: There were no instances of customers downgrading from the pro monthly plan to the basic monthly plan in 2020, indicating a retention of pro monthly subscribers or a lack of documented downgrades during this period.\n\n\n\n\n\n\n\nThe Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n-- Use a recursive CTE to increment rows for all monthly paid plans until customers changing the plan, except 'pro annual'\nWITH RECURSIVE dateRecursion AS (\n    SELECT\n        s.customer_id, s.plan_id, p.plan_name, s.start_date AS payment_date,\n        -- column last_date: last day of the current plan\n        CASE\n            -- if a customer kept using the current plan, last_date = '2020-12-31'\n            WHEN LEAD(s.start_date) OVER(PARTITION BY s.customer_id ORDER BY s.start_date) IS NULL THEN '2020-12-31'\n            -- if a customer changed the plan, last_date = (month difference between start_date and changing date) + start_date\n            ELSE DATE_ADD(\n                start_date,\n                INTERVAL DATEDIFF(LEAD(s.start_date) OVER(PARTITION BY s.customer_id ORDER BY s.start_date), start_date) MONTH\n            ) END AS last_date,\n        p.price AS amount\n    FROM subscriptions s\n    JOIN plans p ON s.plan_id = p.plan_id\n    -- exclude trials because they didn't generate payments\n    WHERE p.plan_name NOT IN ('trial')\n        AND YEAR(start_date) = 2020\n\n    UNION ALL\n\n    SELECT\n        customer_id,\n        plan_id,\n        plan_name,\n        -- increment payment_date by monthly\n        DATE_ADD(payment_date, INTERVAL 1 MONTH) AS payment_date,\n        last_date,\n        amount\n    FROM dateRecursion\n    -- stop incrementing when payment_date = last_date\n    WHERE DATE_ADD(payment_date, INTERVAL 1 MONTH) &lt;= last_date\n        AND plan_name != 'pro annual'\n)\n-- Create a new table [payments]\nSELECT\n    customer_id,\n    plan_id,\n    plan_name,\n    payment_date,\n    amount,\n    ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY payment_date) AS payment_order\nFROM dateRecursion\n-- exclude churns\nWHERE amount IS NOT NULL\nORDER BY customer_id;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n\n\n\nThe following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\n\nAnswer:\nTo assess the rate of growth for Foodie-Fi, we should analyze the following multiple key indicators:\n\nCustomer Base Growth:\n\nCalculate the overall growth in the customer base over specific time periods (for instance, monthly or quarterly.),.\nMonitor the acquisition of new customers and the retention of the existing ones.\n\nSubscription Plan Adoption:\n\nAnalyze the uptake of different subscription plans (Basic, Pro Monthly, Pro Annual) to understand the customer preferences.\nEvaluate the growth rate of Pro Plans compared to the overall customer base.\n\nChurn Reduction:\n\nInvestigate the reduction in churn by assessing the number of customers downgrading or canceling their subscriptions.\nCalculate the churn rate and observe its trend over time.\n\nRevenue Growth:\n\nMonitor the revenue growth over time, by considering both the increase in the customer base and potential changes in subscription plans.\n\nConversion Rates:\n\nAnalyze the conversion rates from Trial users to Paid subscribers, providing insights into the effectiveness of the trial period in conversting users.\n\n\n\n\n\nAnswer:\nTo comprehensively assess the performance of Foodie-Fi, it is recommended to track the following metrics over time.\n\nCustomer Base Growth:\n\nMonitor the overall growth or contraction of the customer base on a regular basis (for instance, monthly, or quarterly or annually).\n\nRevenue Breakdown:\n\nAnalyze the revenue streams from different subscription plans, with a focus on Pro plan customers. This breakdown helps understand the contribution of various plans to the overall revenue.\n\nChurn Rate:\n\nCalculate and track the churn rate over time. Identifying patterns in customer attrition provides insights into service quality, and customer statisfaction and hence, increase in the customer base.\n\nCustomer Acquisition and Conversion Rates:\n\nAssess the effectiveness of marketing strategies by tracking customer acquisition rates. Additionally, monitor the conversion rates from tria to paid subscriptions.\n\nAverage Revenue Per User (ARPU):\n\nCalculate the average revenue generated by each user to understand the averae contribution of each customers to the overall revenue.\n\n\n\n\n\nAnswer:\nThe key areas to understand the customer retention are the following ones:\n\nPlan Changes:\n\nUnderstand the specific triggers that lead the customers to changes their plans. This could involve analyzing the benefits they seek, such as unlimited streaming or offline access. Additionally, explore whether promotional offers or plan-specific features influences plan changes.\nImplement an exit survey strategy for users who either downgrade or cancel their plans. Collecting feedback directly from users can uncover nuanced insights into their decision making process.\n\nCustomer Support:\n\nWe can identify the common queries, concerns, or issues faced by customers from the customer support interactions. Additionally track how effectively customer issues are resolved over time. This helps us understand if the customers are unhappy with the resolutions and our services, thereby leading them to churn out.\n\nContent Preferences:\n\nExplore not only what customers are looking for but also gather insights into why certain content might not meet their expectations. Are there any programs that are not in the video library because of which customers are looking for? Conducting surveys of feedback sessions helps to understand specific content preferences and expectations.\n\nPlatform Comparison:\n\nConsider comparing Foodie-Fi with other platforms. Identify features, shows that competitors offer and where Foodie-Fi might lack. This competitive analysis can reveal opportunities for improvement. Additionally assess user perceptions of Foodie-Fi to competitors through surveys or reviews.\n\nPersonalized Recommendations:\n\nUnderstand how personalized recommendation can positively impact retention. Analyze successful cases where tailored content suggestions lead to increased engagement. Implement ML algorithms to enhance recommendation precision, ensuring users receive content aligned with their preferences.\nImplement a feedback mechanism for users to provide explicit feedback on recommended content. This can refine algorithms and enhance the accuracy of personalized suggestions.\n\n\n\n\n\nAnswer:\nSome valuable questions to include in the exit survey shown to customers who wish to cancel their subscription could be the following:\n\nWhat did you like most about Foodie-Fi, and is there anything specific you would like to see changed or improved on the program?\nAre you satisfied with the current pricing of our plans?\nHow would you rate your satisfaction with our customer support? If not entirely satisfied, please provide details on areas for improvement.\nDid you find the specific program you were looking for for in our Video Library? If not, could you share some details about the content you were seeking?\nWhat is the primary reason for canceling your subscription? (Include options related to content, pricing, customer service, etc.)\nIs there anything else you would like to share or any feedback that hasn’t been covered in the previous questions?\n\n\n\n\nAnswer:\n\nEnhanced Content Library:\n\nIdea: Regularly update and expand the content library with exclusive shows and programs.\nValidation: Monitor the time spent by the customers on the platform, and the frequency of login hours, and take direct feedback in the form of a question like, Is this the show you were looking for?\n\nPersonalized Recommendations:\n\nIdea: Implement advanced algorithms to provide personalized content recommendations based on Individual preferences and viewing history.\nValidation: Track the percentage of users following the personalized recommendations and ask for feedback to check the satisfaction of the customers.\n\nPromotional Offers and Discounts:\n\nIdea: Introduce limited-time promotions, discounts, or bundled subscription options to incentivize customer retention.\nValidation: Analyze the impact on customer retention by doing the before and after analysis.\n\nImproved Customer Support:\n\nIdea: Enhance customer support services by reducing response times, resolving queries effectively in a short period, and gathering feedback for continuous improvement.\nValidation: Guage the customer churn percentage, and customer satisfaction scores, and collect feedback on customer support interactions.\n\nExit Surveys and Feedback Analysis:\n\nIdea: Implement an exit survey for customers canceling their subscriptions to gather insights into reasons for leaving.\nValidation: Analyze any common reason to leave the platform and work on it."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#introduction",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "Subscription based businesses are super popular and Danny realised that there was a large gap in the market - he wanted to create a new streaming service that only had food related content - something like Netflix but with only cooking shows!\nDanny finds a few smart friends to launch his new startup Foodie-Fi in 2020 and started selling monthly and annual subscriptions, giving their customers unlimited on-demand access to exclusive food videos from around the world!\nDanny created Foodie-Fi with a data driven mindset and wanted to ensure all future investment decisions and new features were decided using data. This case study focuses on using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#available-data",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#available-data",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "Danny has shared the data design for Foodie-Fi and also short descriptions on each of the database tables - our case study focuses on only 2 tables but there will be a challenge to create a new table for the Foodie-Fi team.\nAll datasets exist within the foodie_fi database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#entity-relationship-diagram",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#entity-relationship-diagram",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "Customers can choose which plans to join Foodie-Fi when they first sign up.\nBasic plan customers have limited access and can only stream their videos and is only available monthly at $9.90\nPro plan customers have no watch time limits and are able to download videos for offline viewing. Pro plans start at $19.90 a month or $199 for an annual subscription.\nCustomers can sign up to an initial 7 day free trial will automatically continue with the pro monthly subscription plan unless they cancel, downgrade to basic or upgrade to an annual pro plan at any point during the trial.\nWhen customers cancel their Foodie-Fi service - they will have a churn plan record with a null price but their plan will continue until the end of the billing period.\n\n\n\nplan_id\nplan_name\nprice\n\n\n\n\n0\ntrial\n0\n\n\n1\nbasic monthly\n9.90\n\n\n2\npro monthly\n19.90\n\n\n3\npro annual\n199\n\n\n4\nchurn\nnull\n\n\n\n\n\n\nCustomer subscriptions show the exact date where their specific plan_id starts.\nIf customers downgrade from a pro plan or cancel their subscription - the higher plan will remain in place until the period is over - the start_date in the subscriptions table will reflect the date that the actual plan changes.\nWhen customers upgrade their account from a basic plan to a pro or annual pro plan - the higher plan will take effect straightaway.\nWhen customers churn - they will keep their access until the end of their current billing period but the start_date will be technically the day they decided to cancel their service.\n\n\n\ncustomer_id\nplan_id\nstart_date\n\n\n\n\n1\n0\n2020-08-01\n\n\n1\n1\n2020-08-08\n\n\n2\n0\n2020-09-20\n\n\n2\n3\n2020-09-27\n\n\n11\n0\n2020-11-19\n\n\n11\n4\n2020-11-26\n\n\n13\n0\n2020-12-15\n\n\n13\n1\n2020-12-22\n\n\n13\n2\n2021-03-29\n\n\n15\n0\n2020-03-17\n\n\n15\n2\n2020-03-24\n\n\n15\n4\n2020-04-29\n\n\n16\n0\n2020-05-31\n\n\n16\n1\n2020-06-07\n\n\n16\n3\n2020-10-21\n\n\n18\n0\n2020-07-06\n\n\n18\n2\n2020-07-13\n\n\n19\n0\n2020-06-22\n\n\n19\n2\n2020-06-29\n\n\n19\n3\n2020-08-29"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#interactive-sql-session",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#interactive-sql-session",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA foodie_fi;\nUSE foodie_fi;\n\nCREATE TABLE plans (\n  plan_id INTEGER,\n  plan_name VARCHAR(13),\n  price DECIMAL(5,2)\n);\n\nINSERT INTO plans\n  (plan_id, plan_name, price)\nVALUES\n  ('0', 'trial', '0'),\n  ('1', 'basic monthly', '9.90'),\n  ('2', 'pro monthly', '19.90'),\n  ('3', 'pro annual', '199'),\n  ('4', 'churn', null);\n\nCREATE TABLE subscriptions (\n  customer_id INTEGER,\n  plan_id INTEGER,\n  start_date DATE\n);\n\nINSERT INTO subscriptions\n  (customer_id, plan_id, start_date)\nVALUES\n  ('1', '0', '2020-08-01'),\n  ('1', '1', '2020-08-08'),\n  ('2', '0', '2020-09-20'),\n  ('2', '3', '2020-09-27'),\n  ('3', '0', '2020-01-13'),\n  ('3', '1', '2020-01-20'),\n  ('4', '0', '2020-01-17'),\n  ('4', '1', '2020-01-24'),\n  ('4', '4', '2020-04-21'),\n  ('5', '0', '2020-08-03'),\n  ('5', '1', '2020-08-10'),\n  ('6', '0', '2020-12-23'),\n  ('6', '1', '2020-12-30'),\n  ('6', '4', '2021-02-26'),\n  ('7', '0', '2020-02-05'),\n  ('7', '1', '2020-02-12'),\n  ('7', '2', '2020-05-22'),\n  ('8', '0', '2020-06-11'),\n  ('8', '1', '2020-06-18'),\n  ('8', '2', '2020-08-03'),\n  ('9', '0', '2020-12-07'),\n  ('9', '3', '2020-12-14'),\n  ('10', '0', '2020-09-19'),\n  ('10', '2', '2020-09-26'),\n  ('11', '0', '2020-11-19'),\n  ('11', '4', '2020-11-26'),\n  ('12', '0', '2020-09-22'),\n  ('12', '1', '2020-09-29'),\n  ('13', '0', '2020-12-15'),\n  ('13', '1', '2020-12-22')"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#case-study-questions",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "This case study is split into an initial data understanding question before diving straight into data analysis questions before finishing with 1 single extension challenge.\n\n\nBased off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\n\n\n\n\nHow many customers has Foodie-Fi ever had?\nWhat is the monthly distribution of trial plan start_date values for our dataset - use the start of the month as the group by value.\nWhat plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name.\nWhat is the customer count and percentage of customers who have churned rounded to 1 decimal place?\nHow many customers have churned straight after their initial free trial - what percentage is this rounded to the nearest whole number?\nWhat is the number and percentage of customer plans after their initial free trial?\nWhat is the customer count and percentage breakdown of all 5 plan_name values at 2020-12-31?\nHow many customers have upgraded to an annual plan in 2020?\nHow many days on average does it take for a customer to an annual plan from the day they join Foodie-Fi?\nCan you further breakdown this average value into 30 day periods (i.e. 0-30 days, 31-60 days etc)\nHow many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\n\n\n\nThe Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n\n\n\nThe following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\nHow would you calculate the rate of growth for Foodie-Fi?\nWhat key metrics would you recommend Foodie-Fi management to track over time to assess performance of their overall business?\nWhat are some key customer journeys or experiences that you would analyse further to improve customer retention?\nIf the Foodie-Fi team were to create an exit survey shown to customers who wish to cancel their subscription, what questions would you include in the survey?\nWhat business levers could the Foodie-Fi team use to reduce the customer churn rate? How would you validate the effectiveness of your ideas?"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#lets-start-solving-them.",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#lets-start-solving-them.",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "Based off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\nSQL Query:\nSELECT\n    S.customer_id, P.plan_name, P.price, S.start_date\nFROM Subscriptions AS S\nJOIN Plans AS P ON P.plan_id = S.plan_id\nWHERE customer_id IN (1,2,3,4,5,6,7,8);\nOutput:\n\n\n\ncustomer_id\nplan_name\nprice\nstart_date\n\n\n\n\n1\ntrial\n0.00\n2020-08-01\n\n\n1\nbasic monthly\n9.90\n2020-08-08\n\n\n2\ntrial\n0.00\n2020-09-20\n\n\n2\npro annual\n199.00\n2020-09-27\n\n\n3\ntrial\n0.00\n2020-01-13\n\n\n3\nbasic monthly\n9.90\n2020-01-20\n\n\n4\ntrial\n0.00\n2020-01-17\n\n\n4\nbasic monthly\n9.90\n2020-01-24\n\n\n4\nchurn\n\n2020-04-21\n\n\n5\ntrial\n0.00\n2020-08-03\n\n\n5\nbasic monthly\n9.90\n2020-08-10\n\n\n6\ntrial\n0.00\n2020-12-23\n\n\n6\nbasic monthly\n9.90\n2020-12-30\n\n\n6\nchurn\n\n2021-02-26\n\n\n7\ntrial\n0.00\n2020-02-05\n\n\n7\nbasic monthly\n9.90\n2020-02-12\n\n\n7\npro monthly\n19.90\n2020-05-22\n\n\n8\ntrial\n0.00\n2020-06-11\n\n\n8\nbasic monthly\n9.90\n2020-06-18\n\n\n8\npro monthly\n19.90\n2020-08-03\n\n\n\nCustomer 1 signed up on ‘2020-08-01’ for free-trial and on ‘2020-08-08’ customer took the basic monthly plan as the system automatically upgrades to pro monthly plan.\nCustomer 2 signed up on ‘2020-09-20’ for free trial and on ‘2020-09-27’ customer upgraded to pro annual subscription.\nCustomer 3 signed up on ‘2020-01-13’ for free trial and on ‘2020-01-20’ customer took the basic monthly plan instead of going for the pro monthly plan as what system automatically upgrades to.\nCustomer 4 signed up on ‘2020-01-17’ for free trial, on ‘2020-01-24’ customer took the basic monthly plan and then churned out on ‘2020-0421’ (after 3 months of free-trial).\nCustomer 5 signed up on ‘2020-08-03’ for free-trial and on ‘2020-08-10’ customer took the basic monthly plan instead of going for the pro monthly plan as what system automatically upgrades to.\nCustomer 6 signed up on ‘2020-12-23’ for free trial, on ‘2020-12-30’ customer took the basic monthly plan and then churned out on ‘2021-02-26’ (after 2 months of free-trial).\nCustomer 7 signed up on ‘2020-02-05’ for free-trial, on ‘2020-02-12’ customer took the basic monthly plan and then using the basic monthly plan for 3 months upgraded his plan to pro monthly on ‘2020-05-22’.\nSame goes to customer 8, customer signed up on ‘2020-06-11’ for free-trial, on ‘2020-06-18’ customer took the basic monthly plan and then using the basic monthly plan for 2 months upgraded his plan to pro monthly on ‘2020-08-03’."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#b.-data-analysis-questions-1",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#b.-data-analysis-questions-1",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "SELECT COUNT(DISTINCT customer_id) AS num_customers\nFROM SUBSCRIPTIONS;\nOutput:\n\n\n\nnum_customers\n\n\n\n\n1000\n\n\n\n\n\n\nCustomer Acquisition/Total Number of Customers:\n\nThe platform has successfully onboarded 1000 customers, indicating a healthy level of customer acquisition.\n\n\n\n\n\n\nSELECT\n    MONTH(S.start_date) AS month_number,\n    MONTHNAME(S.start_date) AS month_name,\n    COUNT(S.customer_id) AS customer_cnt\nFROM subscriptions AS S\nJOIN plans AS P ON S.plan_id = P.plan_id\nWHERE P.plan_name = 'trial'\nGROUP BY month_number, month_name\nORDER BY MONTH(S.start_date);\nOutput:\n\n\n\nmonth_number\nmonth_name\ncustomer_cnt\n\n\n\n\n1\nJanuary\n88\n\n\n2\nFebruary\n68\n\n\n3\nMarch\n94\n\n\n4\nApril\n81\n\n\n5\nMay\n88\n\n\n6\nJune\n79\n\n\n7\nJuly\n89\n\n\n8\nAugust\n88\n\n\n9\nSeptember\n87\n\n\n10\nOctober\n79\n\n\n11\nNovember\n75\n\n\n12\nDecember\n84\n\n\n\n\n\n\nMonthly Breakdown:\n\nThe number of customers starting trial plans shows variations across different months.\nMonths like March, July, and August witnessed relatively higher numbers of trial plan initiations.\nThe distribution highlights potential seasonal patterns or promotional activities that may have influenced trial plan sign-ups.\nUnderstanding the monthly distribution helps in identifying trends and seasonality in customer acquisition.\n\n\n\n\n\n\nSELECT\n    P.plan_name,\n    COUNT(S.customer_id) AS customer_cnt\nFROM subscriptions AS S\nJOIN plans AS P ON S.plan_id = P.plan_id\nWHERE YEAR(S.start_date) &gt; 2020\nGROUP BY P.plan_name\nORDER BY P.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_cnt\n\n\n\n\nbasic monthly\n8\n\n\nchurn\n71\n\n\npro annual\n63\n\n\npro monthly\n60\n\n\n\n\n\n\nBreakdown by Plan Name:\n\nBasic Monthly: 8 events started after 2020 for the basic monthly plan.\nChurn: There are 71 churn events recorded after 2020.\nPro Annual: 63 customers started their pro annual plans after 2020.\nPro Monthly: 60 customers initiated their pro monthly plans after 2020.\n\nObservations:\n\nThe majority of events after 2020 are churn events, indicating customers canceling their subscriptions.\nPro annual and pro monthly plans also have significant post-2020 start dates, suggesting continued subscription renewals and new sign-ups.\nMonitoring churn rates and understanding the reasons behind churn events is crucial for retaining customers.\n\n\n\n\n\n\nSELECT\n    COUNT(DISTINCT S.customer_id) AS churn_cnt,\n    ROUND(100.0 * COUNT(DISTINCT S.customer_id)\n        /(SELECT COUNT(DISTINCT subscriptions.customer_id)\n            FROM foodie_fi.subscriptions),1) AS churn_percentage\nFROM subscriptions AS S\nJOIN plans AS P\nON S.plan_id = P.plan_id\nWHERE P.plan_name = 'churn';\nOutput:\n\n\n\nchurn_cnt\nchurn_percentage\n\n\n\n\n307\n30.7\n\n\n\n\n\n\nChurn Count and Percentage:\n\nChurn Count: The total number of customers who have churned is 307.\nChurn Percentage: The churn rate, rounded to one decimal place, is 30.7%.\n\nObservations:\n\nChurn is a significant factor impacting Foodie-Fi’s customer base, with almost one-third of customers canceling their subscriptions.\nUnderstanding the reasons behind churn and implementing strategies to reduce it is essential for maintaining a stable and growing subscriber base.\n\nImplications:\n\nMonitoring and analyzing churn metrics regularly is crucial for identifying trends and implementing proactive measures to mitigate churn.\nImplementing retention strategies such as personalized offers, improved customer support, and content recommendations can help reduce churn and improve customer satisfaction.\nContinuous evaluation of churn metrics and adjustment of retention strategies based on insights gained will be essential for long-term business success.\n\n\n\n\n\n\nWITH next_plan_cte AS\n    (SELECT *, LEAD(plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\n        FROM subscriptions),\n    churners AS\n        (SELECT * FROM next_plan_cte\n            WHERE next_plan=4 AND plan_id = 0)\n    SELECT COUNT(customer_id) AS 'churners_after_trail',\n        ROUND(100.0 * COUNT(customer_id)/(SELECT COUNT(DISTINCT customer_id) AS 'distinct_customers'\n                FROM subscriptions),2) AS 'churn_percentage_after_trial'\nFROM churners;\nOutput:\n\n\n\nchurners_cnt_after_trial\nchurners_percentage_after_trial\n\n\n\n\n92\n9.20\n\n\n\n\n\n\nChurners after Free Trial:\n\nChurners Count after Trial: There are 92 customers who have churned immediately after their initial free trial period.\nChurn Percentage after Trial: The percentage of customers who churned after their free trial, rounded to the nearest whole number, is 9%.\n\nObservations:\n\nThe churn rate after the free trial period is a critical metric for understanding the effectiveness of the trial in converting users to paid subscribers.\nA churn rate of 9% suggests that a significant portion of customers are not converting to paid plans after their trial period.\n\nImplications:\n\nAnalyzing the reasons why customers churn after the trial period and addressing any pain points or barriers to subscription conversion is essential. -Implementing strategies to improve the trial experience, provide value during the trial period, and incentivize conversion to paid plans can help reduce churn after the trial.\nContinuous monitoring of churn metrics post-trial and iterating on trial offerings based on customer feedback and behavior will be crucial for improving conversion rates and overall subscriber retention.\n\n\n\n\n\n\nWITH next_plan_cte AS\n    (SELECT subscriptions.customer_id,\n        subscriptions.plan_id, plans.plan_name, start_date,\n        LEAD(subscriptions.plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\n        FROM subscriptions\n        JOIN plans ON subscriptions.plan_id = plans.plan_id)\nSELECT next_plan_cte.plan_name,\n    COUNT(next_plan_cte.customer_id) AS customer_cnt,  -- Specify the table for clarity\n    ROUND(100.0 * COUNT(next_plan_cte.customer_id) / (SELECT COUNT(DISTINCT customer_id) AS distinct_customers FROM subscriptions), 2) AS \"percentage\"  -- Use double quotes for aliases\nFROM next_plan_cte\nWHERE next_plan_cte.plan_name != 'trial'\nGROUP BY next_plan_cte.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_cnt\npercentage\n\n\n\n\nbasic monthly\n546\n54.60\n\n\npro annual\n258\n25.80\n\n\nchurn\n307\n30.70\n\n\npro monthly\n539\n53.90\n\n\n\n\n\n\nCustomer Plan Distribution:\n\nBasic Monthly: 546 customers (54.60%) opted for the Basic Monthly plan after their free trial.\nPro Annual: 258 customers (25.80%) chose the Pro Annual plan after their free trial.\nChurn: 307 customers (30.70%) churned after their free trial, indicating that they did not continue with any paid subscription.\nPro Monthly: 539 customers (53.90%) selected the Pro Monthly plan after their free trial.\n\nObservations:\n\nThe majority of customers opt for either the Basic Monthly or Pro Monthly plans after the free trial, accounting for approximately 54.60% and 53.90%, respectively.\nA significant portion of customers (25.80%) choose the Pro Annual plan, indicating a preference for longer-term commitments.\nThe churn rate after the free trial is notably high, with 30.70% of customers opting out of any paid subscription.\n\nImplications:\n\nOffering a variety of subscription plans caters to different customer preferences and financial capabilities.\nUnderstanding the reasons behind churn after the trial period is crucial for improving retention and conversion rates.\nImplementing targeted marketing strategies, personalized offers, and enhancing the value proposition for paid plans can help reduce churn and increase subscription conversions post-trial.\n\n\n\n\n\n\nWITH activeSubscriptions AS (\nSELECT S.customer_id, S.start_date, P.plan_name,\n    LEAD(S.start_date) OVER (PARTITION BY S.customer_id ORDER BY S.start_date) AS next_date\nFROM Plans AS P JOIN Subscriptions AS S\nON P.plan_id = S.plan_id)\nSELECT A.plan_name, COUNT(A.customer_id) AS customer_count,\n    ROUND(100.0 * (COUNT(A.customer_id))/(SELECT COUNT(DISTINCT customer_id) FROM Subscriptions),2) AS customer_percentage\nFROM activeSubscriptions AS A\nWHERE (next_date IS NOT NULL AND (A.start_date &lt; '2020-12-31' AND A.next_date &gt; '2020-12-31')\n    OR (A.start_date &lt; '2020-12-31' AND A.next_date IS NULL))\nGROUP BY A.plan_name\nORDER BY A.plan_name;\nOutput:\n\n\n\nplan_name\ncustomer_count\ncustomer_percentage\n\n\n\n\nbasic monthly\n224\n22.40\n\n\nchurn\n235\n23.50\n\n\npro annual\n195\n19.50\n\n\npro monthly\n326\n32.60\n\n\ntrial\n19\n1.90\n\n\n\n\n\n\nCustomer Distribution:\n\nBasic Monthly: There are 224 customers (22.40%) subscribed to the Basic Monthly plan.\nChurn: 235 customers (23.50%) have churned, meaning they no longer have an active subscription.\nPro Annual: 195 customers (19.50%) are subscribed to the Pro Annual plan.\nPro Monthly: 326 customers (32.60%) have opted for the Pro Monthly plan.\nTrial: A small portion of customers, 19 (1.90%), are still in the trial phase as of December 31, 2020.\n\nObservations:\n\nThe Pro Monthly plan has the highest customer count at 32.60%, indicating its popularity among subscribers.\nBasic Monthly and Churn plans have relatively similar customer counts, with 22.40% and 23.50%, respectively.\nThe Pro Annual plan has a lower customer count compared to Pro Monthly but still maintains a significant portion at 19.50%.\nThe Trial phase has a minimal impact on the customer base, with only 1.90% of customers still in the trial period.\n\nImplications:\n\nUnderstanding the distribution of customers across different plans helps in evaluating the effectiveness of pricing strategies and plan offerings.\nAnalyzing churn rates alongside active subscriptions provides insights into customer retention and satisfaction levels.\nTargeted marketing and retention efforts can be tailored based on plan preferences and customer behavior to maximize subscription revenue and minimize churn.\n\n\n\n\n\n\nSELECT\n    COUNT(S.customer_id) AS num_of_customers\nFROM subscriptions AS S\nWHERE plan_id = 3 AND YEAR(start_date) = '2020';\nOutput:\n\n\n\nnum_of_customers\n\n\n\n\n195\n\n\n\n\n\n\nNumber of Upgrades:\n\nAnnual Plan Upgrades: In 2020, a total of 195 customers upgraded to the annual subscription plan.\n\nObservations:\n\nThe annual subscription plan attracted a considerable number of customers, indicating its appeal and value proposition.\n\nImplications:\n\nThe popularity of the annual plan upgrade suggests that customers are interested in committing to Foodie-Fi for a longer duration, possibly due to cost savings or enhanced benefits offered by the annual subscription.\n\n\n\n\n\n\nWITH trial_cte AS\n(SELECT customer_id, start_date AS trial_date FROM subscriptions WHERE plan_id = 0),\n    annual_cte AS\n (SELECT customer_id, start_date AS annual_date FROM subscriptions WHERE plan_id = 3)\nSELECT ROUND(AVG(DATEDIFF(annual_date,trial_date)),0) AS 'avg_num_of_days'\nFROM trial_cte JOIN annual_cte ON trial_cte.customer_id = annual_cte.customer_id;\nOutput:\n\n\n\navg_num_of_days\n\n\n\n\n105\n\n\n\n\n\n\nAverage Time to Upgrade:\n\nAverage Duration: On average, it takes approximately 105 days for customers to upgrade from the trial plan to an annual subscription plan.\n\nObservations:\n\nCustomers typically take over three months to transition from the trial plan to the annual subscription, indicating a deliberative decision-making process or possibly a trial period evaluation.\n\nImplications:\n\nFoodie-Fi can implement targeted campaigns or incentives to prompt trial users to upgrade sooner, thereby increasing conversion rates and revenue. Additionally, personalized communication or offers tailored to users’ preferences can expedite the upgrade process.\n\n\n\n\n\n\nWITH trial_plan AS (\n  SELECT customer_id, start_date AS trial_date\n  FROM foodie_fi.subscriptions WHERE plan_id = 0\n), annual_plan AS (\n  SELECT\n    customer_id, start_date AS annual_date\n  FROM foodie_fi.subscriptions WHERE plan_id = 3\n), bins AS (\n  -- bins CTE: Put customers in 30-day buckets based on the average number of days taken to upgrade to a pro annual plan.\n  SELECT\n    FLOOR((DATEDIFF(annual.annual_date, trial.trial_date) - 1) / 30) + 1 AS avg_days_to_upgrade\n  FROM trial_plan AS trial\n  JOIN annual_plan AS annual\n    ON trial.customer_id = annual.customer_id\n)\nSELECT\n  CONCAT(((avg_days_to_upgrade - 1) * 30 + 1), ' - ', (avg_days_to_upgrade * 30), ' days') AS day_period_bucket,\n  COUNT(*) AS num_of_customers\nFROM bins\nGROUP BY avg_days_to_upgrade\nORDER BY avg_days_to_upgrade;\nOutput:\n\n\n\nday_period_bucket\nnum_of_customers\n\n\n\n\n1 - 30 days\n49\n\n\n31 - 60 days\n24\n\n\n61 - 90 days\n34\n\n\n91 - 120 days\n35\n\n\n121 - 150 days\n42\n\n\n151 - 180 days\n36\n\n\n181 - 210 days\n26\n\n\n211 - 240 days\n4\n\n\n241 - 270 days\n5\n\n\n271 - 300 days\n1\n\n\n301 - 330 days\n1\n\n\n331 - 360 days\n1\n\n\n\n\n\n\nKey Findings:\n\nInitial Upgrade Activity: The majority of customers (49) upgrade within the first month (1 - 30 days) after their trial period ends, indicating prompt conversion for a significant portion of users.\nGradual Adoption: A notable number of customers continue to upgrade gradually over time, with 24 customers upgrading between 31 to 60 days, and 34 between 61 to 90 days.\nSteady Conversion: Conversion remains consistent over time, with a relatively even distribution of customers upgrading across subsequent 30-day intervals, until a decline observed after 180 days.\n\nObservations:\n\nQuick Conversions: The early spike in upgrades suggests that a substantial portion of customers are convinced of the value proposition shortly after their trial ends.\nLonger Adoption Period: Some customers take longer to convert, possibly indicating a longer evaluation or decision-making process, necessitating ongoing engagement strategies during this period.\nLate Adopters: A few customers upgrade much later, highlighting the importance of persistent engagement efforts to encourage conversion even beyond the initial months.\n\n\n\n\n\n\nWITH downgraded_cte AS\n(SELECT customer_id, plan_id, start_date,\n    LEAD(plan_id, 1) OVER (PARTITION BY customer_id ORDER BY start_date) AS next_plan\nFROM subscriptions)\nSELECT COUNT(customer_id)\nFROM downgraded_cte\nWHERE plan_id = '2' and next_plan = '1' AND YEAR(start_date);\nOutput:\n\n\n\ncustomer_cnt\n\n\n\n\n0\n\n\n\n\n\n\nKey Finding:\n\nNo Downgrades Detected: There were no instances of customers downgrading from the pro monthly plan to the basic monthly plan in 2020, indicating a retention of pro monthly subscribers or a lack of documented downgrades during this period."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#c.-challenge-payment-question-1",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#c.-challenge-payment-question-1",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "The Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n-- Use a recursive CTE to increment rows for all monthly paid plans until customers changing the plan, except 'pro annual'\nWITH RECURSIVE dateRecursion AS (\n    SELECT\n        s.customer_id, s.plan_id, p.plan_name, s.start_date AS payment_date,\n        -- column last_date: last day of the current plan\n        CASE\n            -- if a customer kept using the current plan, last_date = '2020-12-31'\n            WHEN LEAD(s.start_date) OVER(PARTITION BY s.customer_id ORDER BY s.start_date) IS NULL THEN '2020-12-31'\n            -- if a customer changed the plan, last_date = (month difference between start_date and changing date) + start_date\n            ELSE DATE_ADD(\n                start_date,\n                INTERVAL DATEDIFF(LEAD(s.start_date) OVER(PARTITION BY s.customer_id ORDER BY s.start_date), start_date) MONTH\n            ) END AS last_date,\n        p.price AS amount\n    FROM subscriptions s\n    JOIN plans p ON s.plan_id = p.plan_id\n    -- exclude trials because they didn't generate payments\n    WHERE p.plan_name NOT IN ('trial')\n        AND YEAR(start_date) = 2020\n\n    UNION ALL\n\n    SELECT\n        customer_id,\n        plan_id,\n        plan_name,\n        -- increment payment_date by monthly\n        DATE_ADD(payment_date, INTERVAL 1 MONTH) AS payment_date,\n        last_date,\n        amount\n    FROM dateRecursion\n    -- stop incrementing when payment_date = last_date\n    WHERE DATE_ADD(payment_date, INTERVAL 1 MONTH) &lt;= last_date\n        AND plan_name != 'pro annual'\n)\n-- Create a new table [payments]\nSELECT\n    customer_id,\n    plan_id,\n    plan_name,\n    payment_date,\n    amount,\n    ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY payment_date) AS payment_order\nFROM dateRecursion\n-- exclude churns\nWHERE amount IS NOT NULL\nORDER BY customer_id;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#d.-outside-the-box-questions-1",
    "href": "portfolio/8WeeksSQLChallenge-Foodie_Fi/index.html#d.-outside-the-box-questions-1",
    "title": "Case Study #3 - Foodie Fi.",
    "section": "",
    "text": "The following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\n\nAnswer:\nTo assess the rate of growth for Foodie-Fi, we should analyze the following multiple key indicators:\n\nCustomer Base Growth:\n\nCalculate the overall growth in the customer base over specific time periods (for instance, monthly or quarterly.),.\nMonitor the acquisition of new customers and the retention of the existing ones.\n\nSubscription Plan Adoption:\n\nAnalyze the uptake of different subscription plans (Basic, Pro Monthly, Pro Annual) to understand the customer preferences.\nEvaluate the growth rate of Pro Plans compared to the overall customer base.\n\nChurn Reduction:\n\nInvestigate the reduction in churn by assessing the number of customers downgrading or canceling their subscriptions.\nCalculate the churn rate and observe its trend over time.\n\nRevenue Growth:\n\nMonitor the revenue growth over time, by considering both the increase in the customer base and potential changes in subscription plans.\n\nConversion Rates:\n\nAnalyze the conversion rates from Trial users to Paid subscribers, providing insights into the effectiveness of the trial period in conversting users.\n\n\n\n\n\nAnswer:\nTo comprehensively assess the performance of Foodie-Fi, it is recommended to track the following metrics over time.\n\nCustomer Base Growth:\n\nMonitor the overall growth or contraction of the customer base on a regular basis (for instance, monthly, or quarterly or annually).\n\nRevenue Breakdown:\n\nAnalyze the revenue streams from different subscription plans, with a focus on Pro plan customers. This breakdown helps understand the contribution of various plans to the overall revenue.\n\nChurn Rate:\n\nCalculate and track the churn rate over time. Identifying patterns in customer attrition provides insights into service quality, and customer statisfaction and hence, increase in the customer base.\n\nCustomer Acquisition and Conversion Rates:\n\nAssess the effectiveness of marketing strategies by tracking customer acquisition rates. Additionally, monitor the conversion rates from tria to paid subscriptions.\n\nAverage Revenue Per User (ARPU):\n\nCalculate the average revenue generated by each user to understand the averae contribution of each customers to the overall revenue.\n\n\n\n\n\nAnswer:\nThe key areas to understand the customer retention are the following ones:\n\nPlan Changes:\n\nUnderstand the specific triggers that lead the customers to changes their plans. This could involve analyzing the benefits they seek, such as unlimited streaming or offline access. Additionally, explore whether promotional offers or plan-specific features influences plan changes.\nImplement an exit survey strategy for users who either downgrade or cancel their plans. Collecting feedback directly from users can uncover nuanced insights into their decision making process.\n\nCustomer Support:\n\nWe can identify the common queries, concerns, or issues faced by customers from the customer support interactions. Additionally track how effectively customer issues are resolved over time. This helps us understand if the customers are unhappy with the resolutions and our services, thereby leading them to churn out.\n\nContent Preferences:\n\nExplore not only what customers are looking for but also gather insights into why certain content might not meet their expectations. Are there any programs that are not in the video library because of which customers are looking for? Conducting surveys of feedback sessions helps to understand specific content preferences and expectations.\n\nPlatform Comparison:\n\nConsider comparing Foodie-Fi with other platforms. Identify features, shows that competitors offer and where Foodie-Fi might lack. This competitive analysis can reveal opportunities for improvement. Additionally assess user perceptions of Foodie-Fi to competitors through surveys or reviews.\n\nPersonalized Recommendations:\n\nUnderstand how personalized recommendation can positively impact retention. Analyze successful cases where tailored content suggestions lead to increased engagement. Implement ML algorithms to enhance recommendation precision, ensuring users receive content aligned with their preferences.\nImplement a feedback mechanism for users to provide explicit feedback on recommended content. This can refine algorithms and enhance the accuracy of personalized suggestions.\n\n\n\n\n\nAnswer:\nSome valuable questions to include in the exit survey shown to customers who wish to cancel their subscription could be the following:\n\nWhat did you like most about Foodie-Fi, and is there anything specific you would like to see changed or improved on the program?\nAre you satisfied with the current pricing of our plans?\nHow would you rate your satisfaction with our customer support? If not entirely satisfied, please provide details on areas for improvement.\nDid you find the specific program you were looking for for in our Video Library? If not, could you share some details about the content you were seeking?\nWhat is the primary reason for canceling your subscription? (Include options related to content, pricing, customer service, etc.)\nIs there anything else you would like to share or any feedback that hasn’t been covered in the previous questions?\n\n\n\n\nAnswer:\n\nEnhanced Content Library:\n\nIdea: Regularly update and expand the content library with exclusive shows and programs.\nValidation: Monitor the time spent by the customers on the platform, and the frequency of login hours, and take direct feedback in the form of a question like, Is this the show you were looking for?\n\nPersonalized Recommendations:\n\nIdea: Implement advanced algorithms to provide personalized content recommendations based on Individual preferences and viewing history.\nValidation: Track the percentage of users following the personalized recommendations and ask for feedback to check the satisfaction of the customers.\n\nPromotional Offers and Discounts:\n\nIdea: Introduce limited-time promotions, discounts, or bundled subscription options to incentivize customer retention.\nValidation: Analyze the impact on customer retention by doing the before and after analysis.\n\nImproved Customer Support:\n\nIdea: Enhance customer support services by reducing response times, resolving queries effectively in a short period, and gathering feedback for continuous improvement.\nValidation: Guage the customer churn percentage, and customer satisfaction scores, and collect feedback on customer support interactions.\n\nExit Surveys and Feedback Analysis:\n\nIdea: Implement an exit survey for customers canceling their subscriptions to gather insights into reasons for leaving.\nValidation: Analyze any common reason to leave the platform and work on it."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "Did you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)\nDanny was scrolling through his Instagram feed when something really caught his eye - “80s Retro Styling and Pizza Is The Future!”\nDanny was sold on the idea, but he knew that pizza alone was not going to help him get seed funding to expand his new Pizza Empire - so he had one more genius idea to combine with it - he was going to Uberize it - and so Pizza Runner was launched!\nDanny started by recruiting “runners” to deliver fresh pizza from Pizza Runner Headquarters (otherwise known as Danny’s house) and also maxed out his credit card to pay freelance developers to build a mobile app to accept orders from customers.\n\n\n\nBecause Danny had a few years of experience as a data scientist - he was very aware that data collection was going to be critical for his business’ growth.\nHe has prepared for us an entity relationship diagram of his database design but requires further assistance to clean his data and apply some basic calculations so he can better direct his runners and optimise Pizza Runner’s operations.\nAll datasets exist within the pizza_runner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\n\n\n\n\nThe runners table shows the registration_date for each new runner\n\n\n\nrunner_id\nregistration_date\n\n\n\n\n1\n2021-01-01\n\n\n2\n2021-01-03\n\n\n3\n2021-01-08\n\n\n4\n2021-01-15\n\n\n\n\n\n\nCustomer pizza orders are captured in the customer_orders table with 1 row for each individual pizza that is part of the order.\nThe pizza_id relates to the type of pizza which was ordered whilst the exclusions are the ingredient_id values which should be removed from the pizza and the extras are the ingredient_id values which need to be added to the pizza.\nNote that customers can order multiple pizzas in a single order with varying exclusions and extras values even if the pizza is the same type!\nThe exclusions and extras columns will need to be cleaned up before using them in your queries.\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2021-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2021-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2021-01-02 23:51:23\n\n\n3\n102\n2\nNaN\n\n2021-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2021-01-04 13:23:46\n\n\n5\n104\n1\nnull\n1\n2021-01-08 21:00:29\n\n\n6\n101\n2\nnull\nnull\n2021-01-08 21:03:13\n\n\n7\n105\n2\nnull\n1\n2021-01-08 21:20:29\n\n\n8\n102\n1\nnull\nnull\n2021-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2021-01-10 11:22:59\n\n\n10\n104\n1\nnull\nnull\n2021-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2021-01-11 18:34:49\n\n\n\n\n\n\nAfter each orders are received through the system - they are assigned to a runner - however not all orders are fully completed and can be cancelled by the restaurant or the customer.\nThe pickup_time is the timestamp at which the runner arrives at the Pizza Runner headquarters to pick up the freshly cooked pizzas. The distance and duration fields are related to how far and long the runner had to travel to deliver the order to the respective customer.\nThere are some known data issues with this table so be careful when using this in your queries - make sure to check the data types for each column in the schema SQL!\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2021-01-01 18:15:34\n20km\n32 minutes\n\n\n\n2\n1\n2021-01-01 19:10:54\n20km\n27 minutes\n\n\n\n3\n1\n2021-01-03 00:12:37\n13.4km\n20 mins\nNaN\n\n\n4\n2\n2021-01-04 13:53:03\n23.4\n40\nNaN\n\n\n5\n3\n2021-01-08 21:10:57\n10\n15\nNaN\n\n\n6\n3\nnull\nnull\nnull\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25km\n25mins\nnull\n\n\n8\n2\n2020-01-10 00:15:02\n23.4 km\n15 minute\nnull\n\n\n9\n2\nnull\nnull\nnull\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10 minutes\nnull\n\n\n\n\n\n\nAt the moment - Pizza Runner only has 2 pizzas available the Meat Lovers or Vegetarian!\n\n\n\npizza_id\npizza_name\n\n\n\n\n1\nMeat Lovers\n\n\n2\nVegetarian\n\n\n\n\n\n\nEach pizza_id has a standard set of toppings which are used as part of the pizza recipe.\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n\n\n\n\nThis table contains all of the topping_name values with their corresponding topping_id value\n\n\n\ntopping_id\ntopping_name\n\n\n\n\n1\nBacon\n\n\n2\nBBQ Sauce\n\n\n3\nBeef\n\n\n4\nCheese\n\n\n5\nChicken\n\n\n6\nMushrooms\n\n\n7\nOnions\n\n\n8\nPepperoni\n\n\n9\nPeppers\n\n\n10\nSalami\n\n\n11\nTomatoes\n\n\n12\nTomato Sauce\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA pizza_runner;\nUSE pizza_runner;\n\nDROP TABLE IF EXISTS runners;\nCREATE TABLE runners (\n  `runner_id` INTEGER,\n  `registration_date` DATE\n);\nINSERT INTO runners\n  (`runner_id`, `registration_date`)\nVALUES\n  (1, '2021-01-01'),\n  (2, '2021-01-03'),\n  (3, '2021-01-08'),\n  (4, '2021-01-15');\n\nDROP TABLE IF EXISTS customer_orders;\nCREATE TABLE customer_orders (\n  `order_id` INTEGER,\n  `customer_id` INTEGER,\n  `pizza_id` INTEGER,\n  `exclusions` VARCHAR(4),\n  `extras` VARCHAR(4),\n  `order_time` TIMESTAMP\n);\n\nINSERT INTO customer_orders\n  (`order_id`, `customer_id`, `pizza_id`, `exclusions`, `extras`, `order_time`)\nVALUES\n  ('1', '101', '1', '', '', '2020-01-01 18:05:02'),\n  ('2', '101', '1', '', '', '2020-01-01 19:00:52'),\n  ('3', '102', '1', '', '', '2020-01-02 23:51:23'),\n  ('3', '102', '2', '', NULL, '2020-01-02 23:51:23'),\n  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n  ('4', '103', '2', '4', '', '2020-01-04 13:23:46'),\n  ('5', '104', '1', 'null', '1', '2020-01-08 21:00:29'),\n  ('6', '101', '2', 'null', 'null', '2020-01-08 21:03:13'),\n  ('7', '105', '2', 'null', '1', '2020-01-08 21:20:29'),\n  ('8', '102', '1', 'null', 'null', '2020-01-09 23:54:33'),\n  ('9', '103', '1', '4', '1, 5', '2020-01-10 11:22:59'),\n  ('10', '104', '1', 'null', 'null', '2020-01-11 18:34:49'),\n  ('10', '104', '1', '2, 6', '1, 4', '2020-01-11 18:34:49');\n\nDROP TABLE IF EXISTS runner_orders;\nCREATE TABLE runner_orders (\n  `order_id` INTEGER,\n  `runner_id` INTEGER,\n  `pickup_time` VARCHAR(19),\n  `distance` VARCHAR(7),\n  `duration` VARCHAR(10),\n  `cancellation` VARCHAR(23)\n);\n\nINSERT INTO runner_orders\n  (`order_id`, `runner_id`, `pickup_time`, `distance`, `duration`, `cancellation`)\nVALUES\n  ('1', '1', '2020-01-01 18:15:34', '20km', '32 minutes', ''),\n  ('2', '1', '2020-01-01 19:10:54', '20km', '27 minutes', ''),\n  ('3', '1', '2020-01-03 00:12:37', '13.4km', '20 mins', NULL),\n  ('4', '2', '2020-01-04 13:53:03', '23.4', '40', NULL),\n  ('5', '3', '2020-01-08 21:10:57', '10', '15', NULL),\n  ('6', '3', 'null', 'null', 'null', 'Restaurant Cancellation'),\n  ('7', '2', '2020-01-08 21:30:45', '25km', '25mins', 'null'),\n  ('8', '2', '2020-01-10 00:15:02', '23.4 km', '15 minute', 'null'),\n  ('9', '2', 'null', 'null', 'null', 'Customer Cancellation'),\n  ('10', '1', '2020-01-11 18:50:20', '10km', '10minutes', 'null');\n\nDROP TABLE IF EXISTS pizza_names;\nCREATE TABLE pizza_names (\n  `pizza_id` INTEGER,\n  `pizza_name` TEXT\n);\nINSERT INTO pizza_names\n  (`pizza_id`, `pizza_name`)\nVALUES\n  (1, 'Meatlovers'),\n  (2, 'Vegetarian');\n\nDROP TABLE IF EXISTS pizza_recipes;\nCREATE TABLE pizza_recipes (\n  `pizza_id` INTEGER,\n  `toppings` TEXT\n);\nINSERT INTO pizza_recipes\n  (`pizza_id`, `toppings`)\nVALUES\n  (1, '1, 2, 3, 4, 5, 6, 8, 10'),\n  (2, '4, 6, 7, 9, 11, 12');\n\nDROP TABLE IF EXISTS pizza_toppings;\nCREATE TABLE pizza_toppings (\n  `topping_id` INTEGER,\n  `topping_name` TEXT\n);\nINSERT INTO pizza_toppings\n  (`topping_id`, `topping_name`)\nVALUES\n  (1, 'Bacon'),\n  (2, 'BBQ Sauce'),\n  (3, 'Beef'),\n  (4, 'Cheese'),\n  (5, 'Chicken'),\n  (6, 'Mushrooms'),\n  (7, 'Onions'),\n  (8, 'Pepperoni'),\n  (9, 'Peppers'),\n  (10, 'Salami'),\n  (11, 'Tomatoes'),\n  (12, 'Tomato Sauce');\n\n\n\nThis case study has LOTS of questions - they are broken up by area of focus including:\n\nPizza Metrics\nRunner and Customer Experience\nIngredient Optimisation\nPricing and Ratings\nBonus DML Challenges (DML = Data Manipulation Language)\n\nEach of the following case study questions can be answered using a single SQL statement.\nAgain, there are many questions in this case study - please feel free to pick and choose which ones you’d like to try!\nBefore you start writing your SQL queries however - you might want to investigate the data, you may want to do something with some of those null values and data types in the customer_orders and runner_orders tables!\n\n\n\nHow many pizzas were ordered?\nHow many unique customer orders were made?\nHow many successful orders were delivered by each runner?\nHow many of each type of pizza was delivered?\nHow many Vegetarian and Meatlovers were ordered by each customer?\nWhat was the maximum number of pizzas delivered in a single order?\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\nHow many pizzas were delivered that had both exclusions and extras?\nWhat was the total volume of pizzas ordered for each hour of the day?\nWhat was the volume of orders for each day of the week?\n\n\n\n\n\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\nWhat was the average distance travelled for each customer?\nWhat was the difference between the longest and shortest delivery times for all orders?\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\nWhat is the successful delivery percentage for each runner?\n\n\n\n\n\nWhat are the standard ingredients for each pizza?\nWhat was the most commonly added extra?\nWhat was the most common exclusion?\nGenerate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nGenerate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients\n\nFor example: \"Meat Lovers: 2xBacon, Beef, ... , Salami\".\n\nWhat is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\nIf a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\nWhat if there was an additional $1 charge for any pizza extras?\n\nAdd cheese is $1 extra\n\nThe Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\nUsing your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\nIf a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?\n\n\n\n\n\n\n\n\n\nDropping the customer_orders_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS customer_orders_temp\n\nCREATE TEMPORARY TABLE customer_orders_temp AS\nSELECT order_id, customer_id, pizza_id,\nCASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN ''\nELSE exclusions END AS exclusions,\nCASE WHEN extras IS NULL OR extras LIKE 'null' THEN ''\nELSE extras END AS extras,\norder_time\nFROM customer_orders;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2020-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2020-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2020-01-02 23:51:23\n\n\n3\n102\n2\n\n\n2020-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2020-01-04 13:23:46\n\n\n5\n104\n1\n\n1\n2020-01-08 21:00:29\n\n\n6\n101\n2\n\n\n2020-01-08 21:03:13\n\n\n7\n105\n2\n\n1\n2020-01-08 21:20:29\n\n\n8\n102\n1\n\n\n2020-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2020-01-10 11:22:59\n\n\n10\n104\n1\n\n\n2020-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\nDropping the runner_orders_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS runner_orders_temp\n\nCREATE TEMPORARY TABLE runner_orders_temp AS\nSELECT order_id, runner_id,\n    CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time,\n    CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance)\n    ELSE distance END AS FLOAT) AS distance,\nCAST(CASE WHEN duration LIKE \"null\" THEN NULL\n    WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration)\n    WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration)\n    WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration)\n    ELSE duration END AS FLOAT) AS duration,\n    CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL\n    ELSE cancellation END AS cancellation\nFROM runner_orders;\nChanging the Data Types of columns in runner_orders TABLE\nALTER TABLE runner_orders_temp\nMODIFY COLUMN pickup_time DATETIME,\nMODIFY COLUMN distance FLOAT,\nMODIFY COLUMN duration FLOAT;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2020-01-01 18:15:34\n20\n32\n\n\n\n2\n1\n2020-01-01 19:10:54\n20\n27\n\n\n\n3\n1\n2020-01-03 00:12:37\n13.4\n20\n\n\n\n4\n2\n2020-01-04 13:53:03\n23.4\n40\n\n\n\n5\n3\n2020-01-08 21:10:57\n10\n15\n\n\n\n6\n3\n\n\n\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25\n25\n\n\n\n8\n2\n2020-01-10 00:15:02\n23.4\n15\n\n\n\n9\n2\n\n\n\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10\n10\n\n\n\n\n\n\n\n\n\n\nSELECT COUNT(order_id) AS pizza_cnt\nFROM customer_orders_temp;\nOutput:\n\n\n\npizza_cnt\n\n\n\n\n14\n\n\n\n\n\n\nPizza Order Overview:\n\nA total of 14 pizzas were ordered during the period under consideration.\nUnderstanding the volume of pizza orders helps in assessing demand and operational requirements for Pizza Runner.\n\n\n\n\n\n\nSELECT COUNT(DISTINCT order_id) AS unique_orders\nFROM customer_orders_temp;\nOutput: | unique_orders | |—————| | 10 |\n\n\n\nInsights into Customer Order Frequency:\n\nA total of 10 unique customer orders were made during the specified period.\nUnderstanding the frequency of unique orders provides insights into customer engagement and the overall demand for Pizza Runner’s services.\n\n\n\n\n\n\nSELECT\n    runner_id,\n    COUNT(order_id) AS `successful_delivery`\nFROM runner_orders\nWHERE distance != 0\nGROUP BY runner_id;\nOutput:\n\n\n\nrunner_id\nsuccessful_delivery\n\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n1\n\n\n\n\n\n\nEfficiency in Order Delivery:\n\nRunner 1 completed the highest number of successful deliveries, with a total of 4 orders fulfilled.\nRunner 2 follows closely behind, with 3 successful deliveries.\nRunner 3 completed 1 successful delivery during the specified period.\n\n\n\n\n\n\nSELECT\n    pizza_name,\n    COUNT(runner_orders_temp.order_id) as pizza_cnt\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nJOIN pizza_names ON pizza_names.pizza_id = customer_orders_temp.pizza_id\nWHERE distance != 0\nGROUP BY pizza_name;\nOutput:\n\n\n\npizza_name\npizza_cnt\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\nDistribution of Pizza Types:\n\nMeat Lovers pizza was the most frequently delivered, with a total of 9 orders fulfilled.\nVegetarian pizza accounted for a smaller portion of deliveries, with 3 orders fulfilled.\n\n\n\n\n\n\nSELECT\n    customer_id,\n    pizza_name,\n    COUNT(order_id) as order_cnt\nFROM customer_orders_temp\nJOIN pizza_names ON customer_orders_temp.pizza_id = pizza_names.pizza_id\nGROUP BY customer_id, pizza_name\nORDER BY customer_id, pizza_name;\nOutput:\n\n\n\ncustomer_id\npizza_name\norder_cnt\n\n\n\n\n101\nMeatlovers\n2\n\n\n101\nVegetarian\n1\n\n\n102\nMeatlovers\n2\n\n\n102\nVegetarian\n1\n\n\n103\nMeatlovers\n3\n\n\n103\nVegetarian\n1\n\n\n104\nMeatlovers\n3\n\n\n105\nVegetarian\n1\n\n\n\n\n\n\nCustomer Pizza Preferences:\n\nCustomer 101 ordered 2 Meatlovers pizzas and 1 Vegetarian pizza, indicating a preference for both varieties.\nSimilarly, Customer 102 ordered 2 Meatlovers pizzas and 1 Vegetarian pizza, suggesting a balanced preference for different pizza types.\nCustomer 103 predominantly ordered Meatlovers pizzas, with 3 orders, and also ordered 1 Vegetarian pizza, indicating a preference for meat-based options but also an interest in vegetarian choices.\nCustomer 104 ordered 3 Meatlovers pizzas, indicating a strong preference for this type, while Customer 105 ordered 1 Vegetarian pizza, suggesting a preference for meat-free options.\n\n\n\n\n\n\nSELECT customer_orders_temp.order_id,\n        COUNT(runner_orders_temp.order_id) as pizza_cnt\nFROM customer_orders_temp\nJOIN runner_orders_temp\nON customer_orders_temp.order_id = runner_orders_temp.order_id\nGROUP BY customer_orders_temp.order_id\nORDER BY pizza_cnt DESC;\nOutput:\n\n\n\norder_id\npizza_cnt\n\n\n\n\n4\n3\n\n\n3\n2\n\n\n10\n2\n\n\n1\n1\n\n\n2\n1\n\n\n5\n1\n\n\n6\n1\n\n\n7\n1\n\n\n8\n1\n\n\n9\n1\n\n\n\n\n\n\nOrder Size Overview:\n\nOrder ID 4 recorded the highest number of pizzas delivered in a single order, with 3 pizzas.\nOrders 3 and 10 followed, each consisting of 2 pizzas, indicating moderate order sizes.\nSeveral orders, including IDs 1, 2, 5, 6, 7, 8, and 9, were comprised of a single pizza, representing smaller order sizes.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    SUM(CASE WHEN exclusions &lt;&gt; '' OR extras &lt;&gt; '' THEN 1 ELSE 0 END) AS 'Change',\n    SUM(CASE WHEN exclusions = '' AND extras = '' THEN 1 ELSE 0 END) AS 'No_Change'\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY customer_orders_temp.customer_id;\nOutput:\n\n\n\ncustomer_id\nchange\nno_change\n\n\n\n\n101\n0\n2\n\n\n102\n0\n3\n\n\n103\n3\n0\n\n\n104\n2\n1\n\n\n105\n1\n0\n\n\n\n\n\n\nChange vs. No Change:\n\nCustomer 101 and Customer 102 placed orders without any modifications, indicating a preference for standard pizza options without exclusions or extras.\nCustomer 103 exclusively ordered pizzas with modifications, suggesting a preference for customized or personalized options tailored to specific dietary preferences or taste preferences.\nCustomer 104 had a mix of orders with and without changes, indicating a varied preference for both standard and customized pizza options.\nCustomer 105 ordered pizzas with at least one change, reflecting a preference for personalized pizza options.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    SUM(CASE WHEN exclusions != '' AND extras != '' THEN 1 ELSE 0 END)\n        AS 'pizza_with_exclusions_and_extras'\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.pickup_time IS NOT NULL AND exclusions != '' AND extras != ''\nGROUP BY customer_orders_temp.customer_id\nORDER BY SUM(CASE WHEN exclusions != '' AND extras != '' THEN 1 ELSE 0 END)  DESC;\nOutput:\n\n\n\ncustomer_id\npizza_with_exclusions_and_extras\n\n\n\n\n104\n1\n\n\n\n\n\n\nPizza Customization Trends:\n\nCustomer 104 placed an order that included both exclusions and extras, indicating a preference for a customized pizza with specific modifications to the standard recipe.\n\n\n\n\n\n\nSELECT\n    HOUR(order_time) as 'hour',\n    COUNT(order_id)\nFROM customer_orders_temp\nGROUP BY HOUR(order_time)\nORDER BY HOUR(order_time);\nOutput:\n\n\n\nhour\norder_cnt\n\n\n\n\n11\n1\n\n\n13\n3\n\n\n18\n3\n\n\n19\n1\n\n\n21\n3\n\n\n23\n3\n\n\n\n\n\n\nPeak Ordering Hours:\n\nPizza orders exhibit fluctuations throughout the day, with distinct peaks during specific hours.\nThe busiest hours for pizza orders are observed between 1 PM and 3 PM, with a total volume of 3 orders during each hour.\n\n\n\n\n\n\nSELECT\n    DAYNAME(order_time) as 'day_of_the_week',\n    COUNT(order_id)\nFROM customer_orders_temp\nGROUP BY DAYNAME(order_time)\nORDER BY DAYNAME(order_time) DESC;\nOutput:\n\n\n\nday_of_the_week\norder_cnt\n\n\n\n\nWednesday\n5\n\n\nThursday\n3\n\n\nSaturday\n5\n\n\nFriday\n1\n\n\n\n\n\n\nWeekday vs. Weekend Orders:\n\nPizza orders exhibit variations based on the day of the week, with distinct patterns observed between weekdays and weekends.\nWednesdays and Saturdays emerge as the busiest days for pizza orders, with 5 orders recorded on each day.\nThursdays also demonstrate moderate order volume, with 3 orders placed, indicating consistent demand mid-week.\nFridays recorded the lowest order volume, with only 1 order registered, suggesting a dip in demand at the end of the workweek.\n\n\n\n\n\n\n\n\n\nSELECT\n    WEEK(registration_date) AS 'week',\n    COUNT(runner_id) as num_of_runners\nFROM runners\nGROUP BY WEEK(registration_date);\nOutput:\n\n\n\nweek\nnum_of_runners\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n1\n\n\n\n\n\n\nWeekly Runner Acquisition:\n\nRunner sign-ups fluctuate across different weeks, indicating variations in recruitment efforts and market response over time.\nIn Week 0 (starting from January 1, 2021), 1 runner signed up for Pizza Runner, marking the initial stage of recruitment.\nRunner sign-ups increased in Week 1, with 2 new runners joining the platform, suggesting a positive response to initial marketing and recruitment initiatives.\nHowever, in Week 2, the number of new sign-ups decreased to 1 runner, indicating potential challenges or fluctuations in recruitment effectiveness.\n\n\n\n\n\n\n\nWITH runners_pick_cte AS (\nSELECT runner_id,\n    ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time\nFROM runner_orders_temp\nJOIN customer_orders_temp ON runner_orders_temp.order_id = customer_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY runner_id\n)\nSELECT ROUND(AVG(avg_time),0) AS avg_pick_time\nFROM runners_pick_cte;\nOutput:\n\n\n\navg_pickup_time\n\n\n\n\n16\n\n\n\n\n\n\nAverage Pickup Time:\n\nThe average pickup time for all runners is approximately 16 minutes, indicating the typical duration between order placement and runner arrival at the Pizza Runner HQ.\nThis metric provides insights into the efficiency of runner operations and the responsiveness of the delivery network in fulfilling customer orders promptly.\n\n\n\n\n\n\n\nWITH order_count_cte AS (\nSELECT\n    customer_orders_temp.order_id,\n    COUNT(customer_orders_temp.order_id) AS pizza_order_count,\n    ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time_to_prepare\nFROM runner_orders_temp\nJOIN customer_orders_temp ON runner_orders_temp.order_id = customer_orders_temp.order_id\nWHERE pickup_time IS NOT NULL\nGROUP BY customer_orders_temp.order_id\n)\nSELECT pizza_order_count, ROUND(AVG(avg_time_to_prepare),2) AS avg_time_to_prepare\nFROM order_count_cte\nGROUP BY pizza_order_count;\nOutput:\n\n\n\npizza_order_cnt\navg_time_to_prepare\n\n\n\n\n1\n12.00\n\n\n2\n18.00\n\n\n3\n29.00\n\n\n\n\n\n\nAverage Preparation Time by Pizza Quantity:\n\nOrders consisting of a single pizza have an average preparation time of approximately 12 minutes.\nOrders with two pizzas exhibit a slightly longer average preparation time, averaging around 18 minutes.\nOrders comprising three pizzas demonstrate the longest average preparation time, with an average of 29 minutes.\n\nPotential Relationship:\n\nThe analysis suggests a potential positive correlation between the quantity of pizzas in an order and the time required for preparation.\nAs the number of pizzas in an order increases, the preparation time tends to lengthen, indicating a possible relationship between order complexity and processing duration.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    ROUND(AVG(distance),2) as avg_distance\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY customer_orders_temp.customer_id;\nOutput:\n\n\n\ncustomer_id\navg_distance\n\n\n\n\n101\n20\n\n\n102\n16.73\n\n\n103\n23.4\n\n\n104\n10\n\n\n105\n25\n\n\n\n\n\n\nAverage Distance Travelled:\n\nCustomer 101 had an average delivery distance of 20 kilometers, indicating a moderate travel distance per order.\nCustomer 102’s average delivery distance was approximately 16.73 kilometers, suggesting a slightly shorter travel distance compared to Customer 101.\nCustomer 103 had the longest average delivery distance at 23.4 kilometers, indicating a greater geographical spread of delivery locations.\nCustomer 104 had a relatively shorter average delivery distance of 10 kilometers, suggesting closer proximity to Pizza Runner HQ or a more localized customer base.\nCustomer 105 had the highest average delivery distance of 25 kilometers, indicating deliveries to more distant locations or potentially serving customers across a wider geographic area.\n\n\n\n\n\n\nSELECT (MAX(duration) - MIN(duration)) AS diff\nFROM runner_orders_temp;\nOutput:\n\n\n\ndiff\n\n\n\n\n30\n\n\n\n\n\n\nDelivery Time Range:\n\nThe difference between the longest and shortest delivery times for all orders was 30 minutes.\nThis variability indicates fluctuations in delivery durations across different orders, reflecting diverse factors such as distance, traffic conditions, and order complexity.\n\n\n\n\n\n\nSELECT\n    RO.runner_id, RO.order_id,\n    ROUND(AVG(RO.distance/(RO.duration/60)),2) AS average_speed_in_kmph\nFROM runner_orders AS RO\nJOIN customer_orders AS CO ON RO.order_id = CO.order_id\nWHERE RO.distance != 0\nGROUP BY RO.runner_id, RO.order_id\nORDER BY RO.runner_id, RO.order_id ASC;\nOutput:\n\n\n\nrunner_id\norder_id\naverage_speed_in_kmph\n\n\n\n\n1\n1\n37.5\n\n\n1\n2\n44.44\n\n\n1\n3\n40.2\n\n\n1\n10\n60\n\n\n2\n4\n35.1\n\n\n2\n7\n60\n\n\n2\n8\n93.6\n\n\n3\n5\n40\n\n\n\n\n\n\nRunner-Specific Speed Variability:\n\nThe average speed for each runner varied across different deliveries, reflecting differences in route distance, traffic conditions, and delivery durations.\nRunner 1 demonstrated varying speeds across deliveries, with average speeds ranging from 37.5 km/h to 60 km/h.\nRunner 2 exhibited notable speed disparities, with average speeds ranging from 35.1 km/h to 93.6 km/h.\nRunner 3 maintained a relatively consistent average speed of 40 km/h across deliveries.\n\n\n\n\n\n\nSELECT runner_id,\n(ROUND(SUM(CASE WHEN duration IS NOT NULL THEN 1 ELSE 0 END)/COUNT(runner_id),2) * 100) AS percentage\nFROM runner_orders_temp\nGROUP BY runner_id;\nOutput:\n\n\n\nrunner_id\npercentage\n\n\n\n\n1\n100.00\n\n\n2\n75.00\n\n\n3\n50.00\n\n\n\n\n\n\nRunner-Specific Success Rates:\n\nRunner 1 achieved a perfect delivery success rate, with 100% of their deliveries completed successfully. This indicates consistent performance and reliability in fulfilling delivery commitments.\nRunner 2 achieved a delivery success rate of 75%, indicating that 3 out of 4 deliveries were completed successfully. While the success rate is relatively high, there is room for improvement to enhance consistency and reliability.\nRunner 3 achieved a delivery success rate of 50%, indicating that half of their deliveries were completed successfully. This suggests potential challenges or inconsistencies in delivery execution that may require attention.\n\n\n\n\n\n\n\n\n\n\nDropping the pizza_recipes_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS pizza_recipes_temp\n\nCREATE TEMPORARY TABLE pizza_recipes_temp AS\nSELECT pizza_id, SUBSTRING_INDEX(SUBSTRING_INDEX(toppings, ',', n), ',', -1) AS topping_id\nFROM pizza_recipes\nJOIN (SELECT 1 AS n\n        UNION SELECT 2\n        UNION SELECT 3\n        UNION SELECT 4\n        UNION SELECT 5\n        UNION SELECT 6\n        UNION SELECT 7\n        UNION SELECT 8\n        UNION SELECT 9\n        UNION SELECT 10\n) AS numbers ON CHAR_LENGTH(toppings) - CHAR_LENGTH(REPLACE(toppings, ',', '')) &gt;= n - 1\nORDER BY pizza_id;\nOutput:\n\n\n\npizza_id\ntopping_id\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n1\n4\n\n\n1\n5\n\n\n1\n6\n\n\n1\n8\n\n\n1\n10\n\n\n2\n4\n\n\n2\n6\n\n\n2\n7\n\n\n2\n9\n\n\n2\n11\n\n\n2\n12\n\n\n\nGenerating a unique row number to identify each record\nALTER TABLE customer_orders_temp\nADD COLUMN record_id INT AUTO_INCREMENT PRIMARY KEY;\nBreaking the Extras Column in Customer_Orders_Temp Table\nAssuming your original table is named ‘customer_orders_temp’ and the column is ’extras. Create a temporary table for the exploded extras using a subquery\nDropping the extrasBreak, extrasBreak_ tables if already exists otherwise creating them.\nDROP TABLE IF EXISTS extrasBreak, extrasBreak_\n\nCREATE TEMPORARY TABLE extrasBreak AS\nSELECT record_id, TRIM(value) AS extra_id\nFROM ( SELECT record_id,\n        TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n.digit + 1), ',', -1)) AS value\n    FROM customer_orders_temp\n    LEFT JOIN (\n        SELECT 0 AS digit UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4\n    ) n ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n.digit\n    WHERE TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n.digit + 1), ',', -1)) &lt;&gt; ''\n) AS e;\nAdd rows with null or empty values\nINSERT INTO extrasBreak (record_id, extra_id)\nSELECT record_id, NULL AS extra_id\nFROM customer_orders_temp\nWHERE extras IS NULL OR TRIM(extras) = '';\nCreating a temporary table extrasBreak_\nCREATE TABLE extrasBreak_ AS\nSELECT record_id,\n    CASE WHEN extra_id IS NULL THEN '' ELSE extra_id END AS extra_id\nFROM extrasBreak\nORDER BY record_id, extra_id;\nOutput of extraBreak_ table:\n\n\n\nrecord_id\nextra_id\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n4\n\n\n\n5\n\n\n\n6\n\n\n\n7\n\n\n\n8\n1\n\n\n9\n\n\n\n10\n1\n\n\n11\n\n\n\n12\n1\n\n\n12\n5\n\n\n13\n\n\n\n14\n1\n\n\n14\n4\n\n\n\nBreaking the Exclusion Column in Customer_Orders_Temp Table\nAssuming your original table is named ‘customer_orders_temp’ and the column is ‘exclusions’. Create a temporary table for the exploded exclusions using a subquery\nDropping the exclusionsBreak, exclusionsBreak_ tables if already exists otherwise creating them.\nDROP TABLE IF EXISTS exclusionsBreak, exclusionsBreak_\n\nCREATE TEMPORARY TABLE exclusionsBreak AS\nSELECT record_id, TRIM(value) AS exclusions_id\nFROM ( SELECT record_id,\n        TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n.digit + 1), ',', -1)) AS value\n    FROM customer_orders_temp\n    LEFT JOIN (\n        SELECT 0 AS digit UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4\n    ) n ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n.digit\n    WHERE TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n.digit + 1), ',', -1)) &lt;&gt; ''\n) AS e;\nAdd rows with null or empty values\nINSERT INTO exclusionsBreak (record_id, exclusions_id)\nSELECT record_id, NULL AS exclusions_id\nFROM customer_orders_temp\nWHERE exclusions IS NULL OR TRIM(exclusions) = '';\nCreating a temporary table exclusionsBreak_\nCREATE TABLE exclusionsBreak_ AS\nSELECT record_id,\n    CASE WHEN exclusions_id IS NULL THEN '' ELSE exclusions_id END AS exclusions_id\nFROM exclusionsBreak\nORDER BY record_id, exclusions_id;\nOutput of exclusionsBreak_ table:\n\n\n\nrecord_id\nexclusions_id\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n4\n\n\n\n5\n4\n\n\n6\n4\n\n\n7\n4\n\n\n8\n\n\n\n9\n\n\n\n10\n\n\n\n11\n\n\n\n12\n4\n\n\n13\n\n\n\n14\n2\n\n\n14\n6\n\n\n\n\n\nSELECT\n    pizza_names.pizza_id,\n    pizza_names.pizza_name,\n    GROUP_CONCAT(DISTINCT topping_name) AS topping_name_\nFROM pizza_names\nJOIN pizza_recipes_temp ON pizza_names.pizza_id = pizza_recipes.pizza_id\nJOIN pizza_toppings ON pizza_recipes.topping_id = pizza_toppings.topping_id\nGROUP BY pizza_names.pizza_id,pizza_names.pizza_name\nORDER BY pizza_names.pizza_name;\nOutput:\n\n\n\n\n\n\n\n\npizza_id\npizza_name\ntopping_name\n\n\n\n\n1\nMeatlovers\nBacon, BBQ Sauce, Beef, Cheese, Chicken, Mushrooms, Pepperoni, Salami\n\n\n2\nVegetarian\nCheese, Mushrooms, Onions, Peppers, Tomato Sauce, Tomatoes\n\n\n\n\n\n\nPizza Ingredients Overview:\nA. Meatlovers Pizza:\n\nStandard ingredients for Meatlovers pizza include Bacon, BBQ Sauce, Beef, Cheese, Chicken, Mushrooms, Pepperoni, and Salami.\nThese ingredients typically cater to customers who prefer meat-based toppings on their pizzas.\n\nB. Vegetarian Pizza:\n\nStandard ingredients for Vegetarian pizza consist of Cheese, Mushrooms, Onions, Peppers, Tomato Sauce, and Tomatoes.\nThis pizza variant is suitable for customers who prefer vegetarian options, offering a mix of flavorful vegetables and cheese.\n\n\n\n\n\n\nWITH cte AS (\n    SELECT order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM customer_orders\nJOIN ( SELECT 1 AS n\n       UNION SELECT 2\n) AS numbers ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n - 1\nWHERE extras IS NOT NULL\n)\nSELECT topping_name, COUNT(order_id) AS most_common_extras\n    FROM cte JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\nGROUP BY topping_name LIMIT 1;\nOutput:\n\n\n\ntopping_name\nmost_common_extras\n\n\n\n\nBacon\n4\n\n\n\n\n\n\nMost Common Extra Topping:\n\nBacon: The data reveals that bacon is the most frequently added extra topping by customers, with a count of 4 orders.\n\n\n\n\n\n\nWITH cte AS (\n    SELECT order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM customer_orders\nJOIN ( SELECT 1 AS n\n       UNION SELECT 2\n) AS numbers ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n - 1\nWHERE exclusions IS NOT NULL\n)\nSELECT topping_name, COUNT(order_id) AS most_common_exclusions\nFROM cte JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\nGROUP BY topping_name LIMIT 1;\nOutput:\n\n\n\ntopping_name\nmost_common_exclusions\n\n\n\n\nCheese\n4\n\n\n\n\n\n\nMost Common Exclusion:\n\nCheese: The analysis reveals that cheese is the most frequently excluded ingredient, with a count of 4 orders.\n\n\n\n\n\n\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nWITH extras_cte AS (\nSELECT record_id, GROUP_CONCAT('Extra ', PT.topping_name) AS option_text\nFROM extrasBreak_ AS EB JOIN pizza_toppings AS PT ON EB.extra_id = PT.topping_id GROUP BY record_id\n),\nexclusions_cte AS (\nSELECT record_id, GROUP_CONCAT('Exclusion ', PT.topping_name) AS option_text\nFROM exclusionsBreak_ AS EB JOIN pizza_toppings AS PT ON EB.exclusions_id = PT.topping_id\nGROUP BY record_id\n),\ncombined_cte AS (\nSELECT * FROM extras_cte UNION SELECT * FROM exclusions_cte\n),\npartial_data_cte AS (\nSELECT CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time,\n    IFNULL(GROUP_CONCAT(PN.pizza_name, ' - ', option_text), '') AS pizza_details\nFROM customer_orders_temp AS CO LEFT JOIN combined_cte AS CC ON CO.record_id = CC.record_id\nJOIN pizza_names AS PN ON PN.pizza_id = CO.pizza_id\nGROUP BY CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time\n)\nSELECT PDC.record_id, PDC.order_id, PDC.customer_id, PDC.pizza_id, PDC.order_time,\n    CASE WHEN PDC.pizza_id = '1' AND pizza_details = '' THEN 'MeatLover'\n         WHEN PDC.pizza_id = '2' AND pizza_details = '' THEN 'Vegetarian' ELSE PDC.pizza_details END AS pizza_detail\nFROM partial_data_cte AS PDC;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\nrecord_id\norder_id\ncustomer_id\npizza_id\norder_time\npizza_detail\n\n\n\n\n1\n1\n101\n1\n2020-01-01 18:05:02\nMeatLover\n\n\n2\n2\n101\n1\n2020-01-01 19:00:52\nMeatLover\n\n\n3\n3\n102\n1\n2020-01-02 23:51:23\nMeatLover\n\n\n4\n3\n102\n2\n2020-01-02 23:51:23\nVegetarian\n\n\n5\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers - Exclusion Cheese\n\n\n6\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers - Exclusion Cheese\n\n\n7\n4\n103\n2\n2020-01-04 13:23:46\nVegetarian - Exclusion Cheese\n\n\n8\n5\n104\n1\n2020-01-08 21:00:29\nMeatlovers - Extra Bacon\n\n\n9\n6\n101\n2\n2020-01-08 21:03:13\nVegetarian\n\n\n10\n7\n105\n2\n2020-01-08 21:20:29\nVegetarian - Extra Bacon\n\n\n11\n8\n102\n1\n2020-01-09 23:54:33\nMeatLover\n\n\n12\n9\n103\n1\n2020-01-10 11:22:59\nMeatlovers - Extra Bacon, Extra Chicken, Meatlovers - Exclusion Cheese\n\n\n13\n10\n104\n1\n2020-01-11 18:34:49\nMeatLover\n\n\n14\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers - Extra Bacon, Extra Cheese, Meatlovers - Exclusion BBQ Sauce, Exclusion Mushrooms\n\n\n\n\n\n\n\nFor example: “Meat Lovers: 2xBacon, Beef, … , Salami”.\n\nWITH pizza_ingredients AS (\n    SELECT CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time, PN.pizza_name,\n      CASE WHEN PT.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id)\n      THEN CONCAT('2x ', PT.topping_name) ELSE PT.topping_name END AS ingredients_used\n    FROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR ON CO.pizza_id = PR.pizza_id\n    JOIN pizza_toppings AS PT ON PT.topping_id = PR.topping_id\n    JOIN pizza_names AS PN ON PN.pizza_id = CO.pizza_id\n    WHERE PR.topping_id NOT IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id)\n)\nSELECT PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time,CONCAT(PI.pizza_name, ': ',\n        GROUP_CONCAT(ingredients_used ORDER BY ingredients_used)) AS ingredients_used\nFROM pizza_ingredients AS PI\nGROUP BY PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time, PI.pizza_name\nORDER BY PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\nrecord_id\norder_id\ncustomer_id\npizza_id\norder_time\ningredients_used\n\n\n\n\n1\n1\n101\n1\n2020-01-01 18:05:02\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n2\n2\n101\n1\n2020-01-01 19:00:52\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n3\n3\n102\n1\n2020-01-02 23:51:23\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n4\n3\n102\n2\n2020-01-02 23:51:23\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n5\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n6\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n7\n4\n103\n2\n2020-01-04 13:23:46\nVegetarian: Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n8\n5\n104\n1\n2020-01-08 21:00:29\nMeatlovers: 2x Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n9\n6\n101\n2\n2020-01-08 21:03:13\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n10\n7\n105\n2\n2020-01-08 21:20:29\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n11\n8\n102\n1\n2020-01-09 23:54:33\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n12\n9\n103\n1\n2020-01-10 11:22:59\nMeatlovers: 2x Bacon,2x Chicken,BBQ Sauce,Beef,Cheese,Mushrooms,Pepperoni,Salami\n\n\n13\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n14\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers: 2x Bacon,2x Cheese,BBQ Sauce,Beef,Chicken,Mushrooms,Pepperoni,Salami\n\n\n\n\n\n\nWITH pizza_ingredients AS (\nSELECT CO.order_id, CO.customer_id, PT.topping_name,\n    CASE WHEN PR.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id) THEN 2\n    WHEN PR.topping_id IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id) THEN 0\n    ELSE 1 END AS ingredients_used\nFROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR\nON CO.pizza_id = PR.pizza_id JOIN pizza_toppings AS PT\nON PT.topping_id = PR.topping_id\n)\nSELECT PI.topping_name, SUM(ingredients_used) AS qty_used_of_each_ingredients FROM pizza_ingredients AS PI\nGROUP BY PI.topping_name\nORDER BY PI.topping_name;\nOutput:\n\n\n\ntopping_name\nqty_used_of_each_ingredients\n\n\n\n\nBacon\n13\n\n\nBBQ Sauce\n10\n\n\nBeef\n10\n\n\nCheese\n13\n\n\nChicken\n10\n\n\nMushrooms\n14\n\n\nOnions\n4\n\n\nPepperoni\n10\n\n\nPeppers\n4\n\n\nSalami\n10\n\n\nTomato Sauce\n4\n\n\nTomatoes\n4\n\n\n\n\n\n\nIngredient Usage Overview:\n\nMushrooms: Mushrooms emerge as the most frequently used ingredient, with a total quantity of 14 units across all delivered pizzas.\nBacon, Cheese: Bacon and cheese follow closely behind, with both ingredients utilized in 13 units of pizzas each.\nBBQ Sauce, Beef, Chicken, Pepperoni, Salami: These ingredients are also commonly used, each appearing in 10 units of pizzas.\nOnions, Peppers, Tomato Sauce, Tomatoes: These ingredients have a relatively lower usage frequency, each appearing in 4 units of pizzas.\n\n\n\n\n\n\n\n\n\nSELECT\n    SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) AS Total_revenue\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN pizza_names AS PN ON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0;\nOutput: | Total_revenue | |—————| | 138 |\n\n\n\nRevenue Overview:\n\nTotal Revenue: $138\n\nRevenue Contribution:\n\nMeat Lovers pizzas, priced at $12 each, contributed significantly to Pizza Runner’s revenue, generating a substantial portion of the total revenue.\n\nPricing Strategy:\n\nThe pricing strategy of offering Meat Lovers and Vegetarian pizzas at $12 and $10, respectively, appears to align with customer preferences and market demand. The differential pricing reflects the perceived value of the pizzas and encourages customers to opt for higher-priced options.\n\n\n\n\n\n\nSELECT\n    SUM(\n        CASE \n        WHEN pizza_name = 'Meatlovers' AND extra_one_dollar_charge = '1' THEN (12 + 1)\n        WHEN pizza_name = 'Meatlovers' AND extra_one_dollar_charge = '2' THEN (12 + 2)\n        WHEN pizza_name = 'Vegetarian' AND extra_one_dollar_charge = '1' THEN (10 + 1)\n        WHEN pizza_name = 'Vegetarian' AND extra_one_dollar_charge = '2' THEN (10 + 2)\n        END) AS Total_revenue\nFROM (SELECT \n        CO.order_id, CO.customer_id, CO.pizza_id, RO.distance,\n        PN.pizza_name, \n        (CASE WHEN extras LIKE '%4%' THEN 2 ELSE 1 END) AS extra_one_dollar_charge\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO\nON CO.order_id = RO.order_id\nJOIN pizza_names AS PN\nON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0) AS temp_;\nOutput:\n\n\n\nTotal_revenue\n\n\n\n\n151\n\n\n\n\n\n\nRevenue Overview::\n\nTotal Revenue with Extra Charge: $151\n\nImpact of Extra Charge:\n\nIntroducing an additional $1 charge for pizza extras, particularly for adding cheese, resulted in a slight increase in total revenue compared to the previous scenario.\n\n\n\n\n\n\nDROP TABLE IF EXISTS ratings;\nCREATE TABLE ratings (\n    order_id INT,\n    ratings INT\n);\nINSERT INTO ratings (order_id, ratings)\n-- Inserting some random ratings\nVALUES (1,4),\n       (2,3),\n       (3,4),\n       (4,1),\n       (5,5),\n       (7,2),\n       (8,4),\n       (10,3);\nSELECT * FROM ratings;\nOutput:\n\n\n\norder_id\nratings\n\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n4\n\n\n4\n1\n\n\n5\n5\n\n\n7\n2\n\n\n8\n4\n\n\n10\n3\n\n\n\n\n\n\n- customer_id, order_id, runner_id, rating, order_time, pickup_time, Time between order and pickup, Delivery duration, Average speed, Total number of pizzas.\nSELECT\n    CO.customer_id, CO.order_id, \n    RO.runner_id, R.ratings, \n    CO.order_time, RO.duration,\n    ROUND(TIME_TO_SEC(TIMEDIFF(RO.pickup_time, CO.order_time))/60,0) AS time_between_order_and_pickup_in_minutes,\n    ROUND(AVG(RO.distance/(RO.duration/60)),2) AS average_speed_in_kmph,\n    COUNT(CO.order_id) AS pizza_count\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN ratings AS R ON RO.order_id = R.order_id\nWHERE RO.distance != 0\nGROUP BY CO.customer_id, CO.order_id, RO.runner_id, R.ratings, CO.order_time,\n    RO.duration, time_between_order_and_pickup_in_minutes;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\norder_id\nrunner_id\nratings\norder_time\ndistance\nduration\ntime_between_order_and_pickup_in_minutes\naverage_speed_in_kmph\npizza_count\n\n\n\n\n101\n1\n1\n4\n2020-01-01 18:05:02\n32\n11\n37.5\n1\n1\n\n\n101\n2\n1\n3\n2020-01-01 19:00:52\n27\n10\n44.44\n1\n1\n\n\n102\n3\n1\n4\n2020-01-02 23:51:23\n20\n21\n40.2\n2\n2\n\n\n103\n4\n2\n1\n2020-01-04 13:23:46\n40\n29\n35.1\n3\n3\n\n\n104\n5\n3\n5\n2020-01-08 21:00:29\n15\n10\n40\n1\n1\n\n\n105\n7\n2\n2\n2020-01-08 21:20:29\n25\n10\n60\n1\n1\n\n\n102\n8\n2\n4\n2020-01-09 23:54:33\n15\n20\n93.6\n1\n1\n\n\n104\n10\n1\n3\n2020-01-11 18:34:49\n10\n16\n60\n2\n2\n\n\n\n\n\n\nSELECT\n    SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) AS Total_revenue,\n    ROUND(SUM(RO.distance) * 0.3, 2) AS runner_earned_amount,\n    (SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) - ROUND(SUM(RO.distance) * 0.3, 2))\n        AS Profit_left_after_paying_to_runners\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN pizza_names AS PN ON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0;\nOutput:\n\n\n\n\n\n\n\n\nTotal_revenue\nrunner_earned_amount\nProfit_left_after_paying_to_runners\n\n\n\n\n138\n64.62\n73.38\n\n\n\n\n\n\nRevenue Generation:\n\nThe total revenue generated from delivering pizzas, considering fixed prices for Meat Lovers and Vegetarian pizzas, amounts to $138.\n\nRunner Expenses:\n\nRunners are compensated at a rate of $0.30 per kilometer traveled. The total amount paid to runners for delivering pizzas is $64.62.\n\nProfit Analysis:\n\nAfter deducting the expenses incurred for paying runners from the total revenue generated, Pizza Runner has a profit of $73.38 left over from these deliveries.\n\n\n\n\n\n\nWITH pizza_ingredients AS (\nSELECT CO.order_id, CO.customer_id, PT.topping_name,\n    CASE WHEN PR.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id) THEN 2\n    WHEN PR.topping_id IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id) THEN 0\n    ELSE 1 END AS ingredients_used\nFROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR\nON CO.pizza_id = PR.pizza_id JOIN pizza_toppings AS PT\nON PT.topping_id = PR.topping_id\n)\nSELECT PI.topping_name, SUM(ingredients_used) AS qty_used_of_each_ingredients FROM pizza_ingredients AS PI\nGROUP BY PI.topping_name\nORDER BY PI.topping_name;\nOutput:\n\n\n\ntopping_name\nqty_used_of_each_ingredients\n\n\n\n\nBacon\n13\n\n\nBBQ Sauce\n10\n\n\nBeef\n10\n\n\nCheese\n13\n\n\nChicken\n10\n\n\nMushrooms\n14\n\n\nOnions\n4\n\n\nPepperoni\n10\n\n\nPeppers\n4\n\n\nSalami\n10\n\n\nTomato Sauce\n4\n\n\nTomatoes\n4\n\n\n\n\n\n\nTop Ingredients:\n\nMushrooms: With a total quantity of 14 units, mushrooms emerge as the most commonly used ingredient in delivered pizzas.\nBacon and Cheese: Bacon and cheese tie for the second position, each with a total quantity of 13 units.\nBBQ Sauce, Beef, Chicken, Pepperoni, and Salami: These ingredients have a total quantity of 10 units each, indicating their popularity among the pizza toppings.\nOnions, Peppers, Tomato Sauce, and Tomatoes: These ingredients have a relatively lower total quantity, each appearing four times in the delivered pizzas.\n\n\n\n\n\n\n\n\n\nINSERT INTO pizza_names (pizza_id, pizza_name)\nVALUES (3, 'Supreme');\n\nALTER TABLE pizza_recipes\nMODIFY COLUMN toppings VARCHAR(50);\n\nINSERT INTO pizza_recipes (pizza_id, toppings)\nVALUES (3, '1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12');\nOutput:\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n3\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#introduction",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#introduction",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "Did you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)\nDanny was scrolling through his Instagram feed when something really caught his eye - “80s Retro Styling and Pizza Is The Future!”\nDanny was sold on the idea, but he knew that pizza alone was not going to help him get seed funding to expand his new Pizza Empire - so he had one more genius idea to combine with it - he was going to Uberize it - and so Pizza Runner was launched!\nDanny started by recruiting “runners” to deliver fresh pizza from Pizza Runner Headquarters (otherwise known as Danny’s house) and also maxed out his credit card to pay freelance developers to build a mobile app to accept orders from customers."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#available-data",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#available-data",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "Because Danny had a few years of experience as a data scientist - he was very aware that data collection was going to be critical for his business’ growth.\nHe has prepared for us an entity relationship diagram of his database design but requires further assistance to clean his data and apply some basic calculations so he can better direct his runners and optimise Pizza Runner’s operations.\nAll datasets exist within the pizza_runner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#entity-relationship-diagram",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#entity-relationship-diagram",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "The runners table shows the registration_date for each new runner\n\n\n\nrunner_id\nregistration_date\n\n\n\n\n1\n2021-01-01\n\n\n2\n2021-01-03\n\n\n3\n2021-01-08\n\n\n4\n2021-01-15\n\n\n\n\n\n\nCustomer pizza orders are captured in the customer_orders table with 1 row for each individual pizza that is part of the order.\nThe pizza_id relates to the type of pizza which was ordered whilst the exclusions are the ingredient_id values which should be removed from the pizza and the extras are the ingredient_id values which need to be added to the pizza.\nNote that customers can order multiple pizzas in a single order with varying exclusions and extras values even if the pizza is the same type!\nThe exclusions and extras columns will need to be cleaned up before using them in your queries.\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2021-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2021-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2021-01-02 23:51:23\n\n\n3\n102\n2\nNaN\n\n2021-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2021-01-04 13:23:46\n\n\n5\n104\n1\nnull\n1\n2021-01-08 21:00:29\n\n\n6\n101\n2\nnull\nnull\n2021-01-08 21:03:13\n\n\n7\n105\n2\nnull\n1\n2021-01-08 21:20:29\n\n\n8\n102\n1\nnull\nnull\n2021-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2021-01-10 11:22:59\n\n\n10\n104\n1\nnull\nnull\n2021-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2021-01-11 18:34:49\n\n\n\n\n\n\nAfter each orders are received through the system - they are assigned to a runner - however not all orders are fully completed and can be cancelled by the restaurant or the customer.\nThe pickup_time is the timestamp at which the runner arrives at the Pizza Runner headquarters to pick up the freshly cooked pizzas. The distance and duration fields are related to how far and long the runner had to travel to deliver the order to the respective customer.\nThere are some known data issues with this table so be careful when using this in your queries - make sure to check the data types for each column in the schema SQL!\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2021-01-01 18:15:34\n20km\n32 minutes\n\n\n\n2\n1\n2021-01-01 19:10:54\n20km\n27 minutes\n\n\n\n3\n1\n2021-01-03 00:12:37\n13.4km\n20 mins\nNaN\n\n\n4\n2\n2021-01-04 13:53:03\n23.4\n40\nNaN\n\n\n5\n3\n2021-01-08 21:10:57\n10\n15\nNaN\n\n\n6\n3\nnull\nnull\nnull\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25km\n25mins\nnull\n\n\n8\n2\n2020-01-10 00:15:02\n23.4 km\n15 minute\nnull\n\n\n9\n2\nnull\nnull\nnull\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10 minutes\nnull\n\n\n\n\n\n\nAt the moment - Pizza Runner only has 2 pizzas available the Meat Lovers or Vegetarian!\n\n\n\npizza_id\npizza_name\n\n\n\n\n1\nMeat Lovers\n\n\n2\nVegetarian\n\n\n\n\n\n\nEach pizza_id has a standard set of toppings which are used as part of the pizza recipe.\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n\n\n\n\nThis table contains all of the topping_name values with their corresponding topping_id value\n\n\n\ntopping_id\ntopping_name\n\n\n\n\n1\nBacon\n\n\n2\nBBQ Sauce\n\n\n3\nBeef\n\n\n4\nCheese\n\n\n5\nChicken\n\n\n6\nMushrooms\n\n\n7\nOnions\n\n\n8\nPepperoni\n\n\n9\nPeppers\n\n\n10\nSalami\n\n\n11\nTomatoes\n\n\n12\nTomato Sauce"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#interactive-sql-session",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#interactive-sql-session",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\nCREATE SCHEMA pizza_runner;\nUSE pizza_runner;\n\nDROP TABLE IF EXISTS runners;\nCREATE TABLE runners (\n  `runner_id` INTEGER,\n  `registration_date` DATE\n);\nINSERT INTO runners\n  (`runner_id`, `registration_date`)\nVALUES\n  (1, '2021-01-01'),\n  (2, '2021-01-03'),\n  (3, '2021-01-08'),\n  (4, '2021-01-15');\n\nDROP TABLE IF EXISTS customer_orders;\nCREATE TABLE customer_orders (\n  `order_id` INTEGER,\n  `customer_id` INTEGER,\n  `pizza_id` INTEGER,\n  `exclusions` VARCHAR(4),\n  `extras` VARCHAR(4),\n  `order_time` TIMESTAMP\n);\n\nINSERT INTO customer_orders\n  (`order_id`, `customer_id`, `pizza_id`, `exclusions`, `extras`, `order_time`)\nVALUES\n  ('1', '101', '1', '', '', '2020-01-01 18:05:02'),\n  ('2', '101', '1', '', '', '2020-01-01 19:00:52'),\n  ('3', '102', '1', '', '', '2020-01-02 23:51:23'),\n  ('3', '102', '2', '', NULL, '2020-01-02 23:51:23'),\n  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n  ('4', '103', '1', '4', '', '2020-01-04 13:23:46'),\n  ('4', '103', '2', '4', '', '2020-01-04 13:23:46'),\n  ('5', '104', '1', 'null', '1', '2020-01-08 21:00:29'),\n  ('6', '101', '2', 'null', 'null', '2020-01-08 21:03:13'),\n  ('7', '105', '2', 'null', '1', '2020-01-08 21:20:29'),\n  ('8', '102', '1', 'null', 'null', '2020-01-09 23:54:33'),\n  ('9', '103', '1', '4', '1, 5', '2020-01-10 11:22:59'),\n  ('10', '104', '1', 'null', 'null', '2020-01-11 18:34:49'),\n  ('10', '104', '1', '2, 6', '1, 4', '2020-01-11 18:34:49');\n\nDROP TABLE IF EXISTS runner_orders;\nCREATE TABLE runner_orders (\n  `order_id` INTEGER,\n  `runner_id` INTEGER,\n  `pickup_time` VARCHAR(19),\n  `distance` VARCHAR(7),\n  `duration` VARCHAR(10),\n  `cancellation` VARCHAR(23)\n);\n\nINSERT INTO runner_orders\n  (`order_id`, `runner_id`, `pickup_time`, `distance`, `duration`, `cancellation`)\nVALUES\n  ('1', '1', '2020-01-01 18:15:34', '20km', '32 minutes', ''),\n  ('2', '1', '2020-01-01 19:10:54', '20km', '27 minutes', ''),\n  ('3', '1', '2020-01-03 00:12:37', '13.4km', '20 mins', NULL),\n  ('4', '2', '2020-01-04 13:53:03', '23.4', '40', NULL),\n  ('5', '3', '2020-01-08 21:10:57', '10', '15', NULL),\n  ('6', '3', 'null', 'null', 'null', 'Restaurant Cancellation'),\n  ('7', '2', '2020-01-08 21:30:45', '25km', '25mins', 'null'),\n  ('8', '2', '2020-01-10 00:15:02', '23.4 km', '15 minute', 'null'),\n  ('9', '2', 'null', 'null', 'null', 'Customer Cancellation'),\n  ('10', '1', '2020-01-11 18:50:20', '10km', '10minutes', 'null');\n\nDROP TABLE IF EXISTS pizza_names;\nCREATE TABLE pizza_names (\n  `pizza_id` INTEGER,\n  `pizza_name` TEXT\n);\nINSERT INTO pizza_names\n  (`pizza_id`, `pizza_name`)\nVALUES\n  (1, 'Meatlovers'),\n  (2, 'Vegetarian');\n\nDROP TABLE IF EXISTS pizza_recipes;\nCREATE TABLE pizza_recipes (\n  `pizza_id` INTEGER,\n  `toppings` TEXT\n);\nINSERT INTO pizza_recipes\n  (`pizza_id`, `toppings`)\nVALUES\n  (1, '1, 2, 3, 4, 5, 6, 8, 10'),\n  (2, '4, 6, 7, 9, 11, 12');\n\nDROP TABLE IF EXISTS pizza_toppings;\nCREATE TABLE pizza_toppings (\n  `topping_id` INTEGER,\n  `topping_name` TEXT\n);\nINSERT INTO pizza_toppings\n  (`topping_id`, `topping_name`)\nVALUES\n  (1, 'Bacon'),\n  (2, 'BBQ Sauce'),\n  (3, 'Beef'),\n  (4, 'Cheese'),\n  (5, 'Chicken'),\n  (6, 'Mushrooms'),\n  (7, 'Onions'),\n  (8, 'Pepperoni'),\n  (9, 'Peppers'),\n  (10, 'Salami'),\n  (11, 'Tomatoes'),\n  (12, 'Tomato Sauce');"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#case-study-questions",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#case-study-questions",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "This case study has LOTS of questions - they are broken up by area of focus including:\n\nPizza Metrics\nRunner and Customer Experience\nIngredient Optimisation\nPricing and Ratings\nBonus DML Challenges (DML = Data Manipulation Language)\n\nEach of the following case study questions can be answered using a single SQL statement.\nAgain, there are many questions in this case study - please feel free to pick and choose which ones you’d like to try!\nBefore you start writing your SQL queries however - you might want to investigate the data, you may want to do something with some of those null values and data types in the customer_orders and runner_orders tables!\n\n\n\nHow many pizzas were ordered?\nHow many unique customer orders were made?\nHow many successful orders were delivered by each runner?\nHow many of each type of pizza was delivered?\nHow many Vegetarian and Meatlovers were ordered by each customer?\nWhat was the maximum number of pizzas delivered in a single order?\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\nHow many pizzas were delivered that had both exclusions and extras?\nWhat was the total volume of pizzas ordered for each hour of the day?\nWhat was the volume of orders for each day of the week?\n\n\n\n\n\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\nWhat was the average distance travelled for each customer?\nWhat was the difference between the longest and shortest delivery times for all orders?\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\nWhat is the successful delivery percentage for each runner?\n\n\n\n\n\nWhat are the standard ingredients for each pizza?\nWhat was the most commonly added extra?\nWhat was the most common exclusion?\nGenerate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nGenerate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients\n\nFor example: \"Meat Lovers: 2xBacon, Beef, ... , Salami\".\n\nWhat is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\nIf a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\nWhat if there was an additional $1 charge for any pizza extras?\n\nAdd cheese is $1 extra\n\nThe Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\nUsing your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\nIf a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#lets-start-solving-them.",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#lets-start-solving-them.",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "Dropping the customer_orders_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS customer_orders_temp\n\nCREATE TEMPORARY TABLE customer_orders_temp AS\nSELECT order_id, customer_id, pizza_id,\nCASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN ''\nELSE exclusions END AS exclusions,\nCASE WHEN extras IS NULL OR extras LIKE 'null' THEN ''\nELSE extras END AS extras,\norder_time\nFROM customer_orders;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2020-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2020-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2020-01-02 23:51:23\n\n\n3\n102\n2\n\n\n2020-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2020-01-04 13:23:46\n\n\n5\n104\n1\n\n1\n2020-01-08 21:00:29\n\n\n6\n101\n2\n\n\n2020-01-08 21:03:13\n\n\n7\n105\n2\n\n1\n2020-01-08 21:20:29\n\n\n8\n102\n1\n\n\n2020-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2020-01-10 11:22:59\n\n\n10\n104\n1\n\n\n2020-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\nDropping the runner_orders_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS runner_orders_temp\n\nCREATE TEMPORARY TABLE runner_orders_temp AS\nSELECT order_id, runner_id,\n    CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time,\n    CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance)\n    ELSE distance END AS FLOAT) AS distance,\nCAST(CASE WHEN duration LIKE \"null\" THEN NULL\n    WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration)\n    WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration)\n    WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration)\n    ELSE duration END AS FLOAT) AS duration,\n    CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL\n    ELSE cancellation END AS cancellation\nFROM runner_orders;\nChanging the Data Types of columns in runner_orders TABLE\nALTER TABLE runner_orders_temp\nMODIFY COLUMN pickup_time DATETIME,\nMODIFY COLUMN distance FLOAT,\nMODIFY COLUMN duration FLOAT;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2020-01-01 18:15:34\n20\n32\n\n\n\n2\n1\n2020-01-01 19:10:54\n20\n27\n\n\n\n3\n1\n2020-01-03 00:12:37\n13.4\n20\n\n\n\n4\n2\n2020-01-04 13:53:03\n23.4\n40\n\n\n\n5\n3\n2020-01-08 21:10:57\n10\n15\n\n\n\n6\n3\n\n\n\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25\n25\n\n\n\n8\n2\n2020-01-10 00:15:02\n23.4\n15\n\n\n\n9\n2\n\n\n\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10\n10"
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#a.-pizza-metrics-1",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#a.-pizza-metrics-1",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "SELECT COUNT(order_id) AS pizza_cnt\nFROM customer_orders_temp;\nOutput:\n\n\n\npizza_cnt\n\n\n\n\n14\n\n\n\n\n\n\nPizza Order Overview:\n\nA total of 14 pizzas were ordered during the period under consideration.\nUnderstanding the volume of pizza orders helps in assessing demand and operational requirements for Pizza Runner.\n\n\n\n\n\n\nSELECT COUNT(DISTINCT order_id) AS unique_orders\nFROM customer_orders_temp;\nOutput: | unique_orders | |—————| | 10 |\n\n\n\nInsights into Customer Order Frequency:\n\nA total of 10 unique customer orders were made during the specified period.\nUnderstanding the frequency of unique orders provides insights into customer engagement and the overall demand for Pizza Runner’s services.\n\n\n\n\n\n\nSELECT\n    runner_id,\n    COUNT(order_id) AS `successful_delivery`\nFROM runner_orders\nWHERE distance != 0\nGROUP BY runner_id;\nOutput:\n\n\n\nrunner_id\nsuccessful_delivery\n\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n1\n\n\n\n\n\n\nEfficiency in Order Delivery:\n\nRunner 1 completed the highest number of successful deliveries, with a total of 4 orders fulfilled.\nRunner 2 follows closely behind, with 3 successful deliveries.\nRunner 3 completed 1 successful delivery during the specified period.\n\n\n\n\n\n\nSELECT\n    pizza_name,\n    COUNT(runner_orders_temp.order_id) as pizza_cnt\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nJOIN pizza_names ON pizza_names.pizza_id = customer_orders_temp.pizza_id\nWHERE distance != 0\nGROUP BY pizza_name;\nOutput:\n\n\n\npizza_name\npizza_cnt\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\nDistribution of Pizza Types:\n\nMeat Lovers pizza was the most frequently delivered, with a total of 9 orders fulfilled.\nVegetarian pizza accounted for a smaller portion of deliveries, with 3 orders fulfilled.\n\n\n\n\n\n\nSELECT\n    customer_id,\n    pizza_name,\n    COUNT(order_id) as order_cnt\nFROM customer_orders_temp\nJOIN pizza_names ON customer_orders_temp.pizza_id = pizza_names.pizza_id\nGROUP BY customer_id, pizza_name\nORDER BY customer_id, pizza_name;\nOutput:\n\n\n\ncustomer_id\npizza_name\norder_cnt\n\n\n\n\n101\nMeatlovers\n2\n\n\n101\nVegetarian\n1\n\n\n102\nMeatlovers\n2\n\n\n102\nVegetarian\n1\n\n\n103\nMeatlovers\n3\n\n\n103\nVegetarian\n1\n\n\n104\nMeatlovers\n3\n\n\n105\nVegetarian\n1\n\n\n\n\n\n\nCustomer Pizza Preferences:\n\nCustomer 101 ordered 2 Meatlovers pizzas and 1 Vegetarian pizza, indicating a preference for both varieties.\nSimilarly, Customer 102 ordered 2 Meatlovers pizzas and 1 Vegetarian pizza, suggesting a balanced preference for different pizza types.\nCustomer 103 predominantly ordered Meatlovers pizzas, with 3 orders, and also ordered 1 Vegetarian pizza, indicating a preference for meat-based options but also an interest in vegetarian choices.\nCustomer 104 ordered 3 Meatlovers pizzas, indicating a strong preference for this type, while Customer 105 ordered 1 Vegetarian pizza, suggesting a preference for meat-free options.\n\n\n\n\n\n\nSELECT customer_orders_temp.order_id,\n        COUNT(runner_orders_temp.order_id) as pizza_cnt\nFROM customer_orders_temp\nJOIN runner_orders_temp\nON customer_orders_temp.order_id = runner_orders_temp.order_id\nGROUP BY customer_orders_temp.order_id\nORDER BY pizza_cnt DESC;\nOutput:\n\n\n\norder_id\npizza_cnt\n\n\n\n\n4\n3\n\n\n3\n2\n\n\n10\n2\n\n\n1\n1\n\n\n2\n1\n\n\n5\n1\n\n\n6\n1\n\n\n7\n1\n\n\n8\n1\n\n\n9\n1\n\n\n\n\n\n\nOrder Size Overview:\n\nOrder ID 4 recorded the highest number of pizzas delivered in a single order, with 3 pizzas.\nOrders 3 and 10 followed, each consisting of 2 pizzas, indicating moderate order sizes.\nSeveral orders, including IDs 1, 2, 5, 6, 7, 8, and 9, were comprised of a single pizza, representing smaller order sizes.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    SUM(CASE WHEN exclusions &lt;&gt; '' OR extras &lt;&gt; '' THEN 1 ELSE 0 END) AS 'Change',\n    SUM(CASE WHEN exclusions = '' AND extras = '' THEN 1 ELSE 0 END) AS 'No_Change'\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY customer_orders_temp.customer_id;\nOutput:\n\n\n\ncustomer_id\nchange\nno_change\n\n\n\n\n101\n0\n2\n\n\n102\n0\n3\n\n\n103\n3\n0\n\n\n104\n2\n1\n\n\n105\n1\n0\n\n\n\n\n\n\nChange vs. No Change:\n\nCustomer 101 and Customer 102 placed orders without any modifications, indicating a preference for standard pizza options without exclusions or extras.\nCustomer 103 exclusively ordered pizzas with modifications, suggesting a preference for customized or personalized options tailored to specific dietary preferences or taste preferences.\nCustomer 104 had a mix of orders with and without changes, indicating a varied preference for both standard and customized pizza options.\nCustomer 105 ordered pizzas with at least one change, reflecting a preference for personalized pizza options.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    SUM(CASE WHEN exclusions != '' AND extras != '' THEN 1 ELSE 0 END)\n        AS 'pizza_with_exclusions_and_extras'\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.pickup_time IS NOT NULL AND exclusions != '' AND extras != ''\nGROUP BY customer_orders_temp.customer_id\nORDER BY SUM(CASE WHEN exclusions != '' AND extras != '' THEN 1 ELSE 0 END)  DESC;\nOutput:\n\n\n\ncustomer_id\npizza_with_exclusions_and_extras\n\n\n\n\n104\n1\n\n\n\n\n\n\nPizza Customization Trends:\n\nCustomer 104 placed an order that included both exclusions and extras, indicating a preference for a customized pizza with specific modifications to the standard recipe.\n\n\n\n\n\n\nSELECT\n    HOUR(order_time) as 'hour',\n    COUNT(order_id)\nFROM customer_orders_temp\nGROUP BY HOUR(order_time)\nORDER BY HOUR(order_time);\nOutput:\n\n\n\nhour\norder_cnt\n\n\n\n\n11\n1\n\n\n13\n3\n\n\n18\n3\n\n\n19\n1\n\n\n21\n3\n\n\n23\n3\n\n\n\n\n\n\nPeak Ordering Hours:\n\nPizza orders exhibit fluctuations throughout the day, with distinct peaks during specific hours.\nThe busiest hours for pizza orders are observed between 1 PM and 3 PM, with a total volume of 3 orders during each hour.\n\n\n\n\n\n\nSELECT\n    DAYNAME(order_time) as 'day_of_the_week',\n    COUNT(order_id)\nFROM customer_orders_temp\nGROUP BY DAYNAME(order_time)\nORDER BY DAYNAME(order_time) DESC;\nOutput:\n\n\n\nday_of_the_week\norder_cnt\n\n\n\n\nWednesday\n5\n\n\nThursday\n3\n\n\nSaturday\n5\n\n\nFriday\n1\n\n\n\n\n\n\nWeekday vs. Weekend Orders:\n\nPizza orders exhibit variations based on the day of the week, with distinct patterns observed between weekdays and weekends.\nWednesdays and Saturdays emerge as the busiest days for pizza orders, with 5 orders recorded on each day.\nThursdays also demonstrate moderate order volume, with 3 orders placed, indicating consistent demand mid-week.\nFridays recorded the lowest order volume, with only 1 order registered, suggesting a dip in demand at the end of the workweek."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#b.-runner-and-customer-experience-1",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#b.-runner-and-customer-experience-1",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "SELECT\n    WEEK(registration_date) AS 'week',\n    COUNT(runner_id) as num_of_runners\nFROM runners\nGROUP BY WEEK(registration_date);\nOutput:\n\n\n\nweek\nnum_of_runners\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n1\n\n\n\n\n\n\nWeekly Runner Acquisition:\n\nRunner sign-ups fluctuate across different weeks, indicating variations in recruitment efforts and market response over time.\nIn Week 0 (starting from January 1, 2021), 1 runner signed up for Pizza Runner, marking the initial stage of recruitment.\nRunner sign-ups increased in Week 1, with 2 new runners joining the platform, suggesting a positive response to initial marketing and recruitment initiatives.\nHowever, in Week 2, the number of new sign-ups decreased to 1 runner, indicating potential challenges or fluctuations in recruitment effectiveness.\n\n\n\n\n\n\n\nWITH runners_pick_cte AS (\nSELECT runner_id,\n    ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time\nFROM runner_orders_temp\nJOIN customer_orders_temp ON runner_orders_temp.order_id = customer_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY runner_id\n)\nSELECT ROUND(AVG(avg_time),0) AS avg_pick_time\nFROM runners_pick_cte;\nOutput:\n\n\n\navg_pickup_time\n\n\n\n\n16\n\n\n\n\n\n\nAverage Pickup Time:\n\nThe average pickup time for all runners is approximately 16 minutes, indicating the typical duration between order placement and runner arrival at the Pizza Runner HQ.\nThis metric provides insights into the efficiency of runner operations and the responsiveness of the delivery network in fulfilling customer orders promptly.\n\n\n\n\n\n\n\nWITH order_count_cte AS (\nSELECT\n    customer_orders_temp.order_id,\n    COUNT(customer_orders_temp.order_id) AS pizza_order_count,\n    ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time_to_prepare\nFROM runner_orders_temp\nJOIN customer_orders_temp ON runner_orders_temp.order_id = customer_orders_temp.order_id\nWHERE pickup_time IS NOT NULL\nGROUP BY customer_orders_temp.order_id\n)\nSELECT pizza_order_count, ROUND(AVG(avg_time_to_prepare),2) AS avg_time_to_prepare\nFROM order_count_cte\nGROUP BY pizza_order_count;\nOutput:\n\n\n\npizza_order_cnt\navg_time_to_prepare\n\n\n\n\n1\n12.00\n\n\n2\n18.00\n\n\n3\n29.00\n\n\n\n\n\n\nAverage Preparation Time by Pizza Quantity:\n\nOrders consisting of a single pizza have an average preparation time of approximately 12 minutes.\nOrders with two pizzas exhibit a slightly longer average preparation time, averaging around 18 minutes.\nOrders comprising three pizzas demonstrate the longest average preparation time, with an average of 29 minutes.\n\nPotential Relationship:\n\nThe analysis suggests a potential positive correlation between the quantity of pizzas in an order and the time required for preparation.\nAs the number of pizzas in an order increases, the preparation time tends to lengthen, indicating a possible relationship between order complexity and processing duration.\n\n\n\n\n\n\nSELECT\n    customer_orders_temp.customer_id,\n    ROUND(AVG(distance),2) as avg_distance\nFROM customer_orders_temp\nJOIN runner_orders_temp ON customer_orders_temp.order_id = runner_orders_temp.order_id\nWHERE runner_orders_temp.distance != 0\nGROUP BY customer_orders_temp.customer_id;\nOutput:\n\n\n\ncustomer_id\navg_distance\n\n\n\n\n101\n20\n\n\n102\n16.73\n\n\n103\n23.4\n\n\n104\n10\n\n\n105\n25\n\n\n\n\n\n\nAverage Distance Travelled:\n\nCustomer 101 had an average delivery distance of 20 kilometers, indicating a moderate travel distance per order.\nCustomer 102’s average delivery distance was approximately 16.73 kilometers, suggesting a slightly shorter travel distance compared to Customer 101.\nCustomer 103 had the longest average delivery distance at 23.4 kilometers, indicating a greater geographical spread of delivery locations.\nCustomer 104 had a relatively shorter average delivery distance of 10 kilometers, suggesting closer proximity to Pizza Runner HQ or a more localized customer base.\nCustomer 105 had the highest average delivery distance of 25 kilometers, indicating deliveries to more distant locations or potentially serving customers across a wider geographic area.\n\n\n\n\n\n\nSELECT (MAX(duration) - MIN(duration)) AS diff\nFROM runner_orders_temp;\nOutput:\n\n\n\ndiff\n\n\n\n\n30\n\n\n\n\n\n\nDelivery Time Range:\n\nThe difference between the longest and shortest delivery times for all orders was 30 minutes.\nThis variability indicates fluctuations in delivery durations across different orders, reflecting diverse factors such as distance, traffic conditions, and order complexity.\n\n\n\n\n\n\nSELECT\n    RO.runner_id, RO.order_id,\n    ROUND(AVG(RO.distance/(RO.duration/60)),2) AS average_speed_in_kmph\nFROM runner_orders AS RO\nJOIN customer_orders AS CO ON RO.order_id = CO.order_id\nWHERE RO.distance != 0\nGROUP BY RO.runner_id, RO.order_id\nORDER BY RO.runner_id, RO.order_id ASC;\nOutput:\n\n\n\nrunner_id\norder_id\naverage_speed_in_kmph\n\n\n\n\n1\n1\n37.5\n\n\n1\n2\n44.44\n\n\n1\n3\n40.2\n\n\n1\n10\n60\n\n\n2\n4\n35.1\n\n\n2\n7\n60\n\n\n2\n8\n93.6\n\n\n3\n5\n40\n\n\n\n\n\n\nRunner-Specific Speed Variability:\n\nThe average speed for each runner varied across different deliveries, reflecting differences in route distance, traffic conditions, and delivery durations.\nRunner 1 demonstrated varying speeds across deliveries, with average speeds ranging from 37.5 km/h to 60 km/h.\nRunner 2 exhibited notable speed disparities, with average speeds ranging from 35.1 km/h to 93.6 km/h.\nRunner 3 maintained a relatively consistent average speed of 40 km/h across deliveries.\n\n\n\n\n\n\nSELECT runner_id,\n(ROUND(SUM(CASE WHEN duration IS NOT NULL THEN 1 ELSE 0 END)/COUNT(runner_id),2) * 100) AS percentage\nFROM runner_orders_temp\nGROUP BY runner_id;\nOutput:\n\n\n\nrunner_id\npercentage\n\n\n\n\n1\n100.00\n\n\n2\n75.00\n\n\n3\n50.00\n\n\n\n\n\n\nRunner-Specific Success Rates:\n\nRunner 1 achieved a perfect delivery success rate, with 100% of their deliveries completed successfully. This indicates consistent performance and reliability in fulfilling delivery commitments.\nRunner 2 achieved a delivery success rate of 75%, indicating that 3 out of 4 deliveries were completed successfully. While the success rate is relatively high, there is room for improvement to enhance consistency and reliability.\nRunner 3 achieved a delivery success rate of 50%, indicating that half of their deliveries were completed successfully. This suggests potential challenges or inconsistencies in delivery execution that may require attention."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#data-cleaning-pizza_recipes-table",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#data-cleaning-pizza_recipes-table",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "Dropping the pizza_recipes_temp table if already exists otherwise creating it.\nDROP TABLE IF EXISTS pizza_recipes_temp\n\nCREATE TEMPORARY TABLE pizza_recipes_temp AS\nSELECT pizza_id, SUBSTRING_INDEX(SUBSTRING_INDEX(toppings, ',', n), ',', -1) AS topping_id\nFROM pizza_recipes\nJOIN (SELECT 1 AS n\n        UNION SELECT 2\n        UNION SELECT 3\n        UNION SELECT 4\n        UNION SELECT 5\n        UNION SELECT 6\n        UNION SELECT 7\n        UNION SELECT 8\n        UNION SELECT 9\n        UNION SELECT 10\n) AS numbers ON CHAR_LENGTH(toppings) - CHAR_LENGTH(REPLACE(toppings, ',', '')) &gt;= n - 1\nORDER BY pizza_id;\nOutput:\n\n\n\npizza_id\ntopping_id\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n1\n4\n\n\n1\n5\n\n\n1\n6\n\n\n1\n8\n\n\n1\n10\n\n\n2\n4\n\n\n2\n6\n\n\n2\n7\n\n\n2\n9\n\n\n2\n11\n\n\n2\n12\n\n\n\nGenerating a unique row number to identify each record\nALTER TABLE customer_orders_temp\nADD COLUMN record_id INT AUTO_INCREMENT PRIMARY KEY;\nBreaking the Extras Column in Customer_Orders_Temp Table\nAssuming your original table is named ‘customer_orders_temp’ and the column is ’extras. Create a temporary table for the exploded extras using a subquery\nDropping the extrasBreak, extrasBreak_ tables if already exists otherwise creating them.\nDROP TABLE IF EXISTS extrasBreak, extrasBreak_\n\nCREATE TEMPORARY TABLE extrasBreak AS\nSELECT record_id, TRIM(value) AS extra_id\nFROM ( SELECT record_id,\n        TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n.digit + 1), ',', -1)) AS value\n    FROM customer_orders_temp\n    LEFT JOIN (\n        SELECT 0 AS digit UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4\n    ) n ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n.digit\n    WHERE TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n.digit + 1), ',', -1)) &lt;&gt; ''\n) AS e;\nAdd rows with null or empty values\nINSERT INTO extrasBreak (record_id, extra_id)\nSELECT record_id, NULL AS extra_id\nFROM customer_orders_temp\nWHERE extras IS NULL OR TRIM(extras) = '';\nCreating a temporary table extrasBreak_\nCREATE TABLE extrasBreak_ AS\nSELECT record_id,\n    CASE WHEN extra_id IS NULL THEN '' ELSE extra_id END AS extra_id\nFROM extrasBreak\nORDER BY record_id, extra_id;\nOutput of extraBreak_ table:\n\n\n\nrecord_id\nextra_id\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n4\n\n\n\n5\n\n\n\n6\n\n\n\n7\n\n\n\n8\n1\n\n\n9\n\n\n\n10\n1\n\n\n11\n\n\n\n12\n1\n\n\n12\n5\n\n\n13\n\n\n\n14\n1\n\n\n14\n4\n\n\n\nBreaking the Exclusion Column in Customer_Orders_Temp Table\nAssuming your original table is named ‘customer_orders_temp’ and the column is ‘exclusions’. Create a temporary table for the exploded exclusions using a subquery\nDropping the exclusionsBreak, exclusionsBreak_ tables if already exists otherwise creating them.\nDROP TABLE IF EXISTS exclusionsBreak, exclusionsBreak_\n\nCREATE TEMPORARY TABLE exclusionsBreak AS\nSELECT record_id, TRIM(value) AS exclusions_id\nFROM ( SELECT record_id,\n        TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n.digit + 1), ',', -1)) AS value\n    FROM customer_orders_temp\n    LEFT JOIN (\n        SELECT 0 AS digit UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4\n    ) n ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n.digit\n    WHERE TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n.digit + 1), ',', -1)) &lt;&gt; ''\n) AS e;\nAdd rows with null or empty values\nINSERT INTO exclusionsBreak (record_id, exclusions_id)\nSELECT record_id, NULL AS exclusions_id\nFROM customer_orders_temp\nWHERE exclusions IS NULL OR TRIM(exclusions) = '';\nCreating a temporary table exclusionsBreak_\nCREATE TABLE exclusionsBreak_ AS\nSELECT record_id,\n    CASE WHEN exclusions_id IS NULL THEN '' ELSE exclusions_id END AS exclusions_id\nFROM exclusionsBreak\nORDER BY record_id, exclusions_id;\nOutput of exclusionsBreak_ table:\n\n\n\nrecord_id\nexclusions_id\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n4\n\n\n\n5\n4\n\n\n6\n4\n\n\n7\n4\n\n\n8\n\n\n\n9\n\n\n\n10\n\n\n\n11\n\n\n\n12\n4\n\n\n13\n\n\n\n14\n2\n\n\n14\n6\n\n\n\n\n\nSELECT\n    pizza_names.pizza_id,\n    pizza_names.pizza_name,\n    GROUP_CONCAT(DISTINCT topping_name) AS topping_name_\nFROM pizza_names\nJOIN pizza_recipes_temp ON pizza_names.pizza_id = pizza_recipes.pizza_id\nJOIN pizza_toppings ON pizza_recipes.topping_id = pizza_toppings.topping_id\nGROUP BY pizza_names.pizza_id,pizza_names.pizza_name\nORDER BY pizza_names.pizza_name;\nOutput:\n\n\n\n\n\n\n\n\npizza_id\npizza_name\ntopping_name\n\n\n\n\n1\nMeatlovers\nBacon, BBQ Sauce, Beef, Cheese, Chicken, Mushrooms, Pepperoni, Salami\n\n\n2\nVegetarian\nCheese, Mushrooms, Onions, Peppers, Tomato Sauce, Tomatoes\n\n\n\n\n\n\nPizza Ingredients Overview:\nA. Meatlovers Pizza:\n\nStandard ingredients for Meatlovers pizza include Bacon, BBQ Sauce, Beef, Cheese, Chicken, Mushrooms, Pepperoni, and Salami.\nThese ingredients typically cater to customers who prefer meat-based toppings on their pizzas.\n\nB. Vegetarian Pizza:\n\nStandard ingredients for Vegetarian pizza consist of Cheese, Mushrooms, Onions, Peppers, Tomato Sauce, and Tomatoes.\nThis pizza variant is suitable for customers who prefer vegetarian options, offering a mix of flavorful vegetables and cheese.\n\n\n\n\n\n\nWITH cte AS (\n    SELECT order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM customer_orders\nJOIN ( SELECT 1 AS n\n       UNION SELECT 2\n) AS numbers ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n - 1\nWHERE extras IS NOT NULL\n)\nSELECT topping_name, COUNT(order_id) AS most_common_extras\n    FROM cte JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\nGROUP BY topping_name LIMIT 1;\nOutput:\n\n\n\ntopping_name\nmost_common_extras\n\n\n\n\nBacon\n4\n\n\n\n\n\n\nMost Common Extra Topping:\n\nBacon: The data reveals that bacon is the most frequently added extra topping by customers, with a count of 4 orders.\n\n\n\n\n\n\nWITH cte AS (\n    SELECT order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM customer_orders\nJOIN ( SELECT 1 AS n\n       UNION SELECT 2\n) AS numbers ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n - 1\nWHERE exclusions IS NOT NULL\n)\nSELECT topping_name, COUNT(order_id) AS most_common_exclusions\nFROM cte JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\nGROUP BY topping_name LIMIT 1;\nOutput:\n\n\n\ntopping_name\nmost_common_exclusions\n\n\n\n\nCheese\n4\n\n\n\n\n\n\nMost Common Exclusion:\n\nCheese: The analysis reveals that cheese is the most frequently excluded ingredient, with a count of 4 orders.\n\n\n\n\n\n\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nWITH extras_cte AS (\nSELECT record_id, GROUP_CONCAT('Extra ', PT.topping_name) AS option_text\nFROM extrasBreak_ AS EB JOIN pizza_toppings AS PT ON EB.extra_id = PT.topping_id GROUP BY record_id\n),\nexclusions_cte AS (\nSELECT record_id, GROUP_CONCAT('Exclusion ', PT.topping_name) AS option_text\nFROM exclusionsBreak_ AS EB JOIN pizza_toppings AS PT ON EB.exclusions_id = PT.topping_id\nGROUP BY record_id\n),\ncombined_cte AS (\nSELECT * FROM extras_cte UNION SELECT * FROM exclusions_cte\n),\npartial_data_cte AS (\nSELECT CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time,\n    IFNULL(GROUP_CONCAT(PN.pizza_name, ' - ', option_text), '') AS pizza_details\nFROM customer_orders_temp AS CO LEFT JOIN combined_cte AS CC ON CO.record_id = CC.record_id\nJOIN pizza_names AS PN ON PN.pizza_id = CO.pizza_id\nGROUP BY CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time\n)\nSELECT PDC.record_id, PDC.order_id, PDC.customer_id, PDC.pizza_id, PDC.order_time,\n    CASE WHEN PDC.pizza_id = '1' AND pizza_details = '' THEN 'MeatLover'\n         WHEN PDC.pizza_id = '2' AND pizza_details = '' THEN 'Vegetarian' ELSE PDC.pizza_details END AS pizza_detail\nFROM partial_data_cte AS PDC;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\nrecord_id\norder_id\ncustomer_id\npizza_id\norder_time\npizza_detail\n\n\n\n\n1\n1\n101\n1\n2020-01-01 18:05:02\nMeatLover\n\n\n2\n2\n101\n1\n2020-01-01 19:00:52\nMeatLover\n\n\n3\n3\n102\n1\n2020-01-02 23:51:23\nMeatLover\n\n\n4\n3\n102\n2\n2020-01-02 23:51:23\nVegetarian\n\n\n5\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers - Exclusion Cheese\n\n\n6\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers - Exclusion Cheese\n\n\n7\n4\n103\n2\n2020-01-04 13:23:46\nVegetarian - Exclusion Cheese\n\n\n8\n5\n104\n1\n2020-01-08 21:00:29\nMeatlovers - Extra Bacon\n\n\n9\n6\n101\n2\n2020-01-08 21:03:13\nVegetarian\n\n\n10\n7\n105\n2\n2020-01-08 21:20:29\nVegetarian - Extra Bacon\n\n\n11\n8\n102\n1\n2020-01-09 23:54:33\nMeatLover\n\n\n12\n9\n103\n1\n2020-01-10 11:22:59\nMeatlovers - Extra Bacon, Extra Chicken, Meatlovers - Exclusion Cheese\n\n\n13\n10\n104\n1\n2020-01-11 18:34:49\nMeatLover\n\n\n14\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers - Extra Bacon, Extra Cheese, Meatlovers - Exclusion BBQ Sauce, Exclusion Mushrooms\n\n\n\n\n\n\n\nFor example: “Meat Lovers: 2xBacon, Beef, … , Salami”.\n\nWITH pizza_ingredients AS (\n    SELECT CO.record_id, CO.order_id, CO.customer_id, CO.pizza_id, CO.order_time, PN.pizza_name,\n      CASE WHEN PT.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id)\n      THEN CONCAT('2x ', PT.topping_name) ELSE PT.topping_name END AS ingredients_used\n    FROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR ON CO.pizza_id = PR.pizza_id\n    JOIN pizza_toppings AS PT ON PT.topping_id = PR.topping_id\n    JOIN pizza_names AS PN ON PN.pizza_id = CO.pizza_id\n    WHERE PR.topping_id NOT IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id)\n)\nSELECT PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time,CONCAT(PI.pizza_name, ': ',\n        GROUP_CONCAT(ingredients_used ORDER BY ingredients_used)) AS ingredients_used\nFROM pizza_ingredients AS PI\nGROUP BY PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time, PI.pizza_name\nORDER BY PI.record_id, PI.order_id, PI.customer_id, PI.pizza_id, PI.order_time;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\nrecord_id\norder_id\ncustomer_id\npizza_id\norder_time\ningredients_used\n\n\n\n\n1\n1\n101\n1\n2020-01-01 18:05:02\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n2\n2\n101\n1\n2020-01-01 19:00:52\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n3\n3\n102\n1\n2020-01-02 23:51:23\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n4\n3\n102\n2\n2020-01-02 23:51:23\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n5\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n6\n4\n103\n1\n2020-01-04 13:23:46\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n7\n4\n103\n2\n2020-01-04 13:23:46\nVegetarian: Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n8\n5\n104\n1\n2020-01-08 21:00:29\nMeatlovers: 2x Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n9\n6\n101\n2\n2020-01-08 21:03:13\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n10\n7\n105\n2\n2020-01-08 21:20:29\nVegetarian: Cheese,Mushrooms,Onions,Peppers,Tomato Sauce,Tomatoes\n\n\n11\n8\n102\n1\n2020-01-09 23:54:33\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n12\n9\n103\n1\n2020-01-10 11:22:59\nMeatlovers: 2x Bacon,2x Chicken,BBQ Sauce,Beef,Cheese,Mushrooms,Pepperoni,Salami\n\n\n13\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers: Bacon,BBQ Sauce,Beef,Cheese,Chicken,Mushrooms,Pepperoni,Salami\n\n\n14\n10\n104\n1\n2020-01-11 18:34:49\nMeatlovers: 2x Bacon,2x Cheese,BBQ Sauce,Beef,Chicken,Mushrooms,Pepperoni,Salami\n\n\n\n\n\n\nWITH pizza_ingredients AS (\nSELECT CO.order_id, CO.customer_id, PT.topping_name,\n    CASE WHEN PR.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id) THEN 2\n    WHEN PR.topping_id IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id) THEN 0\n    ELSE 1 END AS ingredients_used\nFROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR\nON CO.pizza_id = PR.pizza_id JOIN pizza_toppings AS PT\nON PT.topping_id = PR.topping_id\n)\nSELECT PI.topping_name, SUM(ingredients_used) AS qty_used_of_each_ingredients FROM pizza_ingredients AS PI\nGROUP BY PI.topping_name\nORDER BY PI.topping_name;\nOutput:\n\n\n\ntopping_name\nqty_used_of_each_ingredients\n\n\n\n\nBacon\n13\n\n\nBBQ Sauce\n10\n\n\nBeef\n10\n\n\nCheese\n13\n\n\nChicken\n10\n\n\nMushrooms\n14\n\n\nOnions\n4\n\n\nPepperoni\n10\n\n\nPeppers\n4\n\n\nSalami\n10\n\n\nTomato Sauce\n4\n\n\nTomatoes\n4\n\n\n\n\n\n\nIngredient Usage Overview:\n\nMushrooms: Mushrooms emerge as the most frequently used ingredient, with a total quantity of 14 units across all delivered pizzas.\nBacon, Cheese: Bacon and cheese follow closely behind, with both ingredients utilized in 13 units of pizzas each.\nBBQ Sauce, Beef, Chicken, Pepperoni, Salami: These ingredients are also commonly used, each appearing in 10 units of pizzas.\nOnions, Peppers, Tomato Sauce, Tomatoes: These ingredients have a relatively lower usage frequency, each appearing in 4 units of pizzas."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#d.-pricing-and-ratings-1",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#d.-pricing-and-ratings-1",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "SELECT\n    SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) AS Total_revenue\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN pizza_names AS PN ON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0;\nOutput: | Total_revenue | |—————| | 138 |\n\n\n\nRevenue Overview:\n\nTotal Revenue: $138\n\nRevenue Contribution:\n\nMeat Lovers pizzas, priced at $12 each, contributed significantly to Pizza Runner’s revenue, generating a substantial portion of the total revenue.\n\nPricing Strategy:\n\nThe pricing strategy of offering Meat Lovers and Vegetarian pizzas at $12 and $10, respectively, appears to align with customer preferences and market demand. The differential pricing reflects the perceived value of the pizzas and encourages customers to opt for higher-priced options.\n\n\n\n\n\n\nSELECT\n    SUM(\n        CASE \n        WHEN pizza_name = 'Meatlovers' AND extra_one_dollar_charge = '1' THEN (12 + 1)\n        WHEN pizza_name = 'Meatlovers' AND extra_one_dollar_charge = '2' THEN (12 + 2)\n        WHEN pizza_name = 'Vegetarian' AND extra_one_dollar_charge = '1' THEN (10 + 1)\n        WHEN pizza_name = 'Vegetarian' AND extra_one_dollar_charge = '2' THEN (10 + 2)\n        END) AS Total_revenue\nFROM (SELECT \n        CO.order_id, CO.customer_id, CO.pizza_id, RO.distance,\n        PN.pizza_name, \n        (CASE WHEN extras LIKE '%4%' THEN 2 ELSE 1 END) AS extra_one_dollar_charge\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO\nON CO.order_id = RO.order_id\nJOIN pizza_names AS PN\nON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0) AS temp_;\nOutput:\n\n\n\nTotal_revenue\n\n\n\n\n151\n\n\n\n\n\n\nRevenue Overview::\n\nTotal Revenue with Extra Charge: $151\n\nImpact of Extra Charge:\n\nIntroducing an additional $1 charge for pizza extras, particularly for adding cheese, resulted in a slight increase in total revenue compared to the previous scenario.\n\n\n\n\n\n\nDROP TABLE IF EXISTS ratings;\nCREATE TABLE ratings (\n    order_id INT,\n    ratings INT\n);\nINSERT INTO ratings (order_id, ratings)\n-- Inserting some random ratings\nVALUES (1,4),\n       (2,3),\n       (3,4),\n       (4,1),\n       (5,5),\n       (7,2),\n       (8,4),\n       (10,3);\nSELECT * FROM ratings;\nOutput:\n\n\n\norder_id\nratings\n\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n4\n\n\n4\n1\n\n\n5\n5\n\n\n7\n2\n\n\n8\n4\n\n\n10\n3\n\n\n\n\n\n\n- customer_id, order_id, runner_id, rating, order_time, pickup_time, Time between order and pickup, Delivery duration, Average speed, Total number of pizzas.\nSELECT\n    CO.customer_id, CO.order_id, \n    RO.runner_id, R.ratings, \n    CO.order_time, RO.duration,\n    ROUND(TIME_TO_SEC(TIMEDIFF(RO.pickup_time, CO.order_time))/60,0) AS time_between_order_and_pickup_in_minutes,\n    ROUND(AVG(RO.distance/(RO.duration/60)),2) AS average_speed_in_kmph,\n    COUNT(CO.order_id) AS pizza_count\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN ratings AS R ON RO.order_id = R.order_id\nWHERE RO.distance != 0\nGROUP BY CO.customer_id, CO.order_id, RO.runner_id, R.ratings, CO.order_time,\n    RO.duration, time_between_order_and_pickup_in_minutes;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\norder_id\nrunner_id\nratings\norder_time\ndistance\nduration\ntime_between_order_and_pickup_in_minutes\naverage_speed_in_kmph\npizza_count\n\n\n\n\n101\n1\n1\n4\n2020-01-01 18:05:02\n32\n11\n37.5\n1\n1\n\n\n101\n2\n1\n3\n2020-01-01 19:00:52\n27\n10\n44.44\n1\n1\n\n\n102\n3\n1\n4\n2020-01-02 23:51:23\n20\n21\n40.2\n2\n2\n\n\n103\n4\n2\n1\n2020-01-04 13:23:46\n40\n29\n35.1\n3\n3\n\n\n104\n5\n3\n5\n2020-01-08 21:00:29\n15\n10\n40\n1\n1\n\n\n105\n7\n2\n2\n2020-01-08 21:20:29\n25\n10\n60\n1\n1\n\n\n102\n8\n2\n4\n2020-01-09 23:54:33\n15\n20\n93.6\n1\n1\n\n\n104\n10\n1\n3\n2020-01-11 18:34:49\n10\n16\n60\n2\n2\n\n\n\n\n\n\nSELECT\n    SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) AS Total_revenue,\n    ROUND(SUM(RO.distance) * 0.3, 2) AS runner_earned_amount,\n    (SUM(CASE WHEN PN.pizza_name = 'Meatlovers' THEN 12 ELSE 10 END) - ROUND(SUM(RO.distance) * 0.3, 2))\n        AS Profit_left_after_paying_to_runners\nFROM customer_orders_temp AS CO\nJOIN runner_orders_temp AS RO ON CO.order_id = RO.order_id\nJOIN pizza_names AS PN ON CO.pizza_id = PN.pizza_id\nWHERE RO.distance != 0;\nOutput:\n\n\n\n\n\n\n\n\nTotal_revenue\nrunner_earned_amount\nProfit_left_after_paying_to_runners\n\n\n\n\n138\n64.62\n73.38\n\n\n\n\n\n\nRevenue Generation:\n\nThe total revenue generated from delivering pizzas, considering fixed prices for Meat Lovers and Vegetarian pizzas, amounts to $138.\n\nRunner Expenses:\n\nRunners are compensated at a rate of $0.30 per kilometer traveled. The total amount paid to runners for delivering pizzas is $64.62.\n\nProfit Analysis:\n\nAfter deducting the expenses incurred for paying runners from the total revenue generated, Pizza Runner has a profit of $73.38 left over from these deliveries.\n\n\n\n\n\n\nWITH pizza_ingredients AS (\nSELECT CO.order_id, CO.customer_id, PT.topping_name,\n    CASE WHEN PR.topping_id IN (SELECT extra_id FROM extrasBreak_ AS EB1 WHERE CO.record_id = EB1.record_id) THEN 2\n    WHEN PR.topping_id IN (SELECT exclusions_id FROM exclusionsBreak_ AS EB2 WHERE CO.record_id = EB2.record_id) THEN 0\n    ELSE 1 END AS ingredients_used\nFROM customer_orders_temp AS CO JOIN pizza_recipes_temp AS PR\nON CO.pizza_id = PR.pizza_id JOIN pizza_toppings AS PT\nON PT.topping_id = PR.topping_id\n)\nSELECT PI.topping_name, SUM(ingredients_used) AS qty_used_of_each_ingredients FROM pizza_ingredients AS PI\nGROUP BY PI.topping_name\nORDER BY PI.topping_name;\nOutput:\n\n\n\ntopping_name\nqty_used_of_each_ingredients\n\n\n\n\nBacon\n13\n\n\nBBQ Sauce\n10\n\n\nBeef\n10\n\n\nCheese\n13\n\n\nChicken\n10\n\n\nMushrooms\n14\n\n\nOnions\n4\n\n\nPepperoni\n10\n\n\nPeppers\n4\n\n\nSalami\n10\n\n\nTomato Sauce\n4\n\n\nTomatoes\n4\n\n\n\n\n\n\nTop Ingredients:\n\nMushrooms: With a total quantity of 14 units, mushrooms emerge as the most commonly used ingredient in delivered pizzas.\nBacon and Cheese: Bacon and cheese tie for the second position, each with a total quantity of 13 units.\nBBQ Sauce, Beef, Chicken, Pepperoni, and Salami: These ingredients have a total quantity of 10 units each, indicating their popularity among the pizza toppings.\nOnions, Peppers, Tomato Sauce, and Tomatoes: These ingredients have a relatively lower total quantity, each appearing four times in the delivered pizzas."
  },
  {
    "objectID": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#e.-bonus-questions-1",
    "href": "portfolio/8WeeksSQLChallenge-Pizza_Runner/index.html#e.-bonus-questions-1",
    "title": "Case Study #2 - Pizza Runner.",
    "section": "",
    "text": "INSERT INTO pizza_names (pizza_id, pizza_name)\nVALUES (3, 'Supreme');\n\nALTER TABLE pizza_recipes\nMODIFY COLUMN toppings VARCHAR(50);\n\nINSERT INTO pizza_recipes (pizza_id, toppings)\nVALUES (3, '1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12');\nOutput:\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n3\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Creating Python modules in Scripts for training and prediction",
    "text": "Creating Python modules in Scripts for training and prediction\nFirst we will write/create python modules like for augmentations, config, training & validation loops, prediction_to_generate_on_test_dataset etc.\nFor training augmentations we’ll be using like flipping the image, creating random patches in the image, randomly rotating 90 degrees, Adjusting the brightness and contrast, adding noise in the images, Shifting and sheering the image and finally normalizing the statistics of the image (since we will be using transfer learning therefore we need to prepare the images in the same way they were trained on - depending on the specific model we want to use).\n\n%%writefile Scripts/augmentations.py\nfrom Scripts.config import Config\nimport albumentations as A\n\ntraining_augmentations = A.Compose(\n    [\n        A.CoarseDropout(p=0.6),\n        A.RandomRotate90(p=0.6),\n        A.Flip(p=0.4),\n        A.OneOf(\n            [\n                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3),\n                A.HueSaturationValue(\n                    hue_shift_limit=20, sat_shift_limit=60, val_shift_limit=50\n                ),\n            ],\n            p=0.7,\n        ),\n        A.OneOf([A.GaussianBlur(), A.GaussNoise()], p=0.65),\n        A.ShiftScaleRotate(\n            shift_limit=0.0625, scale_limit=0.35, rotate_limit=45, p=0.5\n        ),\n        A.OneOf(\n            [\n                A.OpticalDistortion(p=0.3),\n                A.GridDistortion(p=0.1),\n                A.PiecewiseAffine(p=0.3),\n            ],\n            p=0.7,\n        ),\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        ),\n    ]\n)\n\nvalidation_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\ntesting_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\n\nWriting Scripts/augmentations.py\n\n\nCreating a config module which will contain model configurations like number of epochs to run, size of an image, weight decay (for regularization) etc., it also contains the path of the files and folders of the data.\n\n%%writefile Scripts/config.py\nimport torch\n\nclass Config:\n    EPOCHS = 5\n    IMG_SIZE = 512\n    ES_PATIENCE = 2\n    WEIGHT_DECAY = 0.001\n    VAL_BATCH_SIZE = 32 * 2\n    RANDOM_STATE = 1994\n    LEARNING_RATE = 5e-5\n    TRAIN_BATCH_SIZE = 32\n    MEAN = (0.485, 0.456, 0.406)\n    STD = (0.229, 0.224, 0.225)\n    TRAIN_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n        \"target\",\n        \"tfrecord\",\n    ]\n    TEST_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n    ]\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    ################ Setting paths to data input ################\n\n    data_2020 = \"../input/jpeg-melanoma-512x512/\"\n    train_folder_2020 = data_2020 + \"train/\"\n    test_folder_2020 = data_2020 + \"test/\"\n    test_csv_path_2020 = data_2020 + \"test.csv\"\n    train_csv_path_2020 = data_2020 + \"train.csv\"\n    submission_csv_path = data_2020 + \"sample_submission.csv\"\n\nWriting Scripts/config.py\n\n\nCreating a single dataset class to read the images (both training, validation & testing images), the function is capable of handling/reading the tabular features.\nThe function takes a dataframe, a list of tabular features (if we want to use for training) i.e., list of strings like ['sex_missing',anatom_site_general_challenge_head_neck','anatom_site_general_challenge_lower_extremity',     anatom_site_general_challenge_torso','anatom_site_general_challenge_upper_extremity','scaled_age'] , the augmentations we want to use and finally whether the dataset is a training, validation or testing dataset.\nFor training and validation we set is_test=False and for testing we set is_test=True to differentiate between the datasets.\n\n%%writefile Scripts/dataset.py\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom typing import List, Callable\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass DatasetRetriever(nn.Module):\n    \"\"\"\n    Dataset class to read the images and tabular features from a\n    dataframe and returns the dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        tabular_features: List[str] = None,\n        use_tabular_features: bool = False,\n        augmentations: Callable = None,\n        is_test: bool = False,\n    ):\n        \"\"\" \"\"\"\n        self.df = df\n        self.tabular_features = tabular_features\n        self.use_tabular_features = use_tabular_features\n        self.augmentations = augmentations\n        self.is_test = is_test\n\n    def __len__(self):\n        \"\"\"\n        Function returns the number of images in a dataframe.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Function the takes an images and it's corresponding\n        tabular/meta features & target feature (for training\n        and validation) and returns a dictionary, otherwise,\n        for test dataset it only returns a dictionary of\n        an image and tabular features.\n        \"\"\"\n        image_path = self.df[\"image_path\"].iloc[index]\n        image = Image.open(image_path)\n        image = np.array(image)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.tensor(image, dtype=torch.float)\n        if self.use_tabular_features:\n            if len(self.tabular_features) &gt; 0 and self.is_test is False:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"tabular_features\": tabular_features,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif len(self.tabular_features) &gt; 0 and self.is_test is True:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                return {\"image\": image, \"tabular_features\": tabular_features}\n        else:\n            if self.is_test is False:\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif self.is_test is True:\n                return {\"image\": image}\n\nWriting Scripts/dataset.py\n\n\nNow we create a model class to create a model instance of EfficientNet model.\nCurrently, this function is capable of reading the images only and not the tabular features.\nSince in this project/notebook we are using the images only therefore, this function is good enough for that.\n\n%%writefile Scripts/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Class to instantiate EfficientNet-b5 model object which only\n    used images as inputs.\n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n    \n    \n# class Model(nn.Module):\n#     \"\"\"\n#     Class to instantiate EfficientNet-b5 model object which uses images\n#     as well as tabular features as inputs.\n#     \"\"\"\n#     def __init__(self, model_name='efficientnet-b5', pool_type=F.adaptive_avg_pool2d,\n#                 num_tabular_features=0):\n#         super().__init__()\n#         self.pool_type = pool_type\n#         self.model_name = model_name\n#         self.backbone = EfficientNet.from_pretrained(model_name)\n#         in_features = getattr(self.backbone, \"_fc\").in_features\n#         if num_tabular_features&gt;0:\n#             self.meta = nn.Sequential(\n#                 nn.Linear(num_tabular_features, 512),\n#                 nn.BatchNorm1d(512),\n#                 nn.ReLU(),\n#                 nn.Dropout(p=0.5),\n#                 nn.Linear(512, 128),\n#                 nn.BatchNorm1d(128),\n#                 nn.ReLU())\n#             in_features += 128\n#         self.output = nn.Linear(in_features, 1)\n    \n#     def forward(self, image, tabular_features=None):\n#         features = self.pool_type(self.backbone.extract_features(image), 1)\n#         cnn_features = features.view(image.size(0),-1)\n#         if num_tabular_features&gt;0:\n#             tabular_features = self.meta(tabular_features)\n#             all_features = torch.cat((cnn_features, tabular_features), dim=1)\n#             output = self.output(all_features)\n#             return output\n#         else:\n#             output = self.output(cnn_features)\n#             return output\n\nWriting Scripts/model.py\n\n\nWe create a validation function that predicts and generates probabilities only on the validation corresponding to a specific fold.\nThis function might be useful in come cases. This function is capable of running on a single gpu or multi-gpu device as well as on cpu.\n\n%%writefile Scripts/predict_on_validation_data.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import validation_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_validation_dataset(\n    validation_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    validation dataset and returns a submission.csv file.\n    Args:\n        validation_dataset = validation_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    valid_dataset = DatasetRetriever(\n        df=validation_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=True,\n    )\n    valid_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    valid_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(valid_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            valid_predictions.extend(valid_probs)\n    valid_predictions = [\n        valid_predictions[img].item() for img in range(len(valid_predictions))\n    ]\n    return valid_predictions\n\nWriting Scripts/predict_on_validation_data.py\n\n\nThis below function is used to generate the prediction probabilities on the testing dataset provided for the competition and generates a submission.csv file for the public and private leaderboard results.\n\n%%writefile Scripts/predict_on_test.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import testing_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_test_and_generate_submission_file(\n    test_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    test dataset and returns a submission.csv file.\n    Args:\n        test_df = test_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    test_dataset = DatasetRetriever(\n        df=test_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=testing_augmentations,\n        is_test=True,\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    test_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(test_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            test_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            test_predictions.extend(test_probs)\n    submission_df = pd.read_csv(Config.submission_csv_path)\n    test_predictions = [\n        test_predictions[img].item() for img in range(len(test_predictions))\n    ]\n    submission_df[\"target\"] = test_predictions\n    submission_df.to_csv(\"../working/submission.csv\", index=False)\n\nWriting Scripts/predict_on_test.py\n\n\nNow, we create a train_model module which has a run_model function that takes a fold number and the training dataframe.\nThe function creates training and validation dataframe , then we create training and validation datasets which only reads images and no tabular features, next we initialize seed (for reproduciblity of results), model object, loss function, optimizer, scheduler and a scaler object (for mixed precision).\n\n%%writefile Scripts/train_model.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nimport torch.cuda.amp as amp\nfrom Scripts.utils import create_folds\nfrom Scripts.utils import seed_everything\nfrom Scripts.dataset import DatasetRetriever\nfrom timeit import default_timer as timer\nfrom Scripts.training_and_validation_loops import train\nfrom torch.utils.data import Dataset, DataLoader\nfrom Scripts.augmentations import training_augmentations, validation_augmentations\n\ndef run_model(fold, train_df):\n    train_df = create_folds(train_df=train_df)\n    train_data = train_df.loc[train_df[\"fold\"] != fold].reset_index(drop=True)\n    valid_data = train_df.loc[train_df[\"fold\"] == fold].reset_index(drop=True)\n    validation_targets = valid_data[\"target\"]\n    train_dataset = DatasetRetriever(\n        df=train_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=training_augmentations,\n        is_test=False,\n    )\n    valid_dataset = DatasetRetriever(\n        df=valid_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=False,\n    )\n    training_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=Config.TRAIN_BATCH_SIZE,\n        shuffle=True,\n        num_workers=os.cpu_count(),\n    )\n    validation_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    seed_everything(Config.RANDOM_STATE)\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    loss = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(\n        params=model.parameters(),\n        lr=Config.LEARNING_RATE,\n        weight_decay=Config.WEIGHT_DECAY,\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer=optimizer,\n        mode=\"max\",\n        factor=0.2,\n        patience=2,\n        threshold=1e-3,\n        verbose=True,\n    )\n    scaler = amp.GradScaler()\n    start_time = timer()\n    model_save_path = f\"../working/Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"\n    model_results = train(\n        model=model,\n        train_dataloader=training_dataloader,\n        valid_dataloader=validation_dataloader,\n        loss_fn=loss,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=Config.DEVICE,\n        scaler=scaler,\n        epochs=Config.EPOCHS,\n        es_patience=2,\n        model_save_path=model_save_path,\n        validation_targets=validation_targets,\n    )\n    end_time = timer()\n    print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\nWriting Scripts/train_model.py\n\n\nRegular pytorch training and validation loops epochs for a single epoch and finally for N number of epochs a train function.\n\n%%writefile Scripts/training_and_validation_loops.py\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.cuda.amp as amp\nfrom Scripts.utils import EarlyStopping\nfrom sklearn.metrics import roc_auc_score\n\n\ndef train_one_epoch(\n    model, dataloader, loss_fn, optimizer, device, scaler, use_tabular_features=False\n):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, an optimizer, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    , scaler (for mixed precision) and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value on training dataset.\n    \"\"\"\n    train_loss = 0\n    model.train()\n    for data in dataloader:\n        optimizer.zero_grad()\n        if use_tabular_features:\n            data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                data[\"image\"].to(device, dtype=torch.float),\n                data[\"tabular_features\"].to(device, dtype=torch.float),\n                data[\"targets\"].to(device, dtype=torch.float),\n            )\n            with amp.autocast():\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        else:\n            data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                device, dtype=torch.float\n            ), data[\"targets\"].to(device, dtype=torch.float)\n            with amp.autocast():\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        train_loss += loss.item()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n\n\ndef validate_one_epoch(model, dataloader, loss_fn, device, use_tabular_features=False):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value & final predictions on the validation dataset.\n    \"\"\"\n    valid_loss, final_predictions = 0, []\n    model.eval()\n    with torch.inference_mode():\n        for data in dataloader:\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                    data[\"image\"].to(device, dtype=torch.float),\n                    data[\"tabular_features\"].to(device, dtype=torch.float),\n                    data[\"targets\"].to(device, dtype=torch.float),\n                )\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n            else:\n                data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                    device, dtype=torch.float\n                ), data[\"targets\"].to(device, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n            valid_loss += loss.item()\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            final_predictions.extend(valid_probs)\n    valid_loss = valid_loss / len(dataloader)\n    return valid_loss, final_predictions\n\n\ndef train(\n    model,\n    train_dataloader,\n    valid_dataloader,\n    loss_fn,\n    optimizer,\n    scheduler,\n    device,\n    scaler,\n    epochs,\n    es_patience,\n    model_save_path,\n    validation_targets,\n):\n    \"\"\"\n    This function takes a model instance, training dataloader,\n    validation dataloader, loss_fn, optimizer, scheduler, device,\n    scaler (object, for mixed precision), epochs (for how many epochs\n    to run the model), es_patience (number of epochs to wait after which\n    the model should stop training), model_save_path (where to save the\n    model to), validation_targets (used for the calculation of the AUC\n    score) and returns a dictionary object which has training loss,\n    validation loss and validation AUC score.\n    \"\"\"\n    results = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": []}\n\n    early_stopping = EarlyStopping(\n        patience=es_patience, verbose=True, path=model_save_path\n    )\n\n    for epoch in tqdm(range(epochs)):\n        train_loss = train_one_epoch(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            device=device,\n            scaler=scaler,\n            use_tabular_features=False,\n        )\n\n        valid_loss, valid_predictions = validate_one_epoch(\n            model=model,\n            dataloader=valid_dataloader,\n            loss_fn=loss_fn,\n            device=device,\n            use_tabular_features=False,\n        )\n\n        valid_predictions = np.vstack(valid_predictions).ravel()\n\n        valid_auc = roc_auc_score(y_score=valid_predictions, y_true=validation_targets)\n        scheduler.step(valid_auc)\n\n        early_stopping(valid_loss, model)\n\n        if early_stopping.early_stop:\n            print(\"Early Stopping\")\n            break\n\n        model.load_state_dict(torch.load(model_save_path))\n        print(\n            f\"Epoch : {epoch+1} | \"\n            f\"train_loss : {train_loss:.4f} | \"\n            f\"valid_loss : {valid_loss:.4f} | \"\n            f\"valid_auc : {valid_auc:.4f} \"\n        )\n        results[\"train_loss\"].append(train_loss)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_auc\"].append(valid_auc)\n    return results\n\nWriting Scripts/training_and_validation_loops.py\n\n\nIn the utils module we write some useful functions like create_folds (which will divide the training dataset into 5 equal parts and remove duplicate images from the dataset)\nseed_everything (for reproducing the results)\nEarlyStopping class (used to stop model training if our model performance on validation dataset starts to decline), plot_loss_curves (for plotting the training and validation loss and auc_scores for each epoch)\nRareLabelCategoryEncoder (class that combines the category/categories of a feature that appears in the dataset for a certain percentage of times like 5% of the time etc. into a single category called Rare)\nOutlierTreatment (class to cap the values of a feature by learning the lower quantile and upper_quantile values of a feature from the dataset (X) in the fit method and caps(transforms) the feature values in the dataset (x) passed in the transformed method).\n\n%%writefile Scripts/utils.py\nimport os\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import List\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\ndef create_folds(train_df):\n    \"\"\"\n    Function that folds in the training data and removes duplicate\n    images from the training data.\n    \"\"\"\n    train_df = train_df.loc[train_df[\"tfrecord\"] != -1].reset_index(drop=True)\n    train_df[\"fold\"] = train_df[\"tfrecord\"] % 5\n    return train_df\n\n\ndef seed_everything(seed: int):\n    \"\"\"\n    Function to set seed and to make reproducible results.\n    Args:\n        seed (int): like 42\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n    Directly borrowed from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        patience: int = 7,\n        verbose: bool = False,\n        delta: int = 0,\n        trace_func=print,\n    ):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score &lt; self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(\n                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n            )\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --&gt; {val_loss:.6f}).  Saving model ...\"\n            )\n        torch.save(obj=model.state_dict(), f=self.path)\n        self.val_loss_min = val_loss\n\n\ndef plot_loss_curves(results: dict):\n    \"\"\"\n    Function to plot training & validation loss curves & validation AUC\n    Args:\n        results (dict): A dictionary of training loss, validation_loss &\n        validation AUC score.\n    \"\"\"\n    loss = results[\"train_loss\"]\n    valid_loss = results[\"valid_loss\"]\n    # Get the accuracy values of the results dictionary (training and test)\n    valid_auc = results[\"valid_auc\"]\n    # Figure out how many epochs there were\n    epochs = range(len(results[\"train_loss\"]))\n    # Setup a plot\n    plt.figure(figsize=(15, 7))\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, valid_loss, label=\"valid_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, valid_auc, label=\"valid_auc\")\n    plt.title(\"AUC Score\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n\nclass RareLabelCategoryEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to combine rare categories of a categorical variable\n    where a category appearing less than a certain percentage\n    (as a threshold).\n    Example: a category/categories appearing less than 5% of the\n    times are combined a single category called rare.\n    \"\"\"\n\n    def __init__(self, variables: List, tol=0.05):\n        \"\"\"\n        Args:\n            variables (List): A list of variables for which we want\n            to combine into rare categories.\n            tol (int): A Threshold/Tolerance below which we want to\n            consider a category of a feature as rare.\n        \"\"\"\n        if not isinstance(variables, list):\n            raise ValueError(\"Variables should be a list\")\n        self.tol = tol\n        self.variables = variables\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns all the values/categories & the\n        percentage of times it appears in a feature in the\n        dataset passed while using this method.\n\n        Args:\n            X : From this dataset the fit function learns and\n            stores the number of times a category appears in\n            the dataset\n        \"\"\"\n        self.encoder_dict_ = {}\n        for var in self.variables:\n            t = pd.Series(x[var]).value_counts(normalize=True)\n            self.encoder_dict_[var] = list(t[t &gt;= self.tol].index)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        X (pd.DataFrame): Transform/Combines the categories of each\n        features passed into the variables list on the dataset passed\n        in this method and returns the transformed dataset.\n        \"\"\"\n        x = x.copy()\n        for var in self.variables:\n            x[var] = np.where(x[var].isin(self.encoder_dict_[var]), x[var], \"Other\")\n        return x\n\n\nclass OutlierTreatment(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to handle outliers in a continous feature.\n    \"\"\"\n\n    def __init__(\n        self, variable: str, upper_quantile: float = None, lower_quantile: float = None\n    ):\n        \"\"\"\n        Args:\n            variables (str): A variable to cap the upper and\n            lower boundaries of.\n            upper_quantile (float): A maximum value beyond which all the\n            values of a feature are capped at.\n            lower_quantile (float): A minimum value that are lower than\n            of the feature are capped at.\n        \"\"\"\n        if not isinstance(variable, str):\n            raise ValueError(\"Variable should be a string type.\")\n        self.upper_quantile = upper_quantile\n        self.variable = variable\n        self.lower_quantile = lower_quantile\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns the lower & upper quantiles of a feature\n        present in the dataset x.\n        \"\"\"\n        self.upper_quantile = x[self.variable].quantile(self.upper_quantile)\n        self.lower_quantile = x[self.variable].quantile(self.lower_quantile)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        This function caps the upper and lower quantiles in the dataframe\n        x with the values learnt in the dataframe passed in fit() method.\n        \"\"\"\n        x = x.copy()\n        x[self.variable] = np.where(\n            x[self.variable] &gt; self.upper_quantile,\n            self.upper_quantile,\n            np.where(\n                x[self.variable] &lt; self.lower_quantile,\n                self.lower_quantile,\n                x[self.variable],\n            ),\n        )\n        return x\n\nWriting Scripts/utils.py\n\n\n\nInitializing the __init__.py to make the Scripts folder a package.\n\n\n%%writefile Scripts/__init__.py\n\"\"\n\nWriting Scripts/__init__.py"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Creating requirements.txt file to install all the packages for our model training",
    "text": "Creating requirements.txt file to install all the packages for our model training\nThis will contain all the packages required for modeling/training the model.\n\n%%writefile requirements.txt\n# pandas==2.0.0\ntorch==1.13.0\ntorchvision==0.14.0\n# scikit-learn==1.2.2\nefficientnet_pytorch==0.7.1\nalbumentations==1.2.1\n# numpy==1.22.4\ntqdm==4.65.0\n# matplotlib==3.7.1\nPillow==8.4.0\n\nWriting requirements.txt"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Installing all the packages",
    "text": "Installing all the packages\n\n!pip install -r requirements.txt\n\nRequirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.13.0)\nRequirement already satisfied: torchvision==0.14.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.14.0)\nCollecting efficientnet_pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... done\nCollecting albumentations==1.2.1\n  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.7/116.7 kB 5.3 MB/s eta 0:00:00\nCollecting tqdm==4.65.0\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 8.9 MB/s eta 0:00:00\nCollecting Pillow==8.4.0\n  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 32.8 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0-&gt;-r requirements.txt (line 2)) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.7.3)\nRequirement already satisfied: scikit-image&gt;=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.19.3)\nRequirement already satisfied: opencv-python-headless&gt;=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (4.5.4.60)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (6.0)\nRequirement already satisfied: qudida&gt;=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.0.4)\nRequirement already satisfied: scikit-learn&gt;=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.0.2)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2021.11.2)\nRequirement already satisfied: imageio&gt;=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.25.0)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: networkx&gt;=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.6.3)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (23.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.26.14)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.1.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2022.12.7)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (3.1.0)\nRequirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.2.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... done\n  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=d870e4ba77c41d05a67b458d1a03a108aaee250b6f63fca1cad356a67702a3af\n  Stored in directory: /root/.cache/pip/wheels/96/3f/5f/13976445f67f3b4e77b054e65f7f4c39016e92e8358fe088db\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: tqdm, Pillow, efficientnet_pytorch, albumentations\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Uninstalling tqdm-4.64.1:\n      Successfully uninstalled tqdm-4.64.1\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 9.4.0\n    Uninstalling Pillow-9.4.0:\n      Successfully uninstalled Pillow-9.4.0\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.3.0\n    Uninstalling albumentations-1.3.0:\n      Successfully uninstalled albumentations-1.3.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires tqdm&lt;4.65,&gt;=4.48.2, but you have tqdm 4.65.0 which is incompatible.\nSuccessfully installed Pillow-9.4.0 albumentations-1.2.1 efficientnet_pytorch-0.7.1 tqdm-4.65.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore jumping right into the modeling and see the numbers go down or go up, it’s good to look at the data and try to make sense out of it.\nWe’ll do the same here as well, we will look at the distribution of each tabular features we have in the training and testing datasets.\n\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom Scripts.config import Config\n\n## Looking at the Data\ntrain_df = pd.read_csv(Config.train_csv_path_2020,\n                       usecols=Config.TRAIN_COLS)\ntest_df = pd.read_csv(Config.test_csv_path_2020,\n                       usecols=Config.TEST_COLS)\n\n## Creating Image_Path for each images in 2019 & 2020 training datasets\ntrain_df['image_path'] = os.path.join(Config.train_folder_2020) + train_df['image_name'] + \".jpg\"\ntest_df['image_path'] = os.path.join(Config.test_folder_2020) + test_df['image_name'] + \".jpg\"\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nimage_name\npatient_id\nsex\nage_approx\nanatom_site_general_challenge\ntarget\ntfrecord\nimage_path\n\n\n\n\n0\nISIC_2637011\nIP_7279968\nmale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_2637...\n\n\n1\nISIC_0015719\nIP_3075186\nfemale\n45.0\nupper extremity\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0015...\n\n\n2\nISIC_0052212\nIP_2842074\nfemale\n50.0\nlower extremity\n0\n6\n../input/jpeg-melanoma-512x512/train/ISIC_0052...\n\n\n3\nISIC_0068279\nIP_6890425\nfemale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0068...\n\n\n4\nISIC_0074268\nIP_8723313\nfemale\n55.0\nupper extremity\n0\n11\n../input/jpeg-melanoma-512x512/train/ISIC_0074...\n\n\n\n\n\n\n\n\nprint(f\"Number of Unique images id's in the training dataset are - {train_df['image_name'].nunique()} \\n\")\nprint(f\"Number of Unique images id's in the training dataset are - {test_df['image_name'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {train_df['patient_id'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {test_df['patient_id'].nunique()}\")\n\nNumber of Unique images id's in the training dataset are - 33126 \n\nNumber of Unique images id's in the training dataset are - 10982\n\nTotal number of Unique patients id's in the training dataset are - 2056\n\nTotal number of Unique patients id's in the training dataset are - 690"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Patient ID",
    "text": "Patient ID\n\npatients_id_counts_train = train_df['patient_id'].value_counts()\npatients_id_counts_test = test_df['patient_id'].value_counts()\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(19,10), dpi=80, sharex=False, sharey=False)\nsns.distplot(patients_id_counts_train, ax=ax[0,0], color='#3300CC', kde=True)\nax[0,0].set_xlabel(\"Counts\")\nax[0,0].set_ylabel('Frequency')\nax[0,0].set_title('Patient ID counts in training data')\n\nsns.distplot(patients_id_counts_test, ax=ax[0,1], color='#FF0099', kde=True)\nax[0,1].set_xlabel(\"Counts\")\nax[0,1].set_ylabel('Frequency')\nax[0,1].set_title('Patient ID counts in testing data')\n\nsns.boxplot(patients_id_counts_train, ax=ax[1,0], color='#3300CC')\nax[1,0].set_xlabel('Counts')\nax[1,0].set_title('BoxPlot of Patient ID Counts in Train data')\nsns.boxplot(patients_id_counts_test, ax=ax[1,1], color='#FF0099')\nax[1,1].set_xlabel('Counts')\nax[1,1].set_title('BoxPlot of Patient ID Counts in Test data');"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#gender",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#gender",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Gender",
    "text": "Gender\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x=train_df['sex'], color='#3300CC', ax=ax[0])\nax[0].set_ylabel(\"\")\nax[0].set_xlabel('Gender Count')\nax[0].set_title(\"Gender Count in Training data\")\n\nsns.countplot(x=test_df['sex'], color=\"#FF0099\", ax=ax[1])\nax[1].set_ylabel(\"\")\nax[1].set_xlabel('Gender Count')\nax[1].set_title(\"Gender Count in Testing data\");"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#age",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#age",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Age",
    "text": "Age\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\nsns.countplot(train_df['age_approx'], color='#3300CC', ax=ax[0])\nax[0].set_title(\"Distribution of Age feature in Training Data\")\nax[0].set_xlabel(\"Age\")\nax[0].set_ylabel('Frequency')\n\nsns.countplot(test_df['age_approx'], color='#FF0099', ax=ax[1])\nax[1].set_title(\"Distribution of Age feature in Testing Data\")\nax[1].set_xlabel(\"Age\")\nax[1].set_ylabel('Frequency');\n\n\n\n\n\nage_dist_train_test = pd.concat(\n    [train_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93, \n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99]),\n    test_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93,\n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99])\n    ], axis=1)\nage_dist_train_test.columns = ['Train_Age', 'Test_Age']\nage_dist_train_test.T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n10%\n15%\n25%\n50%\n...\n91%\n92%\n93%\n94%\n95%\n96%\n97%\n98%\n99%\nmax\n\n\n\n\nTrain_Age\n33058.0\n48.870016\n14.380360\n0.0\n20.0\n25.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n75.0\n80.0\n90.0\n\n\nTest_Age\n10982.0\n49.525587\n14.370589\n10.0\n20.0\n30.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n80.0\n80.0\n85.0\n90.0\n\n\n\n\n2 rows × 22 columns"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "anatom_site_general_challenge",
    "text": "anatom_site_general_challenge\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\n\ntrain_anatom_site_general = train_df[\"anatom_site_general_challenge\"].value_counts().sort_values(ascending=False)\ntest_anatom_site_general = test_df.anatom_site_general_challenge.value_counts().sort_values(ascending=False)\n\nsns.barplot(x=train_anatom_site_general.index.values, y=train_anatom_site_general.values, ax=ax[0]);\nax[0].set_xlabel(\"\");\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_title(\"Image locations in train\");\n\nsns.barplot(x=test_anatom_site_general.index.values, y=test_anatom_site_general.values, ax=ax[1]);\nax[1].set_xlabel(\"\");\nlabels = ax[1].get_xticklabels();\nax[1].set_xticklabels(labels, rotation=90);\nax[1].set_title(\"Image locations in test\");"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#target",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Target",
    "text": "Target\n\ntrain_df['target'].value_counts(normalize=True, dropna=False) * 100\n\n0    98.237034\n1     1.762966\nName: target, dtype: float64"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Sex Vs. Target & anatom_site_general_challenge Vs. Target",
    "text": "Sex Vs. Target & anatom_site_general_challenge Vs. Target\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x='target', hue='sex',data=train_df, ax=ax[0])\nax[0].set_xlabel(\"Target\", fontsize=15)\nax[0].set_ylabel('Count', fontsize=15)\nax[0].set_title(\"Sex Vs. Target\", fontsize=20)\n\nsns.countplot(x='target', hue='anatom_site_general_challenge',data=train_df, ax=ax[1])\nax[1].set_xlabel(\"Target\", fontsize=15)\nax[1].set_ylabel('Count', fontsize=15)\nax[1].set_title(\"Sex Vs. anatom_site_general_challenge\", fontsize=20);"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Age Vs. Target",
    "text": "Age Vs. Target\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_df.loc[train_df['target'] == 0]['age_approx'], bins=50, label='Benign')\nsns.distplot(train_df.loc[train_df['target'] == 1]['age_approx'], bins=50, label='Malignant')\nplt.legend(loc='best')\nplt.ylabel('Density', fontsize=15)\nplt.xlabel('Age', fontsize=15)\nplt.title(\"Age Vs. Target distribution\", fontsize=20);"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Training our model with No Tabular features",
    "text": "Training our model with No Tabular features\n\nFor now, we will run/train the model on a single fold only.\n\n\nfrom Scripts.train_model import run_model\nrun_model(fold=0, train_df=train_df)\n\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b5-b6417697.pth\n  0%|          | 0/5 [00:00&lt;?, ?it/s] 20%|██        | 1/5 [57:05&lt;3:48:23, 3425.86s/it] 40%|████      | 2/5 [1:51:06&lt;2:45:49, 3316.66s/it] 60%|██████    | 3/5 [2:45:18&lt;1:49:34, 3287.46s/it] 80%|████████  | 4/5 [3:39:10&lt;54:25, 3265.50s/it]  100%|██████████| 5/5 [4:32:49&lt;00:00, 3273.97s/it]\n\n\n\n\n\nLoaded pretrained weights for efficientnet-b5\nValidation loss decreased (inf --&gt; 0.071890).  Saving model ...\nEpoch : 1 | train_loss : 0.0986 | valid_loss : 0.0719 | valid_auc : 0.8579 \nValidation loss decreased (0.071890 --&gt; 0.067238).  Saving model ...\nEpoch : 2 | train_loss : 0.0749 | valid_loss : 0.0672 | valid_auc : 0.8902 \nEarlyStopping counter: 1 out of 2\nEpoch : 3 | train_loss : 0.0708 | valid_loss : 0.0709 | valid_auc : 0.8746 \nValidation loss decreased (0.067238 --&gt; 0.065598).  Saving model ...\nEpoch : 4 | train_loss : 0.0706 | valid_loss : 0.0656 | valid_auc : 0.8971 \nEarlyStopping counter: 1 out of 2\nEpoch : 5 | train_loss : 0.0683 | valid_loss : 0.0673 | valid_auc : 0.9015 \nTotal training time: 16369.841 seconds"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Generating submission file on the test dataset",
    "text": "Generating submission file on the test dataset\n\nfrom Scripts.predict_on_test import predict_on_test_and_generate_submission_file\n\nmodel_path = \"../working/Models/efficientnet_b5_checkpoint_fold_0.pt\"\npredict_on_test_and_generate_submission_file(test_df=test_df,\n                                            use_tabular_features=False,\n                                            model_path=model_path)\n\nLoaded pretrained weights for efficientnet-b5"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Building Gradio demo",
    "text": "Building Gradio demo\nNow that we have finalized the model we’ll be deploying, we will use this EfficientNet-B5 model to predict on new images.\nFor this project I will be using Gradio library (product of HuggingFace).\nWhy Gradio? The homepage of Gradio descibes it as: &gt; Gradio is the fastest way to build/demo your machine learning model with a friendly web interface so that anyone can use it, anywhere.\n\nGradio Overview\nIn general, we can have any combination of inputs like - Images - Tabular data - Text - Numbers - Video - Audio - etc.\nIn our case we have images and inputs and the output is returned as a probability of whether a patient is sufferig from melanoma skin cancer disease.\nGradio provides an interface that maps from the input(s) to output(s).\ngr.Interface(function, inputs, outputs)\nWhere, fn is a python function to map inputs to outputs\nGradio provides a very helpful Interface class to create an inputs -&gt; model/function -&gt; outputs workflow where the inputs and outputs could be almost anything we want.\n\n## Installing Gradio and importing it. \ntry:\n    import gradio as gr\nexcept:\n    !pip install -q gradio\n    import gradio as gr\nprint(f\"Gradio version : {gr.__version__}\")\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nGradio version : 3.27.0"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Creating a model instance and putting it on the CPU",
    "text": "Creating a model instance and putting it on the CPU\nFirst, let’s make sure our EfficientNetB5 model on CPU\n\nimport torch\nimport numpy as np\nimport albumentations as A\nfrom Scripts.model import Model\n\nefficientnet_b5_model = Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\nnext(iter(model.parameters())).device\n\nLoaded pretrained weights for efficientnet-b5\n\n\ndevice(type='cpu')"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Creating a function to predict on a single images",
    "text": "Creating a function to predict on a single images\nWe create a function that takes an input: image -&gt; transform -&gt; predict with EfficientNetB5 -&gt; output: probability.\nThis will be our fn parameter for our Gradio Interface.\n\n## predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n\nLet’s see our function by performing a prediction of an image from the training dataset.\nWe’ll get some images from training dataset and extract the images paths list. Then we’ll open an image with Image.open().\nFinally, we pass the image to predict_on_single_image().\n\nimport torch\nimport pathlib\nimport numpy as np\nfrom PIL import Image\n\n## Taking some images out\nimages = train_df.iloc[1:10,]\nimages_paths_list = images['image_path'].tolist()\n\n## Opening an Image\nimg = Image.open(images_paths_list[8])\n\n## Predicting on the image using the function\npredict_on_single_image(img)\n\n{'Probability of Having Melanoma': 0.5395416617393494,\n 'Probability of Not having Melanoma': 0.46045833826065063}\n\n\n\nCreating a list of example images\nBefore we create a demo, we first create a list of examples.\nGradio’s Interface class takes a list of examples parameter is a list of lists.\nSo, we create a list of lists containing the filepaths of images.\nOur gradio demo will showcase these as example inputs to our demo so people can try.\n\nexample_list = [[str(file_path)] for file_path in images_paths_list]\nexample_list\n\n[['../input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg']]\n\n\n\n\nBuilding a Gradio interface\nPutting everything together -&gt;\nGradio Interface Workflow: input: image -&gt; transform -&gt; predict with EfficientNetB5 model -&gt; probability: output\nWe can do with the Gr.Interface() class with the following parameters: - fn: a python function that maps from inputs to outputs, in our case the predict_on_single_image() function. - inputs: the input to our Interface, such as image using gradio.Image(). - outputs: the output of the Interface once the inputs are processed with the fn, such as a Number gradio.Number() (for our case probability). - examples: a list of examples to showcase for the demo. - title: a string title of the demo. - description: a string description of the demo.\nOnce, we’ve created a demo instance of gr.Interface(), we use demo.launch() command.\n\nimport gradio as gr\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\nRunning on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://e228186381e2781228.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n\n\n\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 420, in run_asgi\n    self.scope, self.receive, self.send\n  File \"/opt/conda/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\n    return await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/applications.py\", line 270, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/applications.py\", line 124, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n    raise exc\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\n    raise e\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 706, in __call__\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 276, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/lib/python3.7/site-packages/starlette/routing.py\", line 66, in app\n    response = await func(request)\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 238, in app\n    dependant=dependant, values=values, is_coroutine=is_coroutine\n  File \"/opt/conda/lib/python3.7/site-packages/fastapi/routing.py\", line 163, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/opt/conda/lib/python3.7/site-packages/gradio/routes.py\", line 345, in file\n    f\"File cannot be fetched: {path_or_url}. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\"\nValueError: File cannot be fetched: /kaggle/input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app.\n\n\nWoow!!!\nOur application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting, we can upload our Gradio app Interface to HuggingFace Spaces.\nNow download all the files and folders from kaggle output manually & this kaggle kernel locally"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app",
    "text": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app\nWe’ll deploy the demo application on HuggingFace Spaces.\nWhat is HuggingFace Spaces?\nIt is a resource that allows anybody to host and share machine learning application.\n\nDeployed Gradio App Structure\nTo upload our gradio app, we’ll want to put everything together into a singe directory.\nFor example, our demo might live at the path demos/melanoma_skin_cancer_files with the following structure:\ndemos/\n    └── melanoma_skin_cancer_files/\n        ├── efficientnet_b5_checkpoint_fold_0.pt\n        ├── app.py\n        ├── examples/\n        │   ├── image_1.jpg\n        │   ├── image_2.jpg\n        │   └── image_3.jpg\n        ├── model.py\n        └── requirements.txt\nWhere: - efficientnet_b5_checkpoint_fold_0 is our trained model. - app.py contains our Gradio app. Note: app.py is the default filename used for HuggingFace Spaces, if we deploy our apps there. - examples contains sample images to showcase the demo of our Gradio application. - model.py contains the main model/transformations code associated with our model. - requirements.txt file contains the dependencies/packages to run our application such as torch, albumentations, torchvision, gradio, numpy.\n\n\nCreating a demo folder to store our Melanoma skin cancer App files\nTo begin, we’ll create an empty directory demos/ that will contain all our necessary files for the application.\nWe can achive this using Python’s pathlib.Path(\"path_of_dir\") to establish directory path and then pathlib.Path(\"path_of_dir\").mkdir() to create it.\n\n############### ROOT_DIR : I Have put the files in my E: drive\n## Importing Packages \nimport shutil\nfrom pathlib import Path\nimport os\n\nROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n\n## Create Melanoma skin cancer demo path\nmelanoma_app_demo_path = Path(f\"{ROOT_DIR}/demos/melanoma_skin_cancer_files\")\n\n## Removing files that might already exist and creating a new directory.\nif melanoma_app_demo_path.exists():\n    shutil.rmtree(melanoma_app_demo_path)\n    melanoma_app_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n                                exist_ok=True) # Create even if they already exists? \nelse:\n    ## If the path doesn't exist, create one \n    melanoma_app_demo_path.mkdir(parents=True,\n                                exist_ok=True)\n\n\n\nCreating a folder of example images to use with our Melanoma skin cancer demo\nNow we’ll create an empty directory called examples and store some images (namely - ISIC_0015719.jpg, ISIC_0052212.jpg, ISIC_0068279.jpg) from the training dataset provided in the competition (which we download manually) from the here. Download the 1ts three images from the training dataset mentioned above.\nPut these images in the Data/Input (whatever you want to call) folder.\nTo do so we’ll:\n\nCreate an empty directory examples/ within the demos/melanoma_skin_cancer_files directory.\nDownload the top 3 mentioned images from the training dataset from the link above.\nCollect the filepaths into a list.\nCopy these 3 images from the train dataset to the demos/melanom_skin_cancer_files/examples/ directory.\n\n\nimport shutil\nfrom pathlib import Path\n\n## Create examples directory\nmelanoma_app_examples_path = melanoma_app_demo_path / \"examples\"\nmelanoma_app_examples_path.mkdir(parents=True, exist_ok=True)\n\n## collecting the image paths of 4 images \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0149568.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\n\n## collecting the image paths of some more images but this time from the testing folder \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0052060.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082004.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082785.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0105104.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0112420.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0155983.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0171865.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\nNow we verify our example images are present, let’s list the contents of our demo/melanoma_skin_cancer/examples/ directory with os.listdir() and then format the filepaths into a list of lists (to make it compatible with the Gradio’s gradio.Interface(), example parameter).\n\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\nexample_list\n\n[['examples/ISIC_0015719.jpg'],\n ['examples/ISIC_0052060.jpg'],\n ['examples/ISIC_0052212.jpg'],\n ['examples/ISIC_0068279.jpg'],\n ['examples/ISIC_0082004.jpg'],\n ['examples/ISIC_0082785.jpg'],\n ['examples/ISIC_0105104.jpg'],\n ['examples/ISIC_0112420.jpg'],\n ['examples/ISIC_0149568.jpg'],\n ['examples/ISIC_0155983.jpg'],\n ['examples/ISIC_0171865.jpg']]\n\n\n\n\nMoving our trained EfficientNet-B5 model into our Melanoma demo directory.\nWe previously saved our model binary file into the Models directory while training as Models/efficientnet_b5_checkpoint_fold_0.pt.\nWe use Python’s shutil.move() method and passing in src(the source path of the target file) and dst (the destination folder path of the target file to be moved into) parameters.\n\n## Importing Libraries\nimport shutil\n\n## Create a source path for our target model\nefficientnet_b5_model_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Models\\\\efficientnet_b5_checkpoint_fold_0.pt\"\n\n## Create a destination path for our target model\nefficientnet_b5_model_destination = melanoma_app_demo_path / efficientnet_b5_model_path.split(\"\\\\\")[-1]\n\n## Try to move the file\ntry:\n    print(f\"Attempting to move the {efficientnet_b5_model_path} to {efficientnet_b5_model_destination}\")\n    \n    ## Move the model\n    shutil.move(src=efficientnet_b5_model_path,\n               dst=efficientnet_b5_model_destination)\n    \n    print(\"Model move completed\")\n## If the model has already been moved, check if it exists\nexcept:\n    print(f\"No model found at {efficientnet_b5_model_path}, perhaps it's already moved.\")\n    print(f\"Model already exists at {efficientnet_b5_model_destination}: {efficientnet_b5_model_destination.exists()}\")\n\nAttempting to move the E:\\Melanoma_skin_cancer_detection\\output\\working\\Models\\efficientnet_b5_checkpoint_fold_0.pt to E:\\Melanoma_skin_cancer_detection\\demos\\melanoma_skin_cancer_files\\efficientnet_b5_checkpoint_fold_0.pt\nModel move completed\n\n\n\n\nTurning our EfficientNet-B5 model into a Python script (model.py)\nOur current model’s state_dict() is saved to demos/melanoma_skin_cancer/efficientnet_b5_checkpoint_fold_0.pt.\nTo load it it we can use model.load_state_dict() with torch.load(). But before that we need to instantiate a model.\nTo do this in a modular fashion we’ll create a script model.py which contains the model definition into a function called Model().\n\n## Now if we look into which directory we are currently, we'll find that using the following code\nos.getcwd()\n\n'E:\\\\Melanoma_skin_cancer_detection\\\\notebooks'\n\n\nNow we will move into the demos directory where we will write some helper utilities.\nIn cd ../demos/: .. means we are moving outside of the notebooks directory. demos/: means we moving inside the demos directory.\n\ncd ../demos/\n\nE:\\Melanoma_skin_cancer_detection\\demos\n\n\n\n%%writefile melanoma_skin_cancer_files/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Creates an efficientnet-b5 model instance.    \n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n\nWriting melanoma_skin_cancer_files/model.py\n\n\n\n\nTurning our Melanoma Skin Cancer Gradio App into a Python Script (app.py)\n\n%%writefile melanoma_skin_cancer_files/app.py\n## Importing Libraries\nimport os\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom model import Model\nimport albumentations as A\n\n## Creating a model instance\nefficientnet_b5_model =  Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\n\n## Predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    efficientnet_b5_model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(efficientnet_b5_model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n    \n## Gradio App\nimport gradio as gr\n\n## Examples directory path\nmelanoma_app_examples_path = \"examples\"\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\nWriting melanoma_skin_cancer_files/app.py\n\n\n\n\nCreating a requirements.txt file for our Gradio App(requirements.txt)\nThis is the last file we need to create for our application.\nThis file contains all the necessary packages for our Gradio application.\nWhen we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\n\n%%writefile melanoma_skin_cancer_files/requirements.txt\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\nWriting melanoma_skin_cancer_files/requirements.txt"
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Deploying our Application to HuggingFace Spaces",
    "text": "Deploying our Application to HuggingFace Spaces\nTo deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n\nUploading via the Hugging Face Web Interface (easiest)\nUploading via the command line or terminal\n\nNOTE: To host any application on HuggingFace, we first need to sign up for a free HuggingFace Account\n\nRunning our Application locally\n\nOpen the terminal or command prompt.\nChanging the melanoma_skin_cancer_files directory (cd melanoma_skin_cancer_files).\nCreating an environment (python3 -m venv env) or use (python -m venv env).\nActivating the environment (source env/Scripts/activate).\nInstalling the requirements.txt using pip install -r requirements.txt. &gt; If faced any errors, we might need to upgrade pip using pip install --upgrade pip.\n\nRun the app (python3 app.py).\n\nThis should results in a Gradio demo locally at the URL such as : http://127.0.0.1:7860/.\n\n\nUploading to Hugging Face\nWe’ve verified our Melanoma_skin_cancer detection application is working in our local system.\nTo upload our application to Hugging Face Spaces, we need to do the following.\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to our profile at the top right corner and then select New Space.\nDeclare the name to the space like Chirag1994/melanoma_skin_cancer_detection_app.\nSelect a license (I am using MIT license).\nSelect Gradio as the Space SDK (software development kit).\nChoose whether your Space is Public or Private (I am keeping it Public).\nClick Create Space.\nClone the repository locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in the terminal or command prompt. In our case mine would be like - git clone https://huggingface.co/spaces/Chirag1994/melanoma_skin_cancer_detection_app.\nCopy/Move the contents of the downloaded melanoma_skin_cancer_detection_app folder to the cloned repo folder.\nTo upload files and track larger files (e.g., files that are greater than 10MB) for them we need to install Git LFS which stands for Git large File Storage.\nOpen up the cloned directory using VS code (I’m using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command git lfs install to start tracking the file that we want to track. For example - git lfs track \"efficientnet_b5_checkpoint_fold_0.pt\".\nCreate a new .gitignore file and the files & folders that we don’t want git to track like :\n\n__pycache__/\n.vscode/\nvenv/\n.gitignore\n.gitattributes\n\nAdd the rest of the files and commit them with:\n\ngit add .\ngit commit -m \"commit message that you want\"\n\nPush(load) the files to Hugging Face\n\ngit push\n\nIt might a couple of minutes to finish and then the app will be up and running."
  },
  {
    "objectID": "portfolio/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "href": "portfolio/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch.",
    "section": "Our Final Application deployed on HuggingFace Spaces",
    "text": "Our Final Application deployed on HuggingFace Spaces\n\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/Chirag1994/Melanoma_Skin_Cancer_Detection_App/+\", width=900, height=750)"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "",
    "text": "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as anything rude, disrespectful, or otherwise likely to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#problem-statement",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#problem-statement",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "",
    "text": "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as anything rude, disrespectful, or otherwise likely to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#dataset-description",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#dataset-description",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Dataset Description",
    "text": "Dataset Description\nAs part of the competition, competitors were provided several files, specifically:\njigsaw-toxic-comment-train.csv - data from the Jigsaw toxic comment classification competition. The dataset is made up of English comments from Wikipedia’s talk page edits.\njigsaw-unintended-bias-train.csv - data from the Jigsaw Unintended Bias in Toxicity Classification competition. This is an expanded version of the Civil Comments dataset with a range of additional labels.\nsample_submission.csv - a sample submission file.\ntest.csv - comments from Wikipedia talk pages in different non-English languages.\nvalidation.csv - comments from Wikipedia talk pages in different non-English languages"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#evaluation-metric",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#evaluation-metric",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Evaluation Metric",
    "text": "Evaluation Metric\nSubmissions were evaluated based on Area Under the ROC Curve between the predicted probability and the observed target."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#strategies-to-tackle",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#strategies-to-tackle",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Strategies to Tackle",
    "text": "Strategies to Tackle\n\nMonolingual Approach\nMonolingual models are the type of language models which are trained on a single language.\nThey are focused on understanding and generating text in a specific language.\nFor example, a monolingual model trained on English language will be proficient in understanding and generating English text.\nThese models are typically for tasks such as text classification, sentiment analysis and more within a specific language.\nMonolingual models can be beneficial to utilize when we have a specific language in our training, testing datasets and in the upcoming unseen data.\n\n\nMultilingual Approach\nMultilingual models are, on the other hand, are trained on multiple different languages.\nThey are designed to handle and process text in multiple languages, allowing them to perform across different languages.\nMultilingual models have the advantage of of being able to provide language-agnostic solutions, as they can handle a wide-range of languages.\nThey can be used for zero-shot and few-shot learning, where the model can perform a task in a language it has not been seen specifically trained on by leveraging its knowledge of other languages.\n\n\nWhich models to use for our problem?\nAs per the dataset given in the competition, we have only english data in our training dataset and very samples are given in the validation dataset containing languages Spanish, Turkish and Italian only and the Testing dataset contains languages Turkish, Spanish, Italian, Russian, French and Portugese.\nSince in our validation and test dataset contains non-english languages it would be better approach to build multilingual models rather than monolingual models.\nNow, if we had only one language (as stated above) building monolingual models would be a better choice.\nLet’s discuss Multilingual models approach a bit more:\nHow are multilingual models are trained?\nMultilingual models are pre-trained on a mix of different languages and they don’t distinguish between the languages.\nThe English BERT was pre-trained on English Wikipedia and BookCorpus dataset, while multilingual models like mBERT was pre-trained on 102 different languages from largest Wikipedia dataset and XLM-Roberta was pre-trained on CommonCrawl dataset from 100 different languages respectively.\n\n\nCross Lingual Transfer\nCross-lingual transfer refers to transfer learning using data and models available for one language for which ample such resources are available (e.g., English) to solve tasks in another, commonly more low-resource, language.\nIn our case, we are trying to create an application that can automatically detect whether a sentence or phrase is toxic or not.\nModels like XLM-Roberta provides us the ability to fine tune it on English dataset and predict to identify comments in any different language.\nXLM-R is able to take it’s specific knowledge learnt in one language and apply it to a different langauge (languages), even though it never seen the language during fine-tuning.\nThis concept is of transfer learning applied from one language to another language is referred to as Cross-Lingual Transfer (AKA Zero-Shot Learning) .\nAnother reason to use Pre-Trained multi-lingual models for a task like this (as in our case) is that is the Lack of languages by resources i.e., different languages have different amounts of training data available to build models like BERT and its variants.\nSome languages like English, Chinese, Russian, Indonesian, Vietnamese etc. are the languages that have high resource languages, whereas languages like sundanese, assamese etc. are low resource languages.\nTraining our own BERT like model on these low resources could be very expensive in terms of data collection and performance-wise , therefore, We should leverage these multi-lingual models.\n\n\nWhat experiments did I perform ?\nAt the Overall level, I performed 9 experiments with the following ideas keeping in mind.\nPerform Pre-processing techniques like removal of stopwords, removing URLs, Contraction to Expansion of words, removing multiple characters from words and removal of punctuations.\nFrom model stand point we experimented with 2 models mBERT & XLM-Roberta.\nFrom Training dataset perspective we used 2 types of datasets: one case where we used the provided training datasets where we tried to balance the dataset by the target and the other case where we used the translated training dataset of languages provided in the test dataset along with the english language with class balancing.\nWe always trained on validation dataset as well to further boost the performance of the model.\nNow we build a model with the following ideas: &gt; Training on original training & validation datasets, class balancing (undersampling), fine-tuning the model for 2 epochs on training as well as validation dataset, and will not perform any preprocessing dataset. We will be leveraging the TPUs offered by Kaggle."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#installing-libraries",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#installing-libraries",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n!pip install nltk\n!pip install transformers --quiet\n\nimport re\nimport nltk\nimport string\nimport os, gc\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nnltk.download('stopwords')\n\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 19.0 MB/s eta 0:00:0000:010:01\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\nCollecting regex&gt;=2021.8.3\n  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 KB 38.4 MB/s eta 0:00:00\nInstalling collected packages: regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2023.5.5\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n\n\nD0508 07:23:28.285075581      14 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0508 07:23:28.285109420      14 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0508 07:23:28.285113692      14 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0508 07:23:28.285116753      14 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0508 07:23:28.285119508      14 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0508 07:23:28.285122651      14 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0508 07:23:28.285125779      14 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0508 07:23:28.285136675      14 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0508 07:23:28.285139712      14 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0508 07:23:28.285142515      14 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0508 07:23:28.285145320      14 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0508 07:23:28.285148256      14 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0508 07:23:28.285151131      14 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0508 07:23:28.285153928      14 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0508 07:23:28.285344519      14 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD0508 07:23:28.297613061      14 ev_posix.cc:144]                      Using polling engine: epoll1\nD0508 07:23:28.297635890      14 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0508 07:23:28.297978050      14 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0508 07:23:28.297989261      14 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0508 07:23:28.297993453      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0508 07:23:28.297997280      14 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0508 07:23:28.298001081      14 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0508 07:23:28.298004402      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0508 07:23:28.298011355      14 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0508 07:23:28.298027677      14 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0508 07:23:28.298054234      14 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0508 07:23:28.298068190      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0508 07:23:28.298072029      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0508 07:23:28.298076040      14 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0508 07:23:28.298082495      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0508 07:23:28.298086497      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0508 07:23:28.298090079      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0508 07:23:28.298093976      14 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0508 07:23:28.300184472      14 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0508 07:23:28.318782434     242 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0508 07:23:28.326237166     242 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-05-08T07:23:28.326222349+00:00\"}\n/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#setting-data-paths",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#setting-data-paths",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Setting data paths",
    "text": "Setting data paths\n\nmain_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\ntoxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\nunintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\nvalidation_csv_path = main_data_dir_path + \"validation.csv\"\ntest_csv_path = main_data_dir_path + \"test.csv\"\nsubmission_csv_path = main_data_dir_path + \"sample_submission.csv\""
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#tpu-configurations",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#tpu-configurations",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "TPU Configurations",
    "text": "TPU Configurations\nIntializing the TPU configurations and other constants like number of epochs, batch_size (16 * number of cores offered on TPUS), MAX_LEN (length of the sentence), we use xlm-roberta-large model, number of samples (for undersampling) = 150k, Learning_rate = 1e-5 etc.\n\n#################### TPU Configurations ####################\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'xlm-roberta-large'\nNUM_SAMPLES = 150000\nRANDOM_STATE = 42\nLEARNING_RATE = 1e-5 ######################### MAIN CHANGE ############################\nWEIGHT_DECAY = 1e-6\n\nRunning on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS:  8"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#reading-balancing-the-data-by-target-column",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#reading-balancing-the-data-by-target-column",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Reading & Balancing the data by Target column",
    "text": "Reading & Balancing the data by Target column\n\n## Reading csv files \ntrain1 = pd.read_csv(toxic_comment_train_csv_path)\ntrain2 = pd.read_csv(unintended_bias_train_csv_path)\nvalid = pd.read_csv(validation_csv_path)\ntest = pd.read_csv(test_csv_path)\nsub = pd.read_csv(submission_csv_path)\n\n## Converting floating points to integers ##\ntrain2.toxic = train2['toxic'].round().astype(int)\n\n##### BALANCING THE DATA ##### \n# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n# & sampling 150k observations randomly from non-toxic observation population.\n\n# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n])\n\n## Dropping missing observations with respect to comment-text column \ntrain = train.dropna(subset=['comment_text'])\n\n\ndef encode(texts, tokenizer, max_len):\n    \"\"\"\n    Function takes a list of texts, tokenizer (object)\n    initialized from HuggingFace library, max_len (defines\n    of how long the sentence lengths should be).\n    \"\"\"       \n    tokens = tokenizer(texts, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='np')\n    \n    return tokens"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#encoding-comment_text",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#encoding-comment_text",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Encoding comment_text",
    "text": "Encoding comment_text\nWe first initialize the tokenizer from Hugging Face transformer library and encoding our training, validation and test dataset comment_texts.\n\n## Intializing the tokenizer ##\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_inputs = encode(train['comment_text'].values.tolist(), \n                      tokenizer, max_len=MAX_LEN)\nvalidation_inputs = encode(valid['comment_text'].values.tolist(),\n                          tokenizer, max_len=MAX_LEN)\ntest_inputs = encode(test['content'].values.tolist(),\n                    tokenizer, max_len=MAX_LEN)\n\nDownloading (…)lve/main/config.json: 100%|██████████| 616/616 [00:00&lt;00:00, 138kB/s]\nDownloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00&lt;00:00, 41.6MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00&lt;00:00, 57.5MB/s]"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#preparing-data-using-tf.data.data-api",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#preparing-data-using-tf.data.data-api",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Preparing data using tf.data.Data API",
    "text": "Preparing data using tf.data.Data API\nWriting a function to create a tuple of inputs and outputs, where inputs have a dictionary datatype.\nWe’ll be leveraging tf.data.Data API to pass our inputs and outputs as tuple, i.e., (inputs, outputs), inputs are {\"input_ids\": input_ids, \"attention_mask\": attention_mask} and outputs labels.\n\ndef map_fn(input_ids, attention_mask, labels=None):\n    if labels is not None:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n    else:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n                                                    train_inputs[\"attention_mask\"],\n                                                   train['toxic']))\ntrain_dataset = train_dataset.map(map_fn)\ntrain_dataset = train_dataset.repeat().shuffle(buffer_size=2048,seed=RANDOM_STATE).batch(BATCH_SIZE).prefetch(AUTO)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n                                                         validation_inputs['attention_mask'],\n                                                        valid['toxic']))\nvalidation_dataset = validation_dataset.map(map_fn)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n                                                  test_inputs['attention_mask']))\ntest_dataset = test_dataset.map(map_fn)\ntest_dataset = test_dataset.batch(BATCH_SIZE)"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#building-the-model",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#building-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Building the model",
    "text": "Building the model\n\ndef build_model(transformer_layer, max_len):\n    \"\"\"\n    Creating the model input layers, output layers,\n    model definition and compilation.\n        \n    Returns: model object after compiling. \n    \"\"\"\n    input_ids = tf.keras.layers.Input(shape=(max_len,), \n                                      dtype=tf.int32, \n                                      name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n                                       dtype=tf.int32, \n                                       name=\"attention_mask\")\n    output = transformer_layer.roberta(input_ids, \n                                 attention_mask=attention_mask)[1]\n    x = tf.keras.layers.Dense(1024, activation='relu')(output)\n    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n                             outputs=y)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    AUC = tf.keras.metrics.AUC()\n    \n    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n    return model"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-model-on-tpus",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-model-on-tpus",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Loading model on TPUs",
    "text": "Loading model on TPUs\nIt is important to initialize & compile the model inside the with strategy.scope().\nOne thing I want to point out that for some reason I was getting different results even though I was setting the seed before initializing the model, but the results are always consistent even though the results differ very little every time we run the pipeline.\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    tf.random.set_seed(RANDOM_STATE)\n    model = build_model(transformer_layer,\n                        max_len=MAX_LEN)\nmodel.summary()\n\nDownloading tf_model.h5: 100%|██████████| 2.24G/2.24G [00:46&lt;00:00, 48.0MB/s]\nAll model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 192)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 192)]        0           []                               \n                                                                                                  \n roberta (TFXLMRobertaMainLayer  TFBaseModelOutputWi  559890432  ['input_ids[0][0]',              \n )                              thPoolingAndCrossAt               'attention_mask[0][0]']         \n                                tentions(last_hidde                                               \n                                n_state=(None, 192,                                               \n                                 1024),                                                           \n                                 pooler_output=(Non                                               \n                                e, 1024),                                                         \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n dense (Dense)                  (None, 1024)         1049600     ['roberta[0][1]']                \n                                                                                                  \n outputs (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 560,941,057\nTrainable params: 560,941,057\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-only-english-data-for-2-epochs",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-only-english-data-for-2-epochs",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Training the model on Only English data for 2 epochs",
    "text": "Training the model on Only English data for 2 epochs\n\ntrain_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset,\n                         steps_per_epoch=train_steps_per_epoch,\n                         validation_data=validation_dataset,\n                         epochs=2) \n\nEpoch 1/2\n3795/3795 [==============================] - ETA: 0s - loss: 0.0551 - auc: 0.99703795/3795 [==============================] - 1609s 378ms/step - loss: 0.0551 - auc: 0.9970 - val_loss: 0.4696 - val_auc: 0.8942\nEpoch 2/2\n3795/3795 [==============================] - 1399s 369ms/step - loss: 0.0450 - auc: 0.9979 - val_loss: 0.3125 - val_auc: 0.9083\n\n\n/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n  warnings.warn(\n2023-05-08 07:28:46.072958: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n2023-05-08 07:28:48.332211: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n2023-05-08 07:53:43.374173: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-05-08 07:53:43.934692: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-validation-data-for-2-epochs-further-to-fine-tune-on-it",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-validation-data-for-2-epochs-further-to-fine-tune-on-it",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Training the model on Validation data for 2 epochs further to fine-tune on it",
    "text": "Training the model on Validation data for 2 epochs further to fine-tune on it\n\nvalidation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\nvalidation_history = model.fit(validation_dataset.repeat(),\n                              steps_per_epoch=validation_steps_per_epoch,\n                              epochs=2)\n\nEpoch 1/2\n62/62 [==============================] - 23s 367ms/step - loss: 0.2286 - auc: 0.9315\nEpoch 2/2\n62/62 [==============================] - 107s 365ms/step - loss: 0.1472 - auc: 0.9726\n\n\n\nPublic LeaderBoard score on kaggle (test dataset): 0.936 and Private LeaderBoard score : 0.9346"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#results",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#results",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\nExperiment\nPublic Test LeaderBoard Score\nPrivate Test LeaderBoard Score\n\n\n\n\n1 (mBERT + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8850\n0.8869\n\n\n2 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.9259\n0.9264\n\n\n3 (mBERT + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8259\n0.8239\n\n\n4 (xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8755\n0.8754\n\n\n5 (mBERT + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.9195\n0.9212\n\n\n6 ((xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.9329\n0.9212\n\n\n7 (mBERT + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.8696\n0.9212\n\n\n8 ((xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.8861\n0.8866\n\n\n9 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.936\n0.9346"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#predicting-on-test-dataset",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#predicting-on-test-dataset",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Predicting on Test dataset",
    "text": "Predicting on Test dataset\n\nsub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)\n\n2023-05-08 08:20:04.172862: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-05-08 08:20:04.668074: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n\n\n499/499 [==============================] - 80s 119ms/step\n\n\n\nsub.head()\n\n\n\n\n\n\n\n\nid\ntoxic\n\n\n\n\n0\n0\n0.000308\n\n\n1\n1\n0.000241\n\n\n2\n2\n0.266148\n\n\n3\n3\n0.000063\n\n\n4\n4\n0.000078"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#saving-the-model",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#saving-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Saving the model",
    "text": "Saving the model\n\nmodel_save_path = \"../working/Multilingual_toxic_comment_classifier\"\nmodel.save(model_save_path)\n\nWARNING:absl:Found untraced functions such as _update_step_xla, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 829). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets\n\n\nINFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-the-model",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Loading the model",
    "text": "Loading the model\n\nimport tensorflow as tf\n\nmodel_save_path = \"../working/Multilingual_toxic_comment_classifier\"\nloaded_model = tf.keras.models.load_model(model_save_path)\ny = loaded_model.predict(test_dataset.take(1))\ny[:6]\n\n1/1 [==============================] - 37s 37s/step\n\n\narray([[3.0799874e-04],\n       [2.2472920e-04],\n       [2.6646560e-01],\n       [5.7183450e-05],\n       [7.6287179e-05],\n       [3.1223629e-02]], dtype=float32)\n\n\nWriting the function to prepare for the new text, we encode the text using the tokenizer with the sentence length=192\n\nfrom transformers import AutoTokenizer\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\ntext = \"politicians are like cancer for this country\"\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(text, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='tf')\n    \n    return {\"input_ids\": tokens['input_ids'],\n            \"attention_mask\": tokens['attention_mask']}\n\nPredicting the probability of toxic and non-toxic on a sample text.\n\nprob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\nprob_of_non_toxic_comment = 1 - prob_of_toxic_comment\nprob_of_toxic_comment, prob_of_non_toxic_comment\nprobs = {\"prob_of_toxic_comment\": prob_of_toxic_comment,\n \"prob_of_non_toxic_comment\": prob_of_non_toxic_comment}\nprobs\n\n1/1 [==============================] - 9s 9s/step\n\n\n{'prob_of_toxic_comment': 0.26497197,\n 'prob_of_non_toxic_comment': 0.7350280284881592}\n\n\n\nTesting the model with the Gradio App before final pushing the model to HuggingFace Spaces\n\n!pip3 install gradio --quiet\nimport tensorflow as tf\nimport gradio as gr\n\nloaded_model = tf.keras.models.load_model(model_save_path)\n\nfrom transformers import AutoTokenizer\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\nexamples_list = [\"politicians are like cancer for this country\", \n                 \"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было,\",\n                \"Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хох\",\n                ]\n\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(text, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='tf')\n    \n    return {\"input_ids\": tokens['input_ids'],\n            \"attention_mask\": tokens['attention_mask']}\n\ndef predict(text):\n    prob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\n    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n    prob_of_toxic_comment, prob_of_non_toxic_comment\n    probs = {\"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n             \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment)}\n    return probs\n\ninterface = gr.Interface(fn=predict, inputs=gr.components.Textbox(lines=4,label='Comment'),\n                        outputs=[gr.Label(label='Probabilities')], examples=examples_list,\n                        title='Multi-Lingual Toxic Comment Classification.',\n                        description='XLM-Roberta Large model')\ninterface.launch(debug=False, share=True)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\nRunning on local URL:  http://127.0.0.1:7865\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRunning on public URL: https://af370decb4339b429e.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n1/1 [==============================] - 8s 8s/step\n1/1 [==============================] - 1s 530ms/step\n1/1 [==============================] - 1s 513ms/step\n\n\n\n\n\n\n\n\nWoow!!!\nOur application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting, we can upload our Gradio app Interface to HuggingFace Spaces.\nNow download all the files and folders from kaggle output manually & this kaggle kernel locally"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-multi-lingual-toxic-comment-classification-gradio-demo-into-a-deployable-app",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-multi-lingual-toxic-comment-classification-gradio-demo-into-a-deployable-app",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Turning our Multi-Lingual Toxic Comment Classification Gradio Demo into a deployable app",
    "text": "Turning our Multi-Lingual Toxic Comment Classification Gradio Demo into a deployable app\nWe’ll deploy the demo application on HuggingFace Spaces.\nWhat is HuggingFace Spaces?\nIt is a resource that allows anybody to host and share machine learning application.\n\nDeployed Gradio App Structure\nTo upload our gradio app, we’ll want to put everything together into a singe directory.\nFor example, our demo might live at the path demos/melanoma_skin_cancer_files with the following structure:\ndemos/\n    └── multilingual_toxic_comment_files/\n        ├── Multilingual_toxic_comment_classifier/\n        │   ├── variable/\n        │   │   ├── variables.data-00000-of-00001\n        │   │   └── variables.index\n        │   ├── fingerprint.pb\n        │   ├── keras_metadata.pb\n        │   └── saved_model.pb \n        ├── app.py\n        ├── examples/\n        │   └── dataset\n        └── requirements.txt\nWhere: - Multilingual_toxic_comment_classifier is our saved fine-tuned model (binary files associated). - app.py contains our Gradio app, our data preprocessing function and our predict function. Note: app.py is the default filename used for HuggingFace Spaces, if we deploy our apps there. - examples contains sample dataframe which contains toxic & non-toxic comments from russian, spanish, english, italian, turkish, portugese and last french languages to showcase the demo of our Gradio application. - requirements.txt file contains the dependencies/packages to run our application such as tensorflow, gradio, transformers.\n\n\nCreating a demo folder to store our Multilingual Toxic Comment Classifier App files\nTo begin, we’ll create an empty directory demos/ that will contain all our necessary files for the application.\nWe can achive this using Python’s pathlib.Path(\"path_of_dir\") to establish directory path and then pathlib.Path(\"path_of_dir\").mkdir() to create it.\n\n############### ROOT_DIR : I Have put the files in my E: drive\n## Importing Packages \nimport shutil\nfrom pathlib import Path\nimport os\n\nROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n\n## Create Melanoma skin cancer demo path\nmultilingual_toxic_comment_demo_path = Path(f\"{ROOT_DIR}/demos/multilingual_toxic_comment_files\")\n\n## Removing files that might already exist and creating a new directory.\nif multilingual_toxic_comment_demo_path.exists():\n    shutil.rmtree(multilingual_toxic_comment_demo_path)\n    multilingual_toxic_comment_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n                                exist_ok=True) # Create even if they already exists? \nelse:\n    ## If the path doesn't exist, create one \n    multilingual_toxic_comment_demo_path.mkdir(parents=True,\n                                exist_ok=True)\n\n\n\nCreating a folder of example images to use with our Melanoma skin cancer demo\nNow we’ll create an empty directory called examples and store a sample dataset containing comments from the Russian, Turkish, English, Spanish, Portugese, French, Italian languages. I have collected these comments from online and created a CSV file for them.\nTo do so we’ll:\n\nCreate an empty directory examples/ within the demos/multilingual_toxic_comment_files directory.\nCollect some comment samples from online in these languages and create a CSV file out of them containing both toxic as well as non-toxic comments.\n\n\nimport pandas as pd\nfrom pathlib import Path\n\n## Create examples directory\nmultilingual_toxic_comment_examples_path = multilingual_toxic_comment_demo_path / \"examples\"\nmultilingual_toxic_comment_examples_path.mkdir(parents=True, exist_ok=True)\n\nsample_comments = Path(f\"sample_comments.csv\")\n\ncomments = {\"comment_text\": [\"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\",\n                 \"Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\",\n                 \"В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.\",\n                 \"Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.\",\n                 \"This is such an urgent design problem; kudos to you for taking it on. Very impressive!\",\n                 \"haha you guys are a bunch of losers.\", \"ur a sh*tty comment.\",\n                 \"Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!\",\n                 'SIAMO ALLA FOLLIA', \n                 '20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg ',\n                 \"Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) \"]}\n\npd.DataFrame(comments, \n             columns=['comment_text']).to_csv(multilingual_toxic_comment_examples_path / sample_comments,\n                                              index=False)\n\nNow we verify our example images are present, let’s list the contents of our demo/melanoma_skin_cancer/examples/ directory with os.listdir() and then format the filepaths into a list of lists (to make it compatible with the Gradio’s gradio.Interface(), example parameter).\n\nexample_list = [[example] for example in pd.read_csv(multilingual_toxic_comment_examples_path / sample_comments)['comment_text'].tolist()]\nexample_list\n\n[['Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.'],\n ['Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?'],\n ['В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.'],\n ['Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.'],\n ['This is such an urgent design problem; kudos to you for taking it on. Very impressive!'],\n ['haha you guys are a bunch of losers.'],\n ['ur a sh*tty comment.'],\n ['Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!'],\n ['SIAMO ALLA FOLLIA'],\n ['20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg '],\n ['Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) ']]\n\n\n\n\nMoving our trained XLM-Roberta model binary files into our multilingual_toxic_comment_files demo directory.\nWe have saved our fine-tuned model in outout/working/multilingual_toxic_comment_files/ directory and we’ll move our model files to demos/multilingual_toxic_comment_files/ directory as specified above.\nWe use Python’s shutil.move() method and passing in src(the source path of the target file) and dst (the destination folder path of the target file to be moved into) parameters.\n\n## Importing Libraries\nimport shutil\n\n## Create a source path for our target model\nmultilingual_toxic_comment_model_dir_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Multilingual_toxic_comment_classifier\\\\\"\n\n## Create a destination path for our target model\nmultilingual_toxic_comment_model_dir_destination = multilingual_toxic_comment_demo_path\n\n## Try to move the file\ntry:\n    print(f\"Attempting to move the {multilingual_toxic_comment_model_dir_path} to {multilingual_toxic_comment_model_dir_destination}\")\n    \n    ## Move the model\n    shutil.move(src=multilingual_toxic_comment_model_dir_path,\n           dst=multilingual_toxic_comment_model_dir_destination)\n    \n    print(\"Model move completed\")\n## If the model has already been moved, check if it exists\nexcept:\n    print(f\"No model found at {multilingual_toxic_comment_model_dir_path}, perhaps it's already moved.\")\n    print(f\"Model already exists at {multilingual_toxic_comment_model_dir_destination}: {multilingual_toxic_comment_model_dir_destination.exists()}\")\n\nAttempting to move the E:\\MultiLingual-Toxic-Comment-Classification\\output\\working\\Multilingual_toxic_comment_classifier\\ to E:\\MultiLingual-Toxic-Comment-Classification\\demos\\multilingual_toxic_comment_files\nModel move completed"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-gradio-app-into-a-python-script-app.py",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-gradio-app-into-a-python-script-app.py",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Turning our Gradio App into a Python Script (app.py)",
    "text": "Turning our Gradio App into a Python Script (app.py)\n\n## Now if we look into which directory we are currently, we'll find that using the following code\nimport os\nos.getcwd()\n\n'E:\\\\MultiLingual-Toxic-Comment-Classification\\\\notebooks'\n\n\nNow we will move into the demos directory where we will write some helper utilities.\nIn cd ../demos/: .. means we are moving outside of the notebooks directory. demos/: means we moving inside the demos directory.\n\ncd ../demos/\n\nE:\\MultiLingual-Toxic-Comment-Classification\\demos\n\n\n\nimport tensorflow as tf\nimport gradio as gr\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\nmodel_save_path = \"Multilingual_toxic_comment_classifier/\"\n### Loading the fine-tuned model ###\nloaded_model = tf.keras.models.load_model(model_save_path)\n### Initializing the tokenizer ###\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\nexamples_list = [\n    [example]\n    for example in pd.read_csv(\"examples/sample_comments.csv\")[\"comment_text\"].tolist()\n]\n\n\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(\n        text,\n        max_length=max_len,\n        truncation=True,\n        padding=\"max_length\",\n        add_special_tokens=True,\n        return_tensors=\"tf\",\n    )\n\n    return {\n        \"input_ids\": tokens[\"input_ids\"],\n        \"attention_mask\": tokens[\"attention_mask\"],\n    }\n\n\ndef predict(text):\n    prob_of_toxic_comment = loaded_model.predict(\n        prep_data(text=text, tokenizer=tokenizer_, max_len=192)\n    )[0][0]\n    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n    prob_of_toxic_comment, prob_of_non_toxic_comment\n    probs = {\n        \"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n        \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment),\n    }\n    return probs\n\n\ninterface = gr.Interface(\n    fn=predict,\n    inputs=gr.components.Textbox(lines=4, label=\"Comment\"),\n    outputs=[gr.Label(label=\"Probabilities\")],\n    examples=examples_list,\n    title=\"Multi-Lingual Toxic Comment Classification.\",\n    description=\"XLM-Roberta Large model\",\n)\ninterface.launch(debug=False)\n\nOverwriting multilingual_toxic_comment_files/app.py\n\n\n\nCreating a requirements.txt file for our Gradio App(requirements.txt)\nThis is the last file we need to create for our application.\nThis file contains all the necessary packages for our Gradio application.\nWhen we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n\ntensorflow==2.12\npandas==1.5.2\ngradio==3.1.4\ntransformers==4.28.1\n\n\n%%writefile multilingual_toxic_comment_files/requirements.txt\ntensorflow==2.12\npandas==1.5.2\ngradio==3.1.4\ntransformers==4.28.1\n\nOverwriting multilingual_toxic_comment_files/requirements.txt"
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#deploying-our-application-to-huggingface-spaces",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#deploying-our-application-to-huggingface-spaces",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Deploying our Application to HuggingFace Spaces",
    "text": "Deploying our Application to HuggingFace Spaces\nTo deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n\nUploading via the Hugging Face Web Interface (easiest)\nUploading via the command line or terminal\n\nNOTE: To host any application on HuggingFace, we first need to sign up for a free HuggingFace Account\n\nRunning our Application locally\n\nOpen the terminal or command prompt.\nChanging the multilingual_toxic_comment_files directory (cd multilingual_toxic_comment_files).\nCreating an environment (python3 -m venv env) or use (python -m venv env).\nActivating the environment (source env/Scripts/activate).\nInstalling the requirements.txt using pip install -r requirements.txt. &gt; If faced any errors, we might need to upgrade pip using pip install --upgrade pip.\n\nRun the app (python3 app.py).\n\nThis should results in a Gradio demo locally at the URL such as : http://127.0.0.1:7860/.\n\n\nUploading to Hugging Face\nWe’ve verified our Melanoma_skin_cancer detection application is working in our local system.\nTo upload our application to Hugging Face Spaces, we need to do the following.\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to our profile at the top right corner and then select New Space.\nDeclare the name to the space like Chirag1994/multilingual_toxic_comment_classification_app.\nSelect a license (I am using MIT license).\nSelect Gradio as the Space SDK (software development kit).\nChoose whether your Space is Public or Private (I am keeping it Public).\nClick Create Space.\nClone the repository locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in the terminal or command prompt. In our case mine would be like - git clone https://huggingface.co/spaces/Chirag1994/multilingual_toxic_comment_classification_app.\nCopy/Move the contents of the downloaded multilingual_toxic_comment_classification_app folder to the cloned repo folder.\nTo upload files and track larger files (e.g., files that are greater than 10MB) for them we need to install Git LFS which stands for Git large File Storage.\nOpen up the cloned directory using VS code (I’m using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command git lfs install to start tracking the file that we want to track. For example - git lfs track \"Multilingual_toxic_comment_classifier\" directory files.\nCreate a new .gitignore file and the files & folders that we don’t want git to track like :\n\n__pycache__/\n.vscode/\nvenv/\n.gitignore\n.gitattributes\n\nAdd the rest of the files and commit them with:\n\ngit add .\ngit commit -m \"commit message that you want\"\n\nPush(load) the files to Hugging Face\n\ngit push\n\nIt might a couple of minutes to finish and then the app will be up and running."
  },
  {
    "objectID": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#our-final-application-deployed-on-huggingface-spaces",
    "href": "portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#our-final-application-deployed-on-huggingface-spaces",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs.",
    "section": "Our Final Application deployed on HuggingFace Spaces",
    "text": "Our Final Application deployed on HuggingFace Spaces\n\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://chirag1994-multilingual-toxic-comment-classifier.hf.space\", width=1000, height=800)"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nCase Study #7 - Balanced Tree.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the second case study using MySQL from 8weeksqlchallenge.com. We’ll Explore the Pizza Runner case study with us, delving into the data to optimize…\n\n\n\nChirag Sharma\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #6 - CliqueBait.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the second case study using MySQL from 8weeksqlchallenge.com. We’ll Explore the Pizza Runner case study with us, delving into the data to optimize…\n\n\n\nChirag Sharma\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #5 - Data Mart.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the second case study using MySQL from 8weeksqlchallenge.com. We’ll Explore the Pizza Runner case study with us, delving into the data to optimize…\n\n\n\nChirag Sharma\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #3 - Foodie Fi.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the third case study using MySQL from 8weeksqlchallenge.com. We’ll explore Foodie-Fi’s data-driven journey, deciphering user engagement patterns…\n\n\n\nChirag Sharma\n\n\nJan 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #2 - Pizza Runner.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the second case study using MySQL from 8weeksqlchallenge.com. We’ll Explore the Pizza Runner case study with us, delving into the data to optimize…\n\n\n\nChirag Sharma\n\n\nJan 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #1 - Danny’s Diner.\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the first case study using MySQL from 8weeksqlchallenge.com. We’ll help Danny’s Diner by figuring out when people visit, what they like, and how…\n\n\n\nChirag Sharma\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMelanoma Skin Cancer Detection using Transfer Learning with PyTorch.\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nPyTorch\n\n\nGradio\n\n\nHuggingFace Spaces\n\n\n\nIn this post, we’ll use melanoma images, from the Kaggle Competition Dataset, of skin lesions to determine which images are likely to represent a melanoma. Melanoma is a…\n\n\n\nChirag Sharma\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultilingual Toxic Comment Classification using Tensorflow and TPUs.\n\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nTensorflow\n\n\nTPUs\n\n\nGradio\n\n\nHuggingFace Spaces\n\n\n\nIn this post, we’ll create an application that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely…\n\n\n\nChirag Sharma\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]