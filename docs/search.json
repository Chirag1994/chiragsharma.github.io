[
  {
    "objectID": "computer_vision_portfolio.html",
    "href": "computer_vision_portfolio.html",
    "title": "Computer Vision Portfolio",
    "section": "",
    "text": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch\n\n\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\n\n\n\nJan 9, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chirag Sharma",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana.\n\n\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011\n\n\n\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Chirag Sharma",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Chirag Sharma",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "nlp_&_generativeai_portfolio.html",
    "href": "nlp_&_generativeai_portfolio.html",
    "title": "NLP & GenerativeAI Portfolio",
    "section": "",
    "text": "Multilingual Toxic Comment Classification using Tensorflow and TPUs\n\n\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\n\n\n\nJan 9, 2023\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\n\nCode\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-directories-to-store-python-scripts-and-model",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "",
    "text": "We will first create 2 empty folders - Sripts (this will contain all the python modules for the trainig, validation & testing the model etc.) and Models (which will contain model checkpoint)\nNote that we are running this notebook from this stage in kaggle kernel. To build/Replicate the model, follow the exact same steps mentioned in the notebook.\nWe first build our model in kaggle kernel because of the free computational resources, one can use google colab (free version) but it has certain limitations like it cannot handle the image size like we’ll be using in this project.\nSo we’ll utilize the kaggle computation resources to carry out this project.\n\n\nCode\nimport os\nfrom pathlib import Path\n\n## Creating Empty folders\nscripts_file_path = Path(\"Scripts\")\nmodels_file_path = Path('Models')\nscripts_file_path.mkdir(parents=True, exist_ok=True)\nmodels_file_path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-python-modules-in-scripts-for-training-and-prediction",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating Python modules in Scripts for training and prediction",
    "text": "Creating Python modules in Scripts for training and prediction\nFirst we will write/create python modules like for augmentations, config, training & validation loops, prediction_to_generate_on_test_dataset etc.\nFor training augmentations we’ll be using like flipping the image, creating random patches in the image, randomly rotating 90 degrees, Adjusting the brightness and contrast, adding noise in the images, Shifting and sheering the image and finally normalizing the statistics of the image (since we will be using transfer learning therefore we need to prepare the images in the same way they were trained on - depending on the specific model we want to use).\n\n\nCode\n%%writefile Scripts/augmentations.py\nfrom Scripts.config import Config\nimport albumentations as A\n\ntraining_augmentations = A.Compose(\n    [\n        A.CoarseDropout(p=0.6),\n        A.RandomRotate90(p=0.6),\n        A.Flip(p=0.4),\n        A.OneOf(\n            [\n                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3),\n                A.HueSaturationValue(\n                    hue_shift_limit=20, sat_shift_limit=60, val_shift_limit=50\n                ),\n            ],\n            p=0.7,\n        ),\n        A.OneOf([A.GaussianBlur(), A.GaussNoise()], p=0.65),\n        A.ShiftScaleRotate(\n            shift_limit=0.0625, scale_limit=0.35, rotate_limit=45, p=0.5\n        ),\n        A.OneOf(\n            [\n                A.OpticalDistortion(p=0.3),\n                A.GridDistortion(p=0.1),\n                A.PiecewiseAffine(p=0.3),\n            ],\n            p=0.7,\n        ),\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        ),\n    ]\n)\n\nvalidation_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\ntesting_augmentations = A.Compose(\n    [\n        A.Normalize(\n            mean=Config.MEAN, std=Config.STD, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\n\n\nWriting Scripts/augmentations.py\n\n\nCreating a config module which will contain model configurations like number of epochs to run, size of an image, weight decay (for regularization) etc., it also contains the path of the files and folders of the data.\n\n\nCode\n%%writefile Scripts/config.py\nimport torch\n\nclass Config:\n    EPOCHS = 5\n    IMG_SIZE = 512\n    ES_PATIENCE = 2\n    WEIGHT_DECAY = 0.001\n    VAL_BATCH_SIZE = 32 * 2\n    RANDOM_STATE = 1994\n    LEARNING_RATE = 5e-5\n    TRAIN_BATCH_SIZE = 32\n    MEAN = (0.485, 0.456, 0.406)\n    STD = (0.229, 0.224, 0.225)\n    TRAIN_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n        \"target\",\n        \"tfrecord\",\n    ]\n    TEST_COLS = [\n        \"image_name\",\n        \"patient_id\",\n        \"sex\",\n        \"age_approx\",\n        \"anatom_site_general_challenge\",\n    ]\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    ################ Setting paths to data input ################\n\n    data_2020 = \"../input/jpeg-melanoma-512x512/\"\n    train_folder_2020 = data_2020 + \"train/\"\n    test_folder_2020 = data_2020 + \"test/\"\n    test_csv_path_2020 = data_2020 + \"test.csv\"\n    train_csv_path_2020 = data_2020 + \"train.csv\"\n    submission_csv_path = data_2020 + \"sample_submission.csv\"\n\n\nWriting Scripts/config.py\n\n\nCreating a single dataset class to read the images (both training, validation & testing images), the function is capable of handling/reading the tabular features.\nThe function takes a dataframe, a list of tabular features (if we want to use for training) i.e., list of strings like ['sex_missing',anatom_site_general_challenge_head_neck','anatom_site_general_challenge_lower_extremity',     anatom_site_general_challenge_torso','anatom_site_general_challenge_upper_extremity','scaled_age'] , the augmentations we want to use and finally whether the dataset is a training, validation or testing dataset.\nFor training and validation we set is_test=False and for testing we set is_test=True to differentiate between the datasets.\n\n\nCode\n%%writefile Scripts/dataset.py\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom PIL import Image\nfrom PIL import ImageFile\nfrom typing import List, Callable\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass DatasetRetriever(nn.Module):\n    \"\"\"\n    Dataset class to read the images and tabular features from a\n    dataframe and returns the dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        tabular_features: List[str] = None,\n        use_tabular_features: bool = False,\n        augmentations: Callable = None,\n        is_test: bool = False,\n    ):\n        \"\"\" \"\"\"\n        self.df = df\n        self.tabular_features = tabular_features\n        self.use_tabular_features = use_tabular_features\n        self.augmentations = augmentations\n        self.is_test = is_test\n\n    def __len__(self):\n        \"\"\"\n        Function returns the number of images in a dataframe.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Function the takes an images and it's corresponding\n        tabular/meta features & target feature (for training\n        and validation) and returns a dictionary, otherwise,\n        for test dataset it only returns a dictionary of\n        an image and tabular features.\n        \"\"\"\n        image_path = self.df[\"image_path\"].iloc[index]\n        image = Image.open(image_path)\n        image = np.array(image)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.tensor(image, dtype=torch.float)\n        if self.use_tabular_features:\n            if len(self.tabular_features) &gt; 0 and self.is_test is False:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"tabular_features\": tabular_features,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif len(self.tabular_features) &gt; 0 and self.is_test is True:\n                tabular_features = np.array(\n                    self.df.iloc[index][self.tabular_features].values, dtype=np.float32\n                )\n                return {\"image\": image, \"tabular_features\": tabular_features}\n        else:\n            if self.is_test is False:\n                targets = self.df.target[index]\n                return {\n                    \"image\": image,\n                    \"targets\": torch.tensor(targets, dtype=torch.long),\n                }\n            elif self.is_test is True:\n                return {\"image\": image}\n\n\nWriting Scripts/dataset.py\n\n\nNow we create a model class to create a model instance of EfficientNet model.\nCurrently, this function is capable of reading the images only and not the tabular features.\nSince in this project/notebook we are using the images only therefore, this function is good enough for that.\n\n\nCode\n%%writefile Scripts/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Class to instantiate EfficientNet-b5 model object which only\n    used images as inputs.\n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n    \n    \n# class Model(nn.Module):\n#     \"\"\"\n#     Class to instantiate EfficientNet-b5 model object which uses images\n#     as well as tabular features as inputs.\n#     \"\"\"\n#     def __init__(self, model_name='efficientnet-b5', pool_type=F.adaptive_avg_pool2d,\n#                 num_tabular_features=0):\n#         super().__init__()\n#         self.pool_type = pool_type\n#         self.model_name = model_name\n#         self.backbone = EfficientNet.from_pretrained(model_name)\n#         in_features = getattr(self.backbone, \"_fc\").in_features\n#         if num_tabular_features&gt;0:\n#             self.meta = nn.Sequential(\n#                 nn.Linear(num_tabular_features, 512),\n#                 nn.BatchNorm1d(512),\n#                 nn.ReLU(),\n#                 nn.Dropout(p=0.5),\n#                 nn.Linear(512, 128),\n#                 nn.BatchNorm1d(128),\n#                 nn.ReLU())\n#             in_features += 128\n#         self.output = nn.Linear(in_features, 1)\n    \n#     def forward(self, image, tabular_features=None):\n#         features = self.pool_type(self.backbone.extract_features(image), 1)\n#         cnn_features = features.view(image.size(0),-1)\n#         if num_tabular_features&gt;0:\n#             tabular_features = self.meta(tabular_features)\n#             all_features = torch.cat((cnn_features, tabular_features), dim=1)\n#             output = self.output(all_features)\n#             return output\n#         else:\n#             output = self.output(cnn_features)\n#             return output\n\n\nWriting Scripts/model.py\n\n\nWe create a validation function that predicts and generates probabilities only on the validation corresponding to a specific fold.\nThis function might be useful in come cases. This function is capable of running on a single gpu or multi-gpu device as well as on cpu.\n\n\nCode\n%%writefile Scripts/predict_on_validation_data.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import validation_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_validation_dataset(\n    validation_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    validation dataset and returns a submission.csv file.\n    Args:\n        validation_dataset = validation_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    valid_dataset = DatasetRetriever(\n        df=validation_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=True,\n    )\n    valid_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    valid_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(valid_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            valid_predictions.extend(valid_probs)\n    valid_predictions = [\n        valid_predictions[img].item() for img in range(len(valid_predictions))\n    ]\n    return valid_predictions\n\n\nWriting Scripts/predict_on_validation_data.py\n\n\nThis below function is used to generate the prediction probabilities on the testing dataset provided for the competition and generates a submission.csv file for the public and private leaderboard results.\n\n\nCode\n%%writefile Scripts/predict_on_test.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nfrom Scripts.dataset import DatasetRetriever\nfrom Scripts.augmentations import testing_augmentations\nfrom torch.utils.data import DataLoader\n\n\ndef predict_on_test_and_generate_submission_file(\n    test_df: pd.DataFrame, model_path: str, use_tabular_features: bool = False\n):\n    \"\"\"\n    This function generates prediction probabilities on the\n    test dataset and returns a submission.csv file.\n    Args:\n        test_df = test_dataframe.\n        model_path = location where model state_dict is located.\n        use_tabular_features: whether to use the tabular features\n        or not.\n    \"\"\"\n    test_dataset = DatasetRetriever(\n        df=test_df,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=testing_augmentations,\n        is_test=True,\n    )\n    test_dataloader = DataLoader(\n        dataset=test_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    test_predictions = []\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    with torch.inference_mode():\n        for _, data in enumerate(test_dataloader):\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"] = data[\"image\"].to(\n                    Config.DEVICE, dtype=torch.float\n                ), data[\"tabular_features\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"], data[\"tabular_features\"])\n            else:\n                data[\"image\"] = data[\"image\"].to(Config.DEVICE, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            test_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            test_predictions.extend(test_probs)\n    submission_df = pd.read_csv(Config.submission_csv_path)\n    test_predictions = [\n        test_predictions[img].item() for img in range(len(test_predictions))\n    ]\n    submission_df[\"target\"] = test_predictions\n    submission_df.to_csv(\"../working/submission.csv\", index=False)\n\n\nWriting Scripts/predict_on_test.py\n\n\nNow, we create a train_model module which has a run_model function that takes a fold number and the training dataframe.\nThe function creates training and validation dataframe , then we create training and validation datasets which only reads images and no tabular features, next we initialize seed (for reproduciblity of results), model object, loss function, optimizer, scheduler and a scaler object (for mixed precision).\n\n\nCode\n%%writefile Scripts/train_model.py\nimport os\nimport torch\nfrom Scripts.config import Config\nimport pandas as pd\nimport torch.nn as nn\nfrom Scripts.model import Model\nimport torch.cuda.amp as amp\nfrom Scripts.utils import create_folds\nfrom Scripts.utils import seed_everything\nfrom Scripts.dataset import DatasetRetriever\nfrom timeit import default_timer as timer\nfrom Scripts.training_and_validation_loops import train\nfrom torch.utils.data import Dataset, DataLoader\nfrom Scripts.augmentations import training_augmentations, validation_augmentations\n\ndef run_model(fold, train_df):\n    train_df = create_folds(train_df=train_df)\n    train_data = train_df.loc[train_df[\"fold\"] != fold].reset_index(drop=True)\n    valid_data = train_df.loc[train_df[\"fold\"] == fold].reset_index(drop=True)\n    validation_targets = valid_data[\"target\"]\n    train_dataset = DatasetRetriever(\n        df=train_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=training_augmentations,\n        is_test=False,\n    )\n    valid_dataset = DatasetRetriever(\n        df=valid_data,\n        tabular_features=None,\n        use_tabular_features=False,\n        augmentations=validation_augmentations,\n        is_test=False,\n    )\n    training_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=Config.TRAIN_BATCH_SIZE,\n        shuffle=True,\n        num_workers=os.cpu_count(),\n    )\n    validation_dataloader = DataLoader(\n        dataset=valid_dataset,\n        batch_size=Config.VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=os.cpu_count(),\n    )\n    seed_everything(Config.RANDOM_STATE)\n    if torch.cuda.device_count() in (0, 1):\n        model = Model().to(\n            Config.DEVICE\n        )\n    elif torch.cuda.device_count() &gt; 1:\n        model = Model().to(\n            Config.DEVICE\n        )\n        model = nn.DataParallel(model)\n    loss = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(\n        params=model.parameters(),\n        lr=Config.LEARNING_RATE,\n        weight_decay=Config.WEIGHT_DECAY,\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer=optimizer,\n        mode=\"max\",\n        factor=0.2,\n        patience=2,\n        threshold=1e-3,\n        verbose=True,\n    )\n    scaler = amp.GradScaler()\n    start_time = timer()\n    model_save_path = f\"../working/Models/efficientnet_b5_checkpoint_fold_{fold}.pt\"\n    model_results = train(\n        model=model,\n        train_dataloader=training_dataloader,\n        valid_dataloader=validation_dataloader,\n        loss_fn=loss,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=Config.DEVICE,\n        scaler=scaler,\n        epochs=Config.EPOCHS,\n        es_patience=2,\n        model_save_path=model_save_path,\n        validation_targets=validation_targets,\n    )\n    end_time = timer()\n    print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n\nWriting Scripts/train_model.py\n\n\nRegular pytorch training and validation loops epochs for a single epoch and finally for N number of epochs a train function.\n\n\nCode\n%%writefile Scripts/training_and_validation_loops.py\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.cuda.amp as amp\nfrom Scripts.utils import EarlyStopping\nfrom sklearn.metrics import roc_auc_score\n\n\ndef train_one_epoch(\n    model, dataloader, loss_fn, optimizer, device, scaler, use_tabular_features=False\n):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, an optimizer, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    , scaler (for mixed precision) and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value on training dataset.\n    \"\"\"\n    train_loss = 0\n    model.train()\n    for data in dataloader:\n        optimizer.zero_grad()\n        if use_tabular_features:\n            data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                data[\"image\"].to(device, dtype=torch.float),\n                data[\"tabular_features\"].to(device, dtype=torch.float),\n                data[\"targets\"].to(device, dtype=torch.float),\n            )\n            with amp.autocast():\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        else:\n            data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                device, dtype=torch.float\n            ), data[\"targets\"].to(device, dtype=torch.float)\n            with amp.autocast():\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n                loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n        train_loss += loss.item()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    train_loss = train_loss / len(dataloader)\n    return train_loss\n\n\ndef validate_one_epoch(model, dataloader, loss_fn, device, use_tabular_features=False):\n    \"\"\"\n    Function takes a model instance, dataloader, loss function, device\n    (on which device should you want to run the model on i.e., GPU or CPU)\n    and whether to use tabular features or not.\n    This function runs/passes the images and tabular features for a single epoch\n    and returns the loss value & final predictions on the validation dataset.\n    \"\"\"\n    valid_loss, final_predictions = 0, []\n    model.eval()\n    with torch.inference_mode():\n        for data in dataloader:\n            if use_tabular_features:\n                data[\"image\"], data[\"tabular_features\"], data[\"targets\"] = (\n                    data[\"image\"].to(device, dtype=torch.float),\n                    data[\"tabular_features\"].to(device, dtype=torch.float),\n                    data[\"targets\"].to(device, dtype=torch.float),\n                )\n                y_logits = model(data[\"image\"], data[\"tabular_features\"]).squeeze(dim=0)\n            else:\n                data[\"image\"], data[\"targets\"] = data[\"image\"].to(\n                    device, dtype=torch.float\n                ), data[\"targets\"].to(device, dtype=torch.float)\n                y_logits = model(data[\"image\"]).squeeze(dim=0)\n            loss = loss_fn(y_logits, data[\"targets\"].view(-1, 1))\n            valid_loss += loss.item()\n            valid_probs = torch.sigmoid(y_logits).detach().cpu().numpy()\n            final_predictions.extend(valid_probs)\n    valid_loss = valid_loss / len(dataloader)\n    return valid_loss, final_predictions\n\n\ndef train(\n    model,\n    train_dataloader,\n    valid_dataloader,\n    loss_fn,\n    optimizer,\n    scheduler,\n    device,\n    scaler,\n    epochs,\n    es_patience,\n    model_save_path,\n    validation_targets,\n):\n    \"\"\"\n    This function takes a model instance, training dataloader,\n    validation dataloader, loss_fn, optimizer, scheduler, device,\n    scaler (object, for mixed precision), epochs (for how many epochs\n    to run the model), es_patience (number of epochs to wait after which\n    the model should stop training), model_save_path (where to save the\n    model to), validation_targets (used for the calculation of the AUC\n    score) and returns a dictionary object which has training loss,\n    validation loss and validation AUC score.\n    \"\"\"\n    results = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": []}\n\n    early_stopping = EarlyStopping(\n        patience=es_patience, verbose=True, path=model_save_path\n    )\n\n    for epoch in tqdm(range(epochs)):\n        train_loss = train_one_epoch(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            device=device,\n            scaler=scaler,\n            use_tabular_features=False,\n        )\n\n        valid_loss, valid_predictions = validate_one_epoch(\n            model=model,\n            dataloader=valid_dataloader,\n            loss_fn=loss_fn,\n            device=device,\n            use_tabular_features=False,\n        )\n\n        valid_predictions = np.vstack(valid_predictions).ravel()\n\n        valid_auc = roc_auc_score(y_score=valid_predictions, y_true=validation_targets)\n        scheduler.step(valid_auc)\n\n        early_stopping(valid_loss, model)\n\n        if early_stopping.early_stop:\n            print(\"Early Stopping\")\n            break\n\n        model.load_state_dict(torch.load(model_save_path))\n        print(\n            f\"Epoch : {epoch+1} | \"\n            f\"train_loss : {train_loss:.4f} | \"\n            f\"valid_loss : {valid_loss:.4f} | \"\n            f\"valid_auc : {valid_auc:.4f} \"\n        )\n        results[\"train_loss\"].append(train_loss)\n        results[\"valid_loss\"].append(valid_loss)\n        results[\"valid_auc\"].append(valid_auc)\n    return results\n\n\nWriting Scripts/training_and_validation_loops.py\n\n\nIn the utils module we write some useful functions like create_folds (which will divide the training dataset into 5 equal parts and remove duplicate images from the dataset)\nseed_everything (for reproducing the results)\nEarlyStopping class (used to stop model training if our model performance on validation dataset starts to decline), plot_loss_curves (for plotting the training and validation loss and auc_scores for each epoch)\nRareLabelCategoryEncoder (class that combines the category/categories of a feature that appears in the dataset for a certain percentage of times like 5% of the time etc. into a single category called Rare)\nOutlierTreatment (class to cap the values of a feature by learning the lower quantile and upper_quantile values of a feature from the dataset (X) in the fit method and caps(transforms) the feature values in the dataset (x) passed in the transformed method).\n\n\nCode\n%%writefile Scripts/utils.py\nimport os\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import List\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\ndef create_folds(train_df):\n    \"\"\"\n    Function that folds in the training data and removes duplicate\n    images from the training data.\n    \"\"\"\n    train_df = train_df.loc[train_df[\"tfrecord\"] != -1].reset_index(drop=True)\n    train_df[\"fold\"] = train_df[\"tfrecord\"] % 5\n    return train_df\n\n\ndef seed_everything(seed: int):\n    \"\"\"\n    Function to set seed and to make reproducible results.\n    Args:\n        seed (int): like 42\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n    Directly borrowed from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        patience: int = 7,\n        verbose: bool = False,\n        delta: int = 0,\n        trace_func=print,\n    ):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score &lt; self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(\n                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n            )\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            self.trace_func(\n                f\"Validation loss decreased ({self.val_loss_min:.6f} --&gt; {val_loss:.6f}).  Saving model ...\"\n            )\n        torch.save(obj=model.state_dict(), f=self.path)\n        self.val_loss_min = val_loss\n\n\ndef plot_loss_curves(results: dict):\n    \"\"\"\n    Function to plot training & validation loss curves & validation AUC\n    Args:\n        results (dict): A dictionary of training loss, validation_loss &\n        validation AUC score.\n    \"\"\"\n    loss = results[\"train_loss\"]\n    valid_loss = results[\"valid_loss\"]\n    # Get the accuracy values of the results dictionary (training and test)\n    valid_auc = results[\"valid_auc\"]\n    # Figure out how many epochs there were\n    epochs = range(len(results[\"train_loss\"]))\n    # Setup a plot\n    plt.figure(figsize=(15, 7))\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, valid_loss, label=\"valid_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, valid_auc, label=\"valid_auc\")\n    plt.title(\"AUC Score\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n\nclass RareLabelCategoryEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to combine rare categories of a categorical variable\n    where a category appearing less than a certain percentage\n    (as a threshold).\n    Example: a category/categories appearing less than 5% of the\n    times are combined a single category called rare.\n    \"\"\"\n\n    def __init__(self, variables: List, tol=0.05):\n        \"\"\"\n        Args:\n            variables (List): A list of variables for which we want\n            to combine into rare categories.\n            tol (int): A Threshold/Tolerance below which we want to\n            consider a category of a feature as rare.\n        \"\"\"\n        if not isinstance(variables, list):\n            raise ValueError(\"Variables should be a list\")\n        self.tol = tol\n        self.variables = variables\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns all the values/categories & the\n        percentage of times it appears in a feature in the\n        dataset passed while using this method.\n\n        Args:\n            X : From this dataset the fit function learns and\n            stores the number of times a category appears in\n            the dataset\n        \"\"\"\n        self.encoder_dict_ = {}\n        for var in self.variables:\n            t = pd.Series(x[var]).value_counts(normalize=True)\n            self.encoder_dict_[var] = list(t[t &gt;= self.tol].index)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        X (pd.DataFrame): Transform/Combines the categories of each\n        features passed into the variables list on the dataset passed\n        in this method and returns the transformed dataset.\n        \"\"\"\n        x = x.copy()\n        for var in self.variables:\n            x[var] = np.where(x[var].isin(self.encoder_dict_[var]), x[var], \"Other\")\n        return x\n\n\nclass OutlierTreatment(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Class to handle outliers in a continous feature.\n    \"\"\"\n\n    def __init__(\n        self, variable: str, upper_quantile: float = None, lower_quantile: float = None\n    ):\n        \"\"\"\n        Args:\n            variables (str): A variable to cap the upper and\n            lower boundaries of.\n            upper_quantile (float): A maximum value beyond which all the\n            values of a feature are capped at.\n            lower_quantile (float): A minimum value that are lower than\n            of the feature are capped at.\n        \"\"\"\n        if not isinstance(variable, str):\n            raise ValueError(\"Variable should be a string type.\")\n        self.upper_quantile = upper_quantile\n        self.variable = variable\n        self.lower_quantile = lower_quantile\n\n    def fit(self, x: pd.DataFrame):\n        \"\"\"\n        This function learns the lower & upper quantiles of a feature\n        present in the dataset x.\n        \"\"\"\n        self.upper_quantile = x[self.variable].quantile(self.upper_quantile)\n        self.lower_quantile = x[self.variable].quantile(self.lower_quantile)\n        return self\n\n    def transform(self, x: pd.DataFrame):\n        \"\"\"\n        This function caps the upper and lower quantiles in the dataframe\n        x with the values learnt in the dataframe passed in fit() method.\n        \"\"\"\n        x = x.copy()\n        x[self.variable] = np.where(\n            x[self.variable] &gt; self.upper_quantile,\n            self.upper_quantile,\n            np.where(\n                x[self.variable] &lt; self.lower_quantile,\n                self.lower_quantile,\n                x[self.variable],\n            ),\n        )\n        return x\n\n\nWriting Scripts/utils.py\n\n\n\nInitializing the __init__.py to make the Scripts folder a package.\n\n\n\nCode\n%%writefile Scripts/__init__.py\n\"\"\n\n\nWriting Scripts/__init__.py"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-requirements.txt-file-to-install-all-the-packages-for-our-model-training",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating requirements.txt file to install all the packages for our model training",
    "text": "Creating requirements.txt file to install all the packages for our model training\nThis will contain all the packages required for modeling/training the model.\n\n\nCode\n%%writefile requirements.txt\n# pandas==2.0.0\ntorch==1.13.0\ntorchvision==0.14.0\n# scikit-learn==1.2.2\nefficientnet_pytorch==0.7.1\nalbumentations==1.2.1\n# numpy==1.22.4\ntqdm==4.65.0\n# matplotlib==3.7.1\nPillow==8.4.0\n\n\nWriting requirements.txt"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#installing-all-the-packages",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Installing all the packages",
    "text": "Installing all the packages\n\n\nCode\n!pip install -r requirements.txt\n\n\nRequirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.13.0)\nRequirement already satisfied: torchvision==0.14.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.14.0)\nCollecting efficientnet_pytorch==0.7.1\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... done\nCollecting albumentations==1.2.1\n  Downloading albumentations-1.2.1-py3-none-any.whl (116 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.7/116.7 kB 5.3 MB/s eta 0:00:00\nCollecting tqdm==4.65.0\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 8.9 MB/s eta 0:00:00\nCollecting Pillow==8.4.0\n  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 32.8 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0-&gt;-r requirements.txt (line 2)) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.7.3)\nRequirement already satisfied: scikit-image&gt;=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.19.3)\nRequirement already satisfied: opencv-python-headless&gt;=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (4.5.4.60)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (6.0)\nRequirement already satisfied: qudida&gt;=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (0.0.4)\nRequirement already satisfied: scikit-learn&gt;=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.0.2)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2021.11.2)\nRequirement already satisfied: imageio&gt;=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.25.0)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: networkx&gt;=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (2.6.3)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image&gt;=0.16.1-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (23.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (1.26.14)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2.1.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;torchvision==0.14.0-&gt;-r requirements.txt (line 3)) (2022.12.7)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (3.1.0)\nRequirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn&gt;=0.19.1-&gt;qudida&gt;=0.0.4-&gt;albumentations==1.2.1-&gt;-r requirements.txt (line 6)) (1.2.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... done\n  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=d870e4ba77c41d05a67b458d1a03a108aaee250b6f63fca1cad356a67702a3af\n  Stored in directory: /root/.cache/pip/wheels/96/3f/5f/13976445f67f3b4e77b054e65f7f4c39016e92e8358fe088db\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: tqdm, Pillow, efficientnet_pytorch, albumentations\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Uninstalling tqdm-4.64.1:\n      Successfully uninstalled tqdm-4.64.1\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 9.4.0\n    Uninstalling Pillow-9.4.0:\n      Successfully uninstalled Pillow-9.4.0\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.3.0\n    Uninstalling albumentations-1.3.0:\n      Successfully uninstalled albumentations-1.3.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires tqdm&lt;4.65,&gt;=4.48.2, but you have tqdm 4.65.0 which is incompatible.\nSuccessfully installed Pillow-9.4.0 albumentations-1.2.1 efficientnet_pytorch-0.7.1 tqdm-4.65.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#exploratory-data-analysis",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore jumping right into the modeling and see the numbers go down or go up, it’s good to look at the data and try to make sense out of it.\nWe’ll do the same here as well, we will look at the distribution of each tabular features we have in the training and testing datasets.\n\n\nCode\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom Scripts.config import Config\n\n## Looking at the Data\ntrain_df = pd.read_csv(Config.train_csv_path_2020,\n                       usecols=Config.TRAIN_COLS)\ntest_df = pd.read_csv(Config.test_csv_path_2020,\n                       usecols=Config.TEST_COLS)\n\n## Creating Image_Path for each images in 2019 & 2020 training datasets\ntrain_df['image_path'] = os.path.join(Config.train_folder_2020) + train_df['image_name'] + \".jpg\"\ntest_df['image_path'] = os.path.join(Config.test_folder_2020) + test_df['image_name'] + \".jpg\"\n\ntrain_df.head()\n\n\n\n\n\n\n\n\n\nimage_name\npatient_id\nsex\nage_approx\nanatom_site_general_challenge\ntarget\ntfrecord\nimage_path\n\n\n\n\n0\nISIC_2637011\nIP_7279968\nmale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_2637...\n\n\n1\nISIC_0015719\nIP_3075186\nfemale\n45.0\nupper extremity\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0015...\n\n\n2\nISIC_0052212\nIP_2842074\nfemale\n50.0\nlower extremity\n0\n6\n../input/jpeg-melanoma-512x512/train/ISIC_0052...\n\n\n3\nISIC_0068279\nIP_6890425\nfemale\n45.0\nhead/neck\n0\n0\n../input/jpeg-melanoma-512x512/train/ISIC_0068...\n\n\n4\nISIC_0074268\nIP_8723313\nfemale\n55.0\nupper extremity\n0\n11\n../input/jpeg-melanoma-512x512/train/ISIC_0074...\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Number of Unique images id's in the training dataset are - {train_df['image_name'].nunique()} \\n\")\nprint(f\"Number of Unique images id's in the training dataset are - {test_df['image_name'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {train_df['patient_id'].nunique()}\\n\")\nprint(f\"Total number of Unique patients id's in the training dataset are - {test_df['patient_id'].nunique()}\")\n\n\nNumber of Unique images id's in the training dataset are - 33126 \n\nNumber of Unique images id's in the training dataset are - 10982\n\nTotal number of Unique patients id's in the training dataset are - 2056\n\nTotal number of Unique patients id's in the training dataset are - 690"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#patient-id",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Patient ID",
    "text": "Patient ID\n\n\nCode\npatients_id_counts_train = train_df['patient_id'].value_counts()\npatients_id_counts_test = test_df['patient_id'].value_counts()\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(19,10), dpi=80, sharex=False, sharey=False)\nsns.distplot(patients_id_counts_train, ax=ax[0,0], color='#3300CC', kde=True)\nax[0,0].set_xlabel(\"Counts\")\nax[0,0].set_ylabel('Frequency')\nax[0,0].set_title('Patient ID counts in training data')\n\nsns.distplot(patients_id_counts_test, ax=ax[0,1], color='#FF0099', kde=True)\nax[0,1].set_xlabel(\"Counts\")\nax[0,1].set_ylabel('Frequency')\nax[0,1].set_title('Patient ID counts in testing data')\n\nsns.boxplot(patients_id_counts_train, ax=ax[1,0], color='#3300CC')\nax[1,0].set_xlabel('Counts')\nax[1,0].set_title('BoxPlot of Patient ID Counts in Train data')\nsns.boxplot(patients_id_counts_test, ax=ax[1,1], color='#FF0099')\nax[1,1].set_xlabel('Counts')\nax[1,1].set_title('BoxPlot of Patient ID Counts in Test data');"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#gender",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#gender",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Gender",
    "text": "Gender\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x=train_df['sex'], color='#3300CC', ax=ax[0])\nax[0].set_ylabel(\"\")\nax[0].set_xlabel('Gender Count')\nax[0].set_title(\"Gender Count in Training data\")\n\nsns.countplot(x=test_df['sex'], color=\"#FF0099\", ax=ax[1])\nax[1].set_ylabel(\"\")\nax[1].set_xlabel('Gender Count')\nax[1].set_title(\"Gender Count in Testing data\");"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#age",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#age",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Age",
    "text": "Age\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\nsns.countplot(train_df['age_approx'], color='#3300CC', ax=ax[0])\nax[0].set_title(\"Distribution of Age feature in Training Data\")\nax[0].set_xlabel(\"Age\")\nax[0].set_ylabel('Frequency')\n\nsns.countplot(test_df['age_approx'], color='#FF0099', ax=ax[1])\nax[1].set_title(\"Distribution of Age feature in Testing Data\")\nax[1].set_xlabel(\"Age\")\nax[1].set_ylabel('Frequency');\n\n\n\n\n\n\n\nCode\nage_dist_train_test = pd.concat(\n    [train_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93, \n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99]),\n    test_df['age_approx'].describe(percentiles=[0.01, 0.05, 0.10, 0.15, 0.25, 0.50, 0.75 ,0.90, 0.91, 0.92,0.93,\n                                                        0.94, 0.95, 0.96, 0.97, 0.98, 0.99])\n    ], axis=1)\nage_dist_train_test.columns = ['Train_Age', 'Test_Age']\nage_dist_train_test.T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n10%\n15%\n25%\n50%\n...\n91%\n92%\n93%\n94%\n95%\n96%\n97%\n98%\n99%\nmax\n\n\n\n\nTrain_Age\n33058.0\n48.870016\n14.380360\n0.0\n20.0\n25.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n75.0\n80.0\n90.0\n\n\nTest_Age\n10982.0\n49.525587\n14.370589\n10.0\n20.0\n30.0\n30.0\n35.0\n40.0\n50.0\n...\n70.0\n70.0\n70.0\n70.0\n75.0\n75.0\n80.0\n80.0\n85.0\n90.0\n\n\n\n\n2 rows × 22 columns"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#anatom_site_general_challenge",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "anatom_site_general_challenge",
    "text": "anatom_site_general_challenge\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False , sharey=False, figsize=(20,5))\n\ntrain_anatom_site_general = train_df[\"anatom_site_general_challenge\"].value_counts().sort_values(ascending=False)\ntest_anatom_site_general = test_df.anatom_site_general_challenge.value_counts().sort_values(ascending=False)\n\nsns.barplot(x=train_anatom_site_general.index.values, y=train_anatom_site_general.values, ax=ax[0]);\nax[0].set_xlabel(\"\");\nlabels = ax[0].get_xticklabels();\nax[0].set_xticklabels(labels, rotation=90);\nax[0].set_title(\"Image locations in train\");\n\nsns.barplot(x=test_anatom_site_general.index.values, y=test_anatom_site_general.values, ax=ax[1]);\nax[1].set_xlabel(\"\");\nlabels = ax[1].get_xticklabels();\nax[1].set_xticklabels(labels, rotation=90);\nax[1].set_title(\"Image locations in test\");"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#target",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Target",
    "text": "Target\n\n\nCode\ntrain_df['target'].value_counts(normalize=True, dropna=False) * 100\n\n\n0    98.237034\n1     1.762966\nName: target, dtype: float64"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#sex-vs.-target-anatom_site_general_challenge-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Sex Vs. Target & anatom_site_general_challenge Vs. Target",
    "text": "Sex Vs. Target & anatom_site_general_challenge Vs. Target\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20,5))\nsns.countplot(x='target', hue='sex',data=train_df, ax=ax[0])\nax[0].set_xlabel(\"Target\", fontsize=15)\nax[0].set_ylabel('Count', fontsize=15)\nax[0].set_title(\"Sex Vs. Target\", fontsize=20)\n\nsns.countplot(x='target', hue='anatom_site_general_challenge',data=train_df, ax=ax[1])\nax[1].set_xlabel(\"Target\", fontsize=15)\nax[1].set_ylabel('Count', fontsize=15)\nax[1].set_title(\"Sex Vs. anatom_site_general_challenge\", fontsize=20);"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#age-vs.-target",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Age Vs. Target",
    "text": "Age Vs. Target\n\n\nCode\nplt.figure(figsize=(20,5))\nsns.distplot(train_df.loc[train_df['target'] == 0]['age_approx'], bins=50, label='Benign')\nsns.distplot(train_df.loc[train_df['target'] == 1]['age_approx'], bins=50, label='Malignant')\nplt.legend(loc='best')\nplt.ylabel('Density', fontsize=15)\nplt.xlabel('Age', fontsize=15)\nplt.title(\"Age Vs. Target distribution\", fontsize=20);"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#training-our-model-with-no-tabular-features",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Training our model with No Tabular features",
    "text": "Training our model with No Tabular features\n\nFor now, we will run/train the model on a single fold only.\n\n\n\nCode\nfrom Scripts.train_model import run_model\nrun_model(fold=0, train_df=train_df)\n\n\n\n\n\nLoaded pretrained weights for efficientnet-b5\nValidation loss decreased (inf --&gt; 0.071890).  Saving model ...\nEpoch : 1 | train_loss : 0.0986 | valid_loss : 0.0719 | valid_auc : 0.8579 \nValidation loss decreased (0.071890 --&gt; 0.067238).  Saving model ...\nEpoch : 2 | train_loss : 0.0749 | valid_loss : 0.0672 | valid_auc : 0.8902 \nEarlyStopping counter: 1 out of 2\nEpoch : 3 | train_loss : 0.0708 | valid_loss : 0.0709 | valid_auc : 0.8746 \nValidation loss decreased (0.067238 --&gt; 0.065598).  Saving model ...\nEpoch : 4 | train_loss : 0.0706 | valid_loss : 0.0656 | valid_auc : 0.8971 \nEarlyStopping counter: 1 out of 2\nEpoch : 5 | train_loss : 0.0683 | valid_loss : 0.0673 | valid_auc : 0.9015 \nTotal training time: 16369.841 seconds"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#generating-submission-file-on-the-test-dataset",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Generating submission file on the test dataset",
    "text": "Generating submission file on the test dataset\n\n\nCode\nfrom Scripts.predict_on_test import predict_on_test_and_generate_submission_file\n\nmodel_path = \"../working/Models/efficientnet_b5_checkpoint_fold_0.pt\"\npredict_on_test_and_generate_submission_file(test_df=test_df,\n                                            use_tabular_features=False,\n                                            model_path=model_path)\n\n\nLoaded pretrained weights for efficientnet-b5"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#building-gradio-demo",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Building Gradio demo",
    "text": "Building Gradio demo\nNow that we have finalized the model we’ll be deploying, we will use this EfficientNet-B5 model to predict on new images.\nFor this project I will be using Gradio library (product of HuggingFace).\nWhy Gradio? The homepage of Gradio descibes it as: &gt; Gradio is the fastest way to build/demo your machine learning model with a friendly web interface so that anyone can use it, anywhere.\n\nGradio Overview\nIn general, we can have any combination of inputs like - Images - Tabular data - Text - Numbers - Video - Audio - etc.\nIn our case we have images and inputs and the output is returned as a probability of whether a patient is sufferig from melanoma skin cancer disease.\nGradio provides an interface that maps from the input(s) to output(s).\ngr.Interface(function, inputs, outputs)\nWhere, fn is a python function to map inputs to outputs\nGradio provides a very helpful Interface class to create an inputs -&gt; model/function -&gt; outputs workflow where the inputs and outputs could be almost anything we want.\n\n\nCode\n## Installing Gradio and importing it. \ntry:\n    import gradio as gr\nexcept:\n    !pip install -q gradio\n    import gradio as gr\nprint(f\"Gradio version : {gr.__version__}\")\n\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nGradio version : 3.27.0"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-model-instance-and-putting-it-on-the-cpu",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating a model instance and putting it on the CPU",
    "text": "Creating a model instance and putting it on the CPU\nFirst, let’s make sure our EfficientNetB5 model on CPU\n\n\nCode\nimport torch\nimport numpy as np\nimport albumentations as A\nfrom Scripts.model import Model\n\nefficientnet_b5_model = Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\nnext(iter(model.parameters())).device\n\n\nLoaded pretrained weights for efficientnet-b5\n\n\ndevice(type='cpu')"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#creating-a-function-to-predict-on-a-single-images",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Creating a function to predict on a single images",
    "text": "Creating a function to predict on a single images\nWe create a function that takes an input: image -&gt; transform -&gt; predict with EfficientNetB5 -&gt; output: probability.\nThis will be our fn parameter for our Gradio Interface.\n\n\nCode\n## predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n\n\nLet’s see our function by performing a prediction of an image from the training dataset.\nWe’ll get some images from training dataset and extract the images paths list. Then we’ll open an image with Image.open().\nFinally, we pass the image to predict_on_single_image().\n\n\nCode\nimport torch\nimport pathlib\nimport numpy as np\nfrom PIL import Image\n\n## Taking some images out\nimages = train_df.iloc[1:10,]\nimages_paths_list = images['image_path'].tolist()\n\n## Opening an Image\nimg = Image.open(images_paths_list[8])\n\n## Predicting on the image using the function\npredict_on_single_image(img)\n\n\n{'Probability of Having Melanoma': 0.5395416617393494,\n 'Probability of Not having Melanoma': 0.46045833826065063}\n\n\n\nCreating a list of example images\nBefore we create a demo, we first create a list of examples.\nGradio’s Interface class takes a list of examples parameter is a list of lists.\nSo, we create a list of lists containing the filepaths of images.\nOur gradio demo will showcase these as example inputs to our demo so people can try.\n\n\nCode\nexample_list = [[str(file_path)] for file_path in images_paths_list]\nexample_list\n\n\n[['../input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'],\n ['../input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg']]\n\n\n\n\nBuilding a Gradio interface\nPutting everything together -&gt;\nGradio Interface Workflow: input: image -&gt; transform -&gt; predict with EfficientNetB5 model -&gt; probability: output\nWe can do with the Gr.Interface() class with the following parameters: - fn: a python function that maps from inputs to outputs, in our case the predict_on_single_image() function. - inputs: the input to our Interface, such as image using gradio.Image(). - outputs: the output of the Interface once the inputs are processed with the fn, such as a Number gradio.Number() (for our case probability). - examples: a list of examples to showcase for the demo. - title: a string title of the demo. - description: a string description of the demo.\nOnce, we’ve created a demo instance of gr.Interface(), we use demo.launch() command.\n\n\nCode\nimport gradio as gr\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\n\nRunning on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://e228186381e2781228.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n\n\n\nWoow!!!\nOur application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting, we can upload our Gradio app Interface to HuggingFace Spaces.\nNow download all the files and folders from kaggle output manually & this kaggle kernel locally"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#turning-our-melanoma-skin-cancer-detection-gradio-demo-into-a-deployable-app",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app",
    "text": "Turning our Melanoma skin cancer detection Gradio Demo into a deployable app\nWe’ll deploy the demo application on HuggingFace Spaces.\nWhat is HuggingFace Spaces?\nIt is a resource that allows anybody to host and share machine learning application.\n\nDeployed Gradio App Structure\nTo upload our gradio app, we’ll want to put everything together into a singe directory.\nFor example, our demo might live at the path demos/melanoma_skin_cancer_files with the following structure:\ndemos/\n    └── melanoma_skin_cancer_files/\n        ├── efficientnet_b5_checkpoint_fold_0.pt\n        ├── app.py\n        ├── examples/\n        │   ├── image_1.jpg\n        │   ├── image_2.jpg\n        │   └── image_3.jpg\n        ├── model.py\n        └── requirements.txt\nWhere: - efficientnet_b5_checkpoint_fold_0 is our trained model. - app.py contains our Gradio app. Note: app.py is the default filename used for HuggingFace Spaces, if we deploy our apps there. - examples contains sample images to showcase the demo of our Gradio application. - model.py contains the main model/transformations code associated with our model. - requirements.txt file contains the dependencies/packages to run our application such as torch, albumentations, torchvision, gradio, numpy.\n\n\nCreating a demo folder to store our Melanoma skin cancer App files\nTo begin, we’ll create an empty directory demos/ that will contain all our necessary files for the application.\nWe can achive this using Python’s pathlib.Path(\"path_of_dir\") to establish directory path and then pathlib.Path(\"path_of_dir\").mkdir() to create it.\n\n\nCode\n############### ROOT_DIR : I Have put the files in my E: drive\n## Importing Packages \nimport shutil\nfrom pathlib import Path\nimport os\n\nROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n\n## Create Melanoma skin cancer demo path\nmelanoma_app_demo_path = Path(f\"{ROOT_DIR}/demos/melanoma_skin_cancer_files\")\n\n## Removing files that might already exist and creating a new directory.\nif melanoma_app_demo_path.exists():\n    shutil.rmtree(melanoma_app_demo_path)\n    melanoma_app_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n                                exist_ok=True) # Create even if they already exists? \nelse:\n    ## If the path doesn't exist, create one \n    melanoma_app_demo_path.mkdir(parents=True,\n                                exist_ok=True)\n\n\n\n\nCreating a folder of example images to use with our Melanoma skin cancer demo\nNow we’ll create an empty directory called examples and store some images (namely - ISIC_0015719.jpg, ISIC_0052212.jpg, ISIC_0068279.jpg) from the training dataset provided in the competition (which we download manually) from the here. Download the 1ts three images from the training dataset mentioned above.\nPut these images in the Data/Input (whatever you want to call) folder.\nTo do so we’ll:\n\nCreate an empty directory examples/ within the demos/melanoma_skin_cancer_files directory.\nDownload the top 3 mentioned images from the training dataset from the link above.\nCollect the filepaths into a list.\nCopy these 3 images from the train dataset to the demos/melanom_skin_cancer_files/examples/ directory.\n\n\n\nCode\nimport shutil\nfrom pathlib import Path\n\n## Create examples directory\nmelanoma_app_examples_path = melanoma_app_demo_path / \"examples\"\nmelanoma_app_examples_path.mkdir(parents=True, exist_ok=True)\n\n## collecting the image paths of 4 images \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0015719.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0052212.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0068279.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/train/ISIC_0149568.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\n\n\n\nCode\n## collecting the image paths of some more images but this time from the testing folder \nmelanoma_app_examples = [Path(f\"{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0052060.jpg\"),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082004.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0082785.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0105104.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0112420.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0155983.jpg'),\n                        Path(f'{ROOT_DIR}/input/jpeg-melanoma-512x512/test/ISIC_0171865.jpg')]\nfor example in melanoma_app_examples:\n    destination = melanoma_app_examples_path / example.name\n    shutil.copy(src=example, dst=destination)\n\n\nNow we verify our example images are present, let’s list the contents of our demo/melanoma_skin_cancer/examples/ directory with os.listdir() and then format the filepaths into a list of lists (to make it compatible with the Gradio’s gradio.Interface(), example parameter).\n\n\nCode\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\nexample_list\n\n\n[['examples/ISIC_0015719.jpg'],\n ['examples/ISIC_0052060.jpg'],\n ['examples/ISIC_0052212.jpg'],\n ['examples/ISIC_0068279.jpg'],\n ['examples/ISIC_0082004.jpg'],\n ['examples/ISIC_0082785.jpg'],\n ['examples/ISIC_0105104.jpg'],\n ['examples/ISIC_0112420.jpg'],\n ['examples/ISIC_0149568.jpg'],\n ['examples/ISIC_0155983.jpg'],\n ['examples/ISIC_0171865.jpg']]\n\n\n\n\nMoving our trained EfficientNet-B5 model into our Melanoma demo directory.\nWe previously saved our model binary file into the Models directory while training as Models/efficientnet_b5_checkpoint_fold_0.pt.\nWe use Python’s shutil.move() method and passing in src(the source path of the target file) and dst (the destination folder path of the target file to be moved into) parameters.\n\n\nCode\n## Importing Libraries\nimport shutil\n\n## Create a source path for our target model\nefficientnet_b5_model_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Models\\\\efficientnet_b5_checkpoint_fold_0.pt\"\n\n## Create a destination path for our target model\nefficientnet_b5_model_destination = melanoma_app_demo_path / efficientnet_b5_model_path.split(\"\\\\\")[-1]\n\n## Try to move the file\ntry:\n    print(f\"Attempting to move the {efficientnet_b5_model_path} to {efficientnet_b5_model_destination}\")\n    \n    ## Move the model\n    shutil.move(src=efficientnet_b5_model_path,\n               dst=efficientnet_b5_model_destination)\n    \n    print(\"Model move completed\")\n## If the model has already been moved, check if it exists\nexcept:\n    print(f\"No model found at {efficientnet_b5_model_path}, perhaps it's already moved.\")\n    print(f\"Model already exists at {efficientnet_b5_model_destination}: {efficientnet_b5_model_destination.exists()}\")\n\n\nAttempting to move the E:\\Melanoma_skin_cancer_detection\\output\\working\\Models\\efficientnet_b5_checkpoint_fold_0.pt to E:\\Melanoma_skin_cancer_detection\\demos\\melanoma_skin_cancer_files\\efficientnet_b5_checkpoint_fold_0.pt\nModel move completed\n\n\n\n\nTurning our EfficientNet-B5 model into a Python script (model.py)\nOur current model’s state_dict() is saved to demos/melanoma_skin_cancer/efficientnet_b5_checkpoint_fold_0.pt.\nTo load it it we can use model.load_state_dict() with torch.load(). But before that we need to instantiate a model.\nTo do this in a modular fashion we’ll create a script model.py which contains the model definition into a function called Model().\n\n\nCode\n## Now if we look into which directory we are currently, we'll find that using the following code\nos.getcwd()\n\n\n'E:\\\\Melanoma_skin_cancer_detection\\\\notebooks'\n\n\nNow we will move into the demos directory where we will write some helper utilities.\nIn cd ../demos/: .. means we are moving outside of the notebooks directory. demos/: means we moving inside the demos directory.\n\n\nCode\ncd ../demos/\n\n\nE:\\Melanoma_skin_cancer_detection\\demos\n\n\n\n\nCode\n%%writefile melanoma_skin_cancer_files/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom efficientnet_pytorch import EfficientNet\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Creates an efficientnet-b5 model instance.    \n    \"\"\"\n    def __init__(self, model_name=\"efficientnet-b5\", pool_type=F.adaptive_avg_pool2d):\n        super().__init__()\n        self.pool_type = pool_type\n        self.model_name = model_name\n        self.backbone = EfficientNet.from_pretrained(model_name)\n        in_features = getattr(self.backbone, \"_fc\").in_features\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        features = self.pool_type(self.backbone.extract_features(x), 1)\n        features = features.view(x.size(0), -1)\n        return self.classifier(features)\n\n\nWriting melanoma_skin_cancer_files/model.py\n\n\n\n\nTurning our Melanoma Skin Cancer Gradio App into a Python Script (app.py)\n\n\nCode\n%%writefile melanoma_skin_cancer_files/app.py\n## Importing Libraries\nimport os\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom model import Model\nimport albumentations as A\n\n## Creating a model instance\nefficientnet_b5_model =  Model()\nefficientnet_b5_model = torch.nn.DataParallel(efficientnet_b5_model) ## Must wrap our model in nn.DataParallel()\n    ## if used multi-gpu's to train the model otherwise we would get state_dict keys mismatch error.\nefficientnet_b5_model.load_state_dict(\n    torch.load(\n        f='efficientnet_b5_checkpoint_fold_0.pt',\n        map_location=torch.device(\"cpu\")\n    )\n)\n\n## Predict on a single image\ndef predict_on_single_image(img):\n    \"\"\"\n    Function takes an image, transforms for \n    model training like normalizing the statistics\n    of the image. Converting the numpy array into\n    torch tensor and passing through the model\n    to get the prediction probability of a patient\n    having melanoma. \n    \"\"\"\n    img = np.array(img)\n    transforms = A.Compose([A.Resize(512,512),\n        A.Normalize(mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225), \n                    max_pixel_value=255.0, \n                    always_apply=True\n        )]\n    )\n    img = transforms(image=img)['image']\n    image = np.transpose(img, (2, 0, 1)).astype(np.float32)\n    image = torch.tensor(image, dtype=torch.float).unsqueeze(dim=0)\n    efficientnet_b5_model.eval()\n    with torch.inference_mode():\n        probs = torch.sigmoid(efficientnet_b5_model(image))\n        prob_of_melanoma = probs[0].item()\n        prob_of_not_having_melanoma = 1 - prob_of_melanoma\n        pred_label = {\"Probability of Having Melanoma\": prob_of_melanoma, \n                      \"Probability of Not having Melanoma\": prob_of_not_having_melanoma} \n        return pred_label\n    \n## Gradio App\nimport gradio as gr\n\n## Examples directory path\nmelanoma_app_examples_path = \"examples\"\n\n## Creating the title and description strings  \ntitle = \"Melanoma Cancer Detection App\"\ndescription = 'An efficientnet-b5 model that predicts the probability of a patient having melanoma skin cancer or not.'\nexample_list = [[\"examples/\" + example] for example in os.listdir(melanoma_app_examples_path)]\n\n## Create the Gradio demo\ndemo = gr.Interface(fn=predict_on_single_image,\n                   inputs=gr.Image(type='pil'),\n                   outputs=[gr.Label(label='Probabilities')],\n                   examples=example_list, title=title,\n                   description=description)\n\n## Launch the demo!\ndemo.launch(debug=False, share=True)\n\n\nWriting melanoma_skin_cancer_files/app.py\n\n\n\n\nCreating a requirements.txt file for our Gradio App(requirements.txt)\nThis is the last file we need to create for our application.\nThis file contains all the necessary packages for our Gradio application.\nWhen we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\n\n\nCode\n%%writefile melanoma_skin_cancer_files/requirements.txt\ntorch==1.13.0\ntorchvision==0.14.0\nalbumentations==1.2.1\nefficientnet_pytorch==0.7.1\nPillow==8.4.0\nnumpy==1.22.4\ngradio==3.1.4\n\n\nWriting melanoma_skin_cancer_files/requirements.txt"
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#deploying-our-application-to-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Deploying our Application to HuggingFace Spaces",
    "text": "Deploying our Application to HuggingFace Spaces\nTo deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n\nUploading via the Hugging Face Web Interface (easiest)\nUploading via the command line or terminal\n\nNOTE: To host any application on HuggingFace, we first need to sign up for a free HuggingFace Account\n\nRunning our Application locally\n\nOpen the terminal or command prompt.\nChanging the melanoma_skin_cancer_files directory (cd melanoma_skin_cancer_files).\nCreating an environment (python3 -m venv env) or use (python -m venv env).\nActivating the environment (source env/Scripts/activate).\nInstalling the requirements.txt using pip install -r requirements.txt. &gt; If faced any errors, we might need to upgrade pip using pip install --upgrade pip.\n\nRun the app (python3 app.py).\n\nThis should results in a Gradio demo locally at the URL such as : http://127.0.0.1:7860/.\n\n\nUploading to Hugging Face\nWe’ve verified our Melanoma_skin_cancer detection application is working in our local system.\nTo upload our application to Hugging Face Spaces, we need to do the following.\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to our profile at the top right corner and then select New Space.\nDeclare the name to the space like Chirag1994/melanoma_skin_cancer_detection_app.\nSelect a license (I am using MIT license).\nSelect Gradio as the Space SDK (software development kit).\nChoose whether your Space is Public or Private (I am keeping it Public).\nClick Create Space.\nClone the repository locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in the terminal or command prompt. In our case mine would be like - git clone https://huggingface.co/spaces/Chirag1994/melanoma_skin_cancer_detection_app.\nCopy/Move the contents of the downloaded melanoma_skin_cancer_detection_app folder to the cloned repo folder.\nTo upload files and track larger files (e.g., files that are greater than 10MB) for them we need to install Git LFS which stands for Git large File Storage.\nOpen up the cloned directory using VS code (I’m using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command git lfs install to start tracking the file that we want to track. For example - git lfs track \"efficientnet_b5_checkpoint_fold_0.pt\".\nCreate a new .gitignore file and the files & folders that we don’t want git to track like :\n\n__pycache__/\n.vscode/\nvenv/\n.gitignore\n.gitattributes\n\nAdd the rest of the files and commit them with:\n\ngit add .\ngit commit -m \"commit message that you want\"\n\nPush(load) the files to Hugging Face\n\ngit push\n\nIt might a couple of minutes to finish and then the app will be up and running."
  },
  {
    "objectID": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "href": "portfolio/Computer_Vision_Portfolio/Melanoma/melanoma-final-model-kaggle.html#our-final-application-deployed-on-huggingface-spaces",
    "title": "Melanoma Skin Cancer Detection using Transfer Learning with PyTorch",
    "section": "Our Final Application deployed on HuggingFace Spaces",
    "text": "Our Final Application deployed on HuggingFace Spaces\n\n\nCode\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/Chirag1994/Melanoma_Skin_Cancer_Detection_App/+\", width=900, height=750)"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "",
    "text": "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as anything rude, disrespectful, or otherwise likely to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet."
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#problem-statement",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#problem-statement",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "",
    "text": "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as anything rude, disrespectful, or otherwise likely to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet."
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#dataset-description",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#dataset-description",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Dataset Description",
    "text": "Dataset Description\nAs part of the competition, competitors were provided several files, specifically:\njigsaw-toxic-comment-train.csv - data from the Jigsaw toxic comment classification competition. The dataset is made up of English comments from Wikipedia’s talk page edits.\njigsaw-unintended-bias-train.csv - data from the Jigsaw Unintended Bias in Toxicity Classification competition. This is an expanded version of the Civil Comments dataset with a range of additional labels.\nsample_submission.csv - a sample submission file.\ntest.csv - comments from Wikipedia talk pages in different non-English languages.\nvalidation.csv - comments from Wikipedia talk pages in different non-English languages"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#evaluation-metric",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#evaluation-metric",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Evaluation Metric",
    "text": "Evaluation Metric\nSubmissions were evaluated based on Area Under the ROC Curve between the predicted probability and the observed target."
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#strategies-to-tackle",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#strategies-to-tackle",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Strategies to Tackle",
    "text": "Strategies to Tackle\n\nMonolingual Approach\nMonolingual models are the type of language models which are trained on a single language.\nThey are focused on understanding and generating text in a specific language.\nFor example, a monolingual model trained on English language will be proficient in understanding and generating English text.\nThese models are typically for tasks such as text classification, sentiment analysis and more within a specific language.\nMonolingual models can be beneficial to utilize when we have a specific language in our training, testing datasets and in the upcoming unseen data.\n\n\nMultilingual Approach\nMultilingual models are, on the other hand, are trained on multiple different languages.\nThey are designed to handle and process text in multiple languages, allowing them to perform across different languages.\nMultilingual models have the advantage of of being able to provide language-agnostic solutions, as they can handle a wide-range of languages.\nThey can be used for zero-shot and few-shot learning, where the model can perform a task in a language it has not been seen specifically trained on by leveraging its knowledge of other languages.\n\n\nWhich models to use for our problem?\nAs per the dataset given in the competition, we have only english data in our training dataset and very samples are given in the validation dataset containing languages Spanish, Turkish and Italian only and the Testing dataset contains languages Turkish, Spanish, Italian, Russian, French and Portugese.\nSince in our validation and test dataset contains non-english languages it would be better approach to build multilingual models rather than monolingual models.\nNow, if we had only one language (as stated above) building monolingual models would be a better choice.\nLet’s discuss Multilingual models approach a bit more:\nHow are multilingual models are trained?\nMultilingual models are pre-trained on a mix of different languages and they don’t distinguish between the languages.\nThe English BERT was pre-trained on English Wikipedia and BookCorpus dataset, while multilingual models like mBERT was pre-trained on 102 different languages from largest Wikipedia dataset and XLM-Roberta was pre-trained on CommonCrawl dataset from 100 different languages respectively.\n\n\nCross Lingual Transfer\nCross-lingual transfer refers to transfer learning using data and models available for one language for which ample such resources are available (e.g., English) to solve tasks in another, commonly more low-resource, language.\nIn our case, we are trying to create an application that can automatically detect whether a sentence or phrase is toxic or not.\nModels like XLM-Roberta provides us the ability to fine tune it on English dataset and predict to identify comments in any different language.\nXLM-R is able to take it’s specific knowledge learnt in one language and apply it to a different langauge (languages), even though it never seen the language during fine-tuning.\nThis concept is of transfer learning applied from one language to another language is referred to as Cross-Lingual Transfer (AKA Zero-Shot Learning) .\nAnother reason to use Pre-Trained multi-lingual models for a task like this (as in our case) is that is the Lack of languages by resources i.e., different languages have different amounts of training data available to build models like BERT and its variants.\nSome languages like English, Chinese, Russian, Indonesian, Vietnamese etc. are the languages that have high resource languages, whereas languages like sundanese, assamese etc. are low resource languages.\nTraining our own BERT like model on these low resources could be very expensive in terms of data collection and performance-wise , therefore, We should leverage these multi-lingual models.\n\n\nWhat experiments did I perform ?\nAt the Overall level, I performed 9 experiments with the following ideas keeping in mind.\nPerform Pre-processing techniques like removal of stopwords, removing URLs, Contraction to Expansion of words, removing multiple characters from words and removal of punctuations.\nFrom model stand point we experimented with 2 models mBERT & XLM-Roberta.\nFrom Training dataset perspective we used 2 types of datasets: one case where we used the provided training datasets where we tried to balance the dataset by the target and the other case where we used the translated training dataset of languages provided in the test dataset along with the english language with class balancing.\nWe always trained on validation dataset as well to further boost the performance of the model.\nNow we build a model with the following ideas: &gt; Training on original training & validation datasets, class balancing (undersampling), fine-tuning the model for 2 epochs on training as well as validation dataset, and will not perform any preprocessing dataset. We will be leveraging the TPUs offered by Kaggle."
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#installing-libraries",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#installing-libraries",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n\nCode\n!pip install nltk\n!pip install transformers --quiet\n\nimport re\nimport nltk\nimport string\nimport os, gc\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nnltk.download('stopwords')\n\n\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 19.0 MB/s eta 0:00:0000:010:01\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\nCollecting regex&gt;=2021.8.3\n  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 KB 38.4 MB/s eta 0:00:00\nInstalling collected packages: regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2023.5.5\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n\n\nTrue"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#setting-data-paths",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#setting-data-paths",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Setting data paths",
    "text": "Setting data paths\n\n\nCode\nmain_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\ntoxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\nunintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\nvalidation_csv_path = main_data_dir_path + \"validation.csv\"\ntest_csv_path = main_data_dir_path + \"test.csv\"\nsubmission_csv_path = main_data_dir_path + \"sample_submission.csv\""
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#tpu-configurations",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#tpu-configurations",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "TPU Configurations",
    "text": "TPU Configurations\nIntializing the TPU configurations and other constants like number of epochs, batch_size (16 * number of cores offered on TPUS), MAX_LEN (length of the sentence), we use xlm-roberta-large model, number of samples (for undersampling) = 150k, Learning_rate = 1e-5 etc.\n\n\nCode\n#################### TPU Configurations ####################\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'xlm-roberta-large'\nNUM_SAMPLES = 150000\nRANDOM_STATE = 42\nLEARNING_RATE = 1e-5 ######################### MAIN CHANGE ############################\nWEIGHT_DECAY = 1e-6\n\n\nRunning on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS:  8"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#reading-balancing-the-data-by-target-column",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#reading-balancing-the-data-by-target-column",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Reading & Balancing the data by Target column",
    "text": "Reading & Balancing the data by Target column\n\n\nCode\n## Reading csv files \ntrain1 = pd.read_csv(toxic_comment_train_csv_path)\ntrain2 = pd.read_csv(unintended_bias_train_csv_path)\nvalid = pd.read_csv(validation_csv_path)\ntest = pd.read_csv(test_csv_path)\nsub = pd.read_csv(submission_csv_path)\n\n## Converting floating points to integers ##\ntrain2.toxic = train2['toxic'].round().astype(int)\n\n##### BALANCING THE DATA ##### \n# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n# & sampling 150k observations randomly from non-toxic observation population.\n\n# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n])\n\n## Dropping missing observations with respect to comment-text column \ntrain = train.dropna(subset=['comment_text'])\n\n\n\n\nCode\ndef encode(texts, tokenizer, max_len):\n    \"\"\"\n    Function takes a list of texts, tokenizer (object)\n    initialized from HuggingFace library, max_len (defines\n    of how long the sentence lengths should be).\n    \"\"\"       \n    tokens = tokenizer(texts, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='np')\n    \n    return tokens"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#encoding-comment_text",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#encoding-comment_text",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Encoding comment_text",
    "text": "Encoding comment_text\nWe first initialize the tokenizer from Hugging Face transformer library and encoding our training, validation and test dataset comment_texts.\n\n\nCode\n## Intializing the tokenizer ##\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_inputs = encode(train['comment_text'].values.tolist(), \n                      tokenizer, max_len=MAX_LEN)\nvalidation_inputs = encode(valid['comment_text'].values.tolist(),\n                          tokenizer, max_len=MAX_LEN)\ntest_inputs = encode(test['content'].values.tolist(),\n                    tokenizer, max_len=MAX_LEN)"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#preparing-data-using-tf.data.data-api",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#preparing-data-using-tf.data.data-api",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Preparing data using tf.data.Data API",
    "text": "Preparing data using tf.data.Data API\nWriting a function to create a tuple of inputs and outputs, where inputs have a dictionary datatype.\nWe’ll be leveraging tf.data.Data API to pass our inputs and outputs as tuple, i.e., (inputs, outputs), inputs are {\"input_ids\": input_ids, \"attention_mask\": attention_mask} and outputs labels.\n\n\nCode\ndef map_fn(input_ids, attention_mask, labels=None):\n    if labels is not None:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n    else:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\n\n\n\nCode\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n                                                    train_inputs[\"attention_mask\"],\n                                                   train['toxic']))\ntrain_dataset = train_dataset.map(map_fn)\ntrain_dataset = train_dataset.repeat().shuffle(buffer_size=2048,seed=RANDOM_STATE).batch(BATCH_SIZE).prefetch(AUTO)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n                                                         validation_inputs['attention_mask'],\n                                                        valid['toxic']))\nvalidation_dataset = validation_dataset.map(map_fn)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n                                                  test_inputs['attention_mask']))\ntest_dataset = test_dataset.map(map_fn)\ntest_dataset = test_dataset.batch(BATCH_SIZE)"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#building-the-model",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#building-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Building the model",
    "text": "Building the model\n\n\nCode\ndef build_model(transformer_layer, max_len):\n    \"\"\"\n    Creating the model input layers, output layers,\n    model definition and compilation.\n        \n    Returns: model object after compiling. \n    \"\"\"\n    input_ids = tf.keras.layers.Input(shape=(max_len,), \n                                      dtype=tf.int32, \n                                      name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n                                       dtype=tf.int32, \n                                       name=\"attention_mask\")\n    output = transformer_layer.roberta(input_ids, \n                                 attention_mask=attention_mask)[1]\n    x = tf.keras.layers.Dense(1024, activation='relu')(output)\n    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n                             outputs=y)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    AUC = tf.keras.metrics.AUC()\n    \n    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n    return model"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-model-on-tpus",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-model-on-tpus",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Loading model on TPUs",
    "text": "Loading model on TPUs\nIt is important to initialize & compile the model inside the with strategy.scope().\nOne thing I want to point out that for some reason I was getting different results even though I was setting the seed before initializing the model, but the results are always consistent even though the results differ very little every time we run the pipeline.\n\n\nCode\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    tf.random.set_seed(RANDOM_STATE)\n    model = build_model(transformer_layer,\n                        max_len=MAX_LEN)\nmodel.summary()\n\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 192)]        0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 192)]        0           []                               \n                                                                                                  \n roberta (TFXLMRobertaMainLayer  TFBaseModelOutputWi  559890432  ['input_ids[0][0]',              \n )                              thPoolingAndCrossAt               'attention_mask[0][0]']         \n                                tentions(last_hidde                                               \n                                n_state=(None, 192,                                               \n                                 1024),                                                           \n                                 pooler_output=(Non                                               \n                                e, 1024),                                                         \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n dense (Dense)                  (None, 1024)         1049600     ['roberta[0][1]']                \n                                                                                                  \n outputs (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n                                                                                                  \n==================================================================================================\nTotal params: 560,941,057\nTrainable params: 560,941,057\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-only-english-data-for-2-epochs",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-only-english-data-for-2-epochs",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Training the model on Only English data for 2 epochs",
    "text": "Training the model on Only English data for 2 epochs\n\n\nCode\ntrain_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset,\n                         steps_per_epoch=train_steps_per_epoch,\n                         validation_data=validation_dataset,\n                         epochs=2) \n\n\nEpoch 1/2\n3795/3795 [==============================] - ETA: 0s - loss: 0.0551 - auc: 0.99703795/3795 [==============================] - 1609s 378ms/step - loss: 0.0551 - auc: 0.9970 - val_loss: 0.4696 - val_auc: 0.8942\nEpoch 2/2\n3795/3795 [==============================] - 1399s 369ms/step - loss: 0.0450 - auc: 0.9979 - val_loss: 0.3125 - val_auc: 0.9083"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-validation-data-for-2-epochs-further-to-fine-tune-on-it",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#training-the-model-on-validation-data-for-2-epochs-further-to-fine-tune-on-it",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Training the model on Validation data for 2 epochs further to fine-tune on it",
    "text": "Training the model on Validation data for 2 epochs further to fine-tune on it\n\n\nCode\nvalidation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\nvalidation_history = model.fit(validation_dataset.repeat(),\n                              steps_per_epoch=validation_steps_per_epoch,\n                              epochs=2)\n\n\nEpoch 1/2\n62/62 [==============================] - 23s 367ms/step - loss: 0.2286 - auc: 0.9315\nEpoch 2/2\n62/62 [==============================] - 107s 365ms/step - loss: 0.1472 - auc: 0.9726\n\n\n\nPublic LeaderBoard score on kaggle (test dataset): 0.936 and Private LeaderBoard score : 0.9346"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#results",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#results",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\nExperiment\nPublic Test LeaderBoard Score\nPrivate Test LeaderBoard Score\n\n\n\n\n1 (mBERT + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8850\n0.8869\n\n\n2 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.9259\n0.9264\n\n\n3 (mBERT + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8259\n0.8239\n\n\n4 (xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5)\n0.8755\n0.8754\n\n\n5 (mBERT + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.9195\n0.9212\n\n\n6 ((xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.9329\n0.9212\n\n\n7 (mBERT + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.8696\n0.9212\n\n\n8 ((xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.8861\n0.8866\n\n\n9 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 1e-5)\n0.936\n0.9346"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#predicting-on-test-dataset",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#predicting-on-test-dataset",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Predicting on Test dataset",
    "text": "Predicting on Test dataset\n\n\nCode\nsub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)\n\n\n499/499 [==============================] - 80s 119ms/step\n\n\n\n\nCode\nsub.head()\n\n\n\n\n\n\n\n\n\nid\ntoxic\n\n\n\n\n0\n0\n0.000308\n\n\n1\n1\n0.000241\n\n\n2\n2\n0.266148\n\n\n3\n3\n0.000063\n\n\n4\n4\n0.000078"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#saving-the-model",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#saving-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Saving the model",
    "text": "Saving the model\n\n\nCode\nmodel_save_path = \"../working/Multilingual_toxic_comment_classifier\"\nmodel.save(model_save_path)\n\n\nINFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-the-model",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#loading-the-model",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Loading the model",
    "text": "Loading the model\n\n\nCode\nimport tensorflow as tf\n\nmodel_save_path = \"../working/Multilingual_toxic_comment_classifier\"\nloaded_model = tf.keras.models.load_model(model_save_path)\ny = loaded_model.predict(test_dataset.take(1))\ny[:6]\n\n\n1/1 [==============================] - 37s 37s/step\n\n\narray([[3.0799874e-04],\n       [2.2472920e-04],\n       [2.6646560e-01],\n       [5.7183450e-05],\n       [7.6287179e-05],\n       [3.1223629e-02]], dtype=float32)\n\n\nWriting the function to prepare for the new text, we encode the text using the tokenizer with the sentence length=192\n\n\nCode\nfrom transformers import AutoTokenizer\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\ntext = \"politicians are like cancer for this country\"\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(text, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='tf')\n    \n    return {\"input_ids\": tokens['input_ids'],\n            \"attention_mask\": tokens['attention_mask']}\n\n\nPredicting the probability of toxic and non-toxic on a sample text.\n\n\nCode\nprob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\nprob_of_non_toxic_comment = 1 - prob_of_toxic_comment\nprob_of_toxic_comment, prob_of_non_toxic_comment\nprobs = {\"prob_of_toxic_comment\": prob_of_toxic_comment,\n \"prob_of_non_toxic_comment\": prob_of_non_toxic_comment}\nprobs\n\n\n1/1 [==============================] - 9s 9s/step\n\n\n{'prob_of_toxic_comment': 0.26497197,\n 'prob_of_non_toxic_comment': 0.7350280284881592}\n\n\n\nTesting the model with the Gradio App before final pushing the model to HuggingFace Spaces\n\n\nCode\n!pip3 install gradio --quiet\nimport tensorflow as tf\nimport gradio as gr\n\nloaded_model = tf.keras.models.load_model(model_save_path)\n\nfrom transformers import AutoTokenizer\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\nexamples_list = [\"politicians are like cancer for this country\", \n                 \"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было,\",\n                \"Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хох\",\n                ]\n\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(text, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='tf')\n    \n    return {\"input_ids\": tokens['input_ids'],\n            \"attention_mask\": tokens['attention_mask']}\n\ndef predict(text):\n    prob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\n    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n    prob_of_toxic_comment, prob_of_non_toxic_comment\n    probs = {\"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n             \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment)}\n    return probs\n\ninterface = gr.Interface(fn=predict, inputs=gr.components.Textbox(lines=4,label='Comment'),\n                        outputs=[gr.Label(label='Probabilities')], examples=examples_list,\n                        title='Multi-Lingual Toxic Comment Classification.',\n                        description='XLM-Roberta Large model')\ninterface.launch(debug=False, share=True)\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\nRunning on local URL:  http://127.0.0.1:7865\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRunning on public URL: https://af370decb4339b429e.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n1/1 [==============================] - 8s 8s/step\n1/1 [==============================] - 1s 530ms/step\n1/1 [==============================] - 1s 513ms/step\n\n\n\n\n\n\n\n\nWoow!!!\nOur application is up and running, this link is only temporary and and it remains ony for 72 hours. For permanent hosting, we can upload our Gradio app Interface to HuggingFace Spaces.\nNow download all the files and folders from kaggle output manually & this kaggle kernel locally"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-multi-lingual-toxic-comment-classification-gradio-demo-into-a-deployable-app",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-multi-lingual-toxic-comment-classification-gradio-demo-into-a-deployable-app",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Turning our Multi-Lingual Toxic Comment Classification Gradio Demo into a deployable app",
    "text": "Turning our Multi-Lingual Toxic Comment Classification Gradio Demo into a deployable app\nWe’ll deploy the demo application on HuggingFace Spaces.\nWhat is HuggingFace Spaces?\nIt is a resource that allows anybody to host and share machine learning application.\n\nDeployed Gradio App Structure\nTo upload our gradio app, we’ll want to put everything together into a singe directory.\nFor example, our demo might live at the path demos/melanoma_skin_cancer_files with the following structure:\ndemos/\n    └── multilingual_toxic_comment_files/\n        ├── Multilingual_toxic_comment_classifier/\n        │   ├── variable/\n        │   │   ├── variables.data-00000-of-00001\n        │   │   └── variables.index\n        │   ├── fingerprint.pb\n        │   ├── keras_metadata.pb\n        │   └── saved_model.pb \n        ├── app.py\n        ├── examples/\n        │   └── dataset\n        └── requirements.txt\nWhere: - Multilingual_toxic_comment_classifier is our saved fine-tuned model (binary files associated). - app.py contains our Gradio app, our data preprocessing function and our predict function. Note: app.py is the default filename used for HuggingFace Spaces, if we deploy our apps there. - examples contains sample dataframe which contains toxic & non-toxic comments from russian, spanish, english, italian, turkish, portugese and last french languages to showcase the demo of our Gradio application. - requirements.txt file contains the dependencies/packages to run our application such as tensorflow, gradio, transformers.\n\n\nCreating a demo folder to store our Multilingual Toxic Comment Classifier App files\nTo begin, we’ll create an empty directory demos/ that will contain all our necessary files for the application.\nWe can achive this using Python’s pathlib.Path(\"path_of_dir\") to establish directory path and then pathlib.Path(\"path_of_dir\").mkdir() to create it.\n\n\nCode\n############### ROOT_DIR : I Have put the files in my E: drive\n## Importing Packages \nimport shutil\nfrom pathlib import Path\nimport os\n\nROOT_DIR = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:2])\n\n## Create Melanoma skin cancer demo path\nmultilingual_toxic_comment_demo_path = Path(f\"{ROOT_DIR}/demos/multilingual_toxic_comment_files\")\n\n## Removing files that might already exist and creating a new directory.\nif multilingual_toxic_comment_demo_path.exists():\n    shutil.rmtree(multilingual_toxic_comment_demo_path)\n    multilingual_toxic_comment_demo_path.mkdir(parents=True, # Do we want to make parent folders?\n                                exist_ok=True) # Create even if they already exists? \nelse:\n    ## If the path doesn't exist, create one \n    multilingual_toxic_comment_demo_path.mkdir(parents=True,\n                                exist_ok=True)\n\n\n\n\nCreating a folder of example images to use with our Melanoma skin cancer demo\nNow we’ll create an empty directory called examples and store a sample dataset containing comments from the Russian, Turkish, English, Spanish, Portugese, French, Italian languages. I have collected these comments from online and created a CSV file for them.\nTo do so we’ll:\n\nCreate an empty directory examples/ within the demos/multilingual_toxic_comment_files directory.\nCollect some comment samples from online in these languages and create a CSV file out of them containing both toxic as well as non-toxic comments.\n\n\n\nCode\nimport pandas as pd\nfrom pathlib import Path\n\n## Create examples directory\nmultilingual_toxic_comment_examples_path = multilingual_toxic_comment_demo_path / \"examples\"\nmultilingual_toxic_comment_examples_path.mkdir(parents=True, exist_ok=True)\n\nsample_comments = Path(f\"sample_comments.csv\")\n\ncomments = {\"comment_text\": [\"Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\",\n                 \"Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\",\n                 \"В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.\",\n                 \"Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.\",\n                 \"This is such an urgent design problem; kudos to you for taking it on. Very impressive!\",\n                 \"haha you guys are a bunch of losers.\", \"ur a sh*tty comment.\",\n                 \"Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!\",\n                 'SIAMO ALLA FOLLIA', \n                 '20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg ',\n                 \"Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) \"]}\n\npd.DataFrame(comments, \n             columns=['comment_text']).to_csv(multilingual_toxic_comment_examples_path / sample_comments,\n                                              index=False)\n\n\nNow we verify our example images are present, let’s list the contents of our demo/melanoma_skin_cancer/examples/ directory with os.listdir() and then format the filepaths into a list of lists (to make it compatible with the Gradio’s gradio.Interface(), example parameter).\n\n\nCode\nexample_list = [[example] for example in pd.read_csv(multilingual_toxic_comment_examples_path / sample_comments)['comment_text'].tolist()]\nexample_list\n\n\n[['Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.'],\n ['Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?'],\n ['В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.'],\n ['Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.'],\n ['This is such an urgent design problem; kudos to you for taking it on. Very impressive!'],\n ['haha you guys are a bunch of losers.'],\n ['ur a sh*tty comment.'],\n ['Il solito vizio,o moda, della sinistra di andare ad aiutare tutti tranne chi ne ha bisogno in casa nostra. Quanti autorespiratori si sarebbero potuti acquistare con 50 milioni di euro? Bastardi traditori della patria!!'],\n ['SIAMO ALLA FOLLIA'],\n ['20px Caro editor, encontramos problemas na edição que fez na página Sertanejo universitário. A edição teve de ser revertida por não ser adequada para a Wikipédia. Se quiser experimentar a edição de páginas pode fazê-lo à vontade na página de testes da Wikipédia. Recomenda-se a leitura das páginas Breve introdução sobre a Wikipédia, O que a Wikipédia não é e Erros comuns na Wikipédia. Obrigado pela compreensão.    Vitor       Mazuco    Msg '],\n ['Le contributeur  y  tente de prouver par l absurde que le commentaire de diff du contributeur  x  est ridicule en recopiant ce dernier, et supprime sans autre explication un passage apparemment parfaitement consensuel. Qui plus est, le contributeur  y  ne prend pas la peine de discuter de la précédente contribution du contributeur  x , alors que l article a déjà un bandeau d avertissement à ne pas se lancer dans des guerres d édition. Bref, la prochaine fois, je vous bloque pour désorganisation du projet en vue d une argumentation personnelle. L article est déjà assez instable pour que vous n y mêliez pas une guerre d ego - et si vous n aimez pas qu on vous rappelle de ne pas  jouer au con , qui n est en rien une insulte, mais la détection d un problème de comportement, n y jouez pas. SammyDay (discuter) ']]\n\n\n\n\nMoving our trained XLM-Roberta model binary files into our multilingual_toxic_comment_files demo directory.\nWe have saved our fine-tuned model in outout/working/multilingual_toxic_comment_files/ directory and we’ll move our model files to demos/multilingual_toxic_comment_files/ directory as specified above.\nWe use Python’s shutil.move() method and passing in src(the source path of the target file) and dst (the destination folder path of the target file to be moved into) parameters.\n\n\nCode\n## Importing Libraries\nimport shutil\n\n## Create a source path for our target model\nmultilingual_toxic_comment_model_dir_path = f\"{ROOT_DIR}\\\\output\\\\working\\\\Multilingual_toxic_comment_classifier\\\\\"\n\n## Create a destination path for our target model\nmultilingual_toxic_comment_model_dir_destination = multilingual_toxic_comment_demo_path\n\n## Try to move the file\ntry:\n    print(f\"Attempting to move the {multilingual_toxic_comment_model_dir_path} to {multilingual_toxic_comment_model_dir_destination}\")\n    \n    ## Move the model\n    shutil.move(src=multilingual_toxic_comment_model_dir_path,\n           dst=multilingual_toxic_comment_model_dir_destination)\n    \n    print(\"Model move completed\")\n## If the model has already been moved, check if it exists\nexcept:\n    print(f\"No model found at {multilingual_toxic_comment_model_dir_path}, perhaps it's already moved.\")\n    print(f\"Model already exists at {multilingual_toxic_comment_model_dir_destination}: {multilingual_toxic_comment_model_dir_destination.exists()}\")\n\n\nAttempting to move the E:\\MultiLingual-Toxic-Comment-Classification\\output\\working\\Multilingual_toxic_comment_classifier\\ to E:\\MultiLingual-Toxic-Comment-Classification\\demos\\multilingual_toxic_comment_files\nModel move completed"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-gradio-app-into-a-python-script-app.py",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#turning-our-gradio-app-into-a-python-script-app.py",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Turning our Gradio App into a Python Script (app.py)",
    "text": "Turning our Gradio App into a Python Script (app.py)\n\n\nCode\n## Now if we look into which directory we are currently, we'll find that using the following code\nimport os\nos.getcwd()\n\n\n'E:\\\\MultiLingual-Toxic-Comment-Classification\\\\notebooks'\n\n\nNow we will move into the demos directory where we will write some helper utilities.\nIn cd ../demos/: .. means we are moving outside of the notebooks directory. demos/: means we moving inside the demos directory.\n\n\nCode\ncd ../demos/\n\n\nE:\\MultiLingual-Toxic-Comment-Classification\\demos\n\n\n\n\nCode\nimport tensorflow as tf\nimport gradio as gr\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\nmodel_save_path = \"Multilingual_toxic_comment_classifier/\"\n### Loading the fine-tuned model ###\nloaded_model = tf.keras.models.load_model(model_save_path)\n### Initializing the tokenizer ###\ntokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n\nexamples_list = [\n    [example]\n    for example in pd.read_csv(\"examples/sample_comments.csv\")[\"comment_text\"].tolist()\n]\n\n\ndef prep_data(text, tokenizer, max_len=192):\n    tokens = tokenizer(\n        text,\n        max_length=max_len,\n        truncation=True,\n        padding=\"max_length\",\n        add_special_tokens=True,\n        return_tensors=\"tf\",\n    )\n\n    return {\n        \"input_ids\": tokens[\"input_ids\"],\n        \"attention_mask\": tokens[\"attention_mask\"],\n    }\n\n\ndef predict(text):\n    prob_of_toxic_comment = loaded_model.predict(\n        prep_data(text=text, tokenizer=tokenizer_, max_len=192)\n    )[0][0]\n    prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n    prob_of_toxic_comment, prob_of_non_toxic_comment\n    probs = {\n        \"prob_of_toxic_comment\": float(prob_of_toxic_comment),\n        \"prob_of_non_toxic_comment\": float(prob_of_non_toxic_comment),\n    }\n    return probs\n\n\ninterface = gr.Interface(\n    fn=predict,\n    inputs=gr.components.Textbox(lines=4, label=\"Comment\"),\n    outputs=[gr.Label(label=\"Probabilities\")],\n    examples=examples_list,\n    title=\"Multi-Lingual Toxic Comment Classification.\",\n    description=\"XLM-Roberta Large model\",\n)\ninterface.launch(debug=False)\n\n\nOverwriting multilingual_toxic_comment_files/app.py\n\n\n\nCreating a requirements.txt file for our Gradio App(requirements.txt)\nThis is the last file we need to create for our application.\nThis file contains all the necessary packages for our Gradio application.\nWhen we deploy our demo app to HuggingFace Spaces, it will search through this file and install the dependencies we mention so our appication can run.\n\ntensorflow==2.12\npandas==1.5.2\ngradio==3.1.4\ntransformers==4.28.1\n\n\n\nCode\n%%writefile multilingual_toxic_comment_files/requirements.txt\ntensorflow==2.12\npandas==1.5.2\ngradio==3.1.4\ntransformers==4.28.1\n\n\nOverwriting multilingual_toxic_comment_files/requirements.txt"
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#deploying-our-application-to-huggingface-spaces",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#deploying-our-application-to-huggingface-spaces",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Deploying our Application to HuggingFace Spaces",
    "text": "Deploying our Application to HuggingFace Spaces\nTo deploy our demo, there are 2 main options for uploading to HuggingFace Spaces\n\nUploading via the Hugging Face Web Interface (easiest)\nUploading via the command line or terminal\n\nNOTE: To host any application on HuggingFace, we first need to sign up for a free HuggingFace Account\n\nRunning our Application locally\n\nOpen the terminal or command prompt.\nChanging the multilingual_toxic_comment_files directory (cd multilingual_toxic_comment_files).\nCreating an environment (python3 -m venv env) or use (python -m venv env).\nActivating the environment (source env/Scripts/activate).\nInstalling the requirements.txt using pip install -r requirements.txt. &gt; If faced any errors, we might need to upgrade pip using pip install --upgrade pip.\n\nRun the app (python3 app.py).\n\nThis should results in a Gradio demo locally at the URL such as : http://127.0.0.1:7860/.\n\n\nUploading to Hugging Face\nWe’ve verified our Melanoma_skin_cancer detection application is working in our local system.\nTo upload our application to Hugging Face Spaces, we need to do the following.\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to our profile at the top right corner and then select New Space.\nDeclare the name to the space like Chirag1994/multilingual_toxic_comment_classification_app.\nSelect a license (I am using MIT license).\nSelect Gradio as the Space SDK (software development kit).\nChoose whether your Space is Public or Private (I am keeping it Public).\nClick Create Space.\nClone the repository locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in the terminal or command prompt. In our case mine would be like - git clone https://huggingface.co/spaces/Chirag1994/multilingual_toxic_comment_classification_app.\nCopy/Move the contents of the downloaded multilingual_toxic_comment_classification_app folder to the cloned repo folder.\nTo upload files and track larger files (e.g., files that are greater than 10MB) for them we need to install Git LFS which stands for Git large File Storage.\nOpen up the cloned directory using VS code (I’m using VS code), and use the terminal (git bash in my case) and after installing the git lfs, use the command git lfs install to start tracking the file that we want to track. For example - git lfs track \"Multilingual_toxic_comment_classifier\" directory files.\nCreate a new .gitignore file and the files & folders that we don’t want git to track like :\n\n__pycache__/\n.vscode/\nvenv/\n.gitignore\n.gitattributes\n\nAdd the rest of the files and commit them with:\n\ngit add .\ngit commit -m \"commit message that you want\"\n\nPush(load) the files to Hugging Face\n\ngit push\n\nIt might a couple of minutes to finish and then the app will be up and running."
  },
  {
    "objectID": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#our-final-application-deployed-on-huggingface-spaces",
    "href": "portfolio/NLP_&_GenerativeAI_Portfolio/Multlingual_Toxic_Comment_Classification/Multilingual_Toxic_Comment_Classification_Final.html#our-final-application-deployed-on-huggingface-spaces",
    "title": "Multilingual Toxic Comment Classification using Tensorflow and TPUs",
    "section": "Our Final Application deployed on HuggingFace Spaces",
    "text": "Our Final Application deployed on HuggingFace Spaces\n\n\nCode\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://chirag1994-multilingual-toxic-comment-classifier.hf.space\", width=1000, height=800)"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "",
    "text": "Case_study_7_1.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#introduction",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#introduction",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "Introduction",
    "text": "Introduction\nBalanced Tree Clothing Company prides themselves on providing an optimised range of clothing and lifestyle wear for the modern adventurer!\nDanny, the CEO of this trendy fashion company has asked you to assist the team’s merchandising teams analyse their sales performance and generate a basic financial report to share with the wider business."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#available-data",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#available-data",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "Available Data",
    "text": "Available Data\nFor this case study there is a total of 4 datasets for this case study - however you will only need to utilise 2 main tables to solve all of the regular questions, and the additional 2 tables are used only for the bonus challenge question!\n\nProduct Details\nbalanced_tree.product_details includes all information about the entire range that Balanced Clothing sells in their store.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproduct_id\nprice\nproduct_name\ncategory_id\nsegment_id\nstyle_id\ncategory_name\nsegment_name\nstyle_name\n\n\n\n\nc4a632\n13\nNavy Oversized Jeans - Womens\n1\n3\n7\nWomens\nJeans\nNavy Oversized\n\n\ne83aa3\n32\nBlack Straight Jeans - Womens\n1\n3\n8\nWomens\nJeans\nBlack Straight\n\n\ne31d39\n10\nCream Relaxed Jeans - Womens\n1\n3\n9\nWomens\nJeans\nCream Relaxed\n\n\nd5e9a6\n23\nKhaki Suit Jacket - Womens\n1\n4\n10\nWomens\nJacket\nKhaki Suit\n\n\n72f5d4\n19\nIndigo Rain Jacket - Womens\n1\n4\n11\nWomens\nJacket\nIndigo Rain\n\n\n9ec847\n54\nGrey Fashion Jacket - Womens\n1\n4\n12\nWomens\nJacket\nGrey Fashion\n\n\n5d267b\n40\nWhite Tee Shirt - Mens\n2\n5\n13\nMens\nShirt\nWhite Tee\n\n\nc8d436\n10\nTeal Button Up Shirt - Mens\n2\n5\n14\nMens\nShirt\nTeal Button Up\n\n\n2a2353\n57\nBlue Polo Shirt - Mens\n2\n5\n15\nMens\nShirt\nBlue Polo\n\n\nf084eb\n36\nNavy Solid Socks - Mens\n2\n6\n16\nMens\nSocks\nNavy Solid\n\n\nb9a74d\n17\nWhite Striped Socks - Mens\n2\n6\n17\nMens\nSocks\nWhite Striped\n\n\n2feb6b\n29\nPink Fluro Polkadot Socks - Mens\n2\n6\n18\nMens\nSocks\nPink Fluro Polkadot\n\n\n\n\n\nProduct Sales\nbalanced_tree.sales contains product level information for all the transactions made for Balanced Tree including quantity, price, percentage discount, member status, a transaction ID and also the transaction timestamp.\n\n\n\n\n\n\n\n\n\n\n\n\nprod_id\nqty\nprice\ndiscount\nmember\ntxn_id\nstart_txn_time\n\n\n\n\nc4a632\n4\n13\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n5d267b\n4\n40\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nb9a74d\n4\n17\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\n2feb6b\n2\n29\n17\nt\n54f307\n2021-02-13 01:59:43.296\n\n\nc4a632\n5\n13\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\ne31d39\n2\n10\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n72f5d4\n3\n19\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\n2a2353\n3\n57\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nf084eb\n3\n36\n21\nt\n26cc98\n2021-01-19 01:39:00.3456\n\n\nc4a632\n1\n13\n21\nf\nef648d\n2021-01-27 02:18:17.1648\n\n\n\n\n\nProduct Hierarcy & Product Price\nThes tables are used only for the bonus question where we will use them to recreate the balanced_tree.product_details table.\nbalanced_tree.product_hierarchy\n\n\n\nid\nparent_id\nlevel_text\nlevel_name\n\n\n\n\n1\n\nWomens\nCategory\n\n\n2\n\nMens\nCategory\n\n\n3\n1\nJeans\nSegment\n\n\n4\n1\nJacket\nSegment\n\n\n5\n2\nShirt\nSegment\n\n\n6\n2\nSocks\nSegment\n\n\n7\n3\nNavy Oversized\nStyle\n\n\n8\n3\nBlack Straight\nStyle\n\n\n9\n3\nCream Relaxed\nStyle\n\n\n10\n4\nKhaki Suit\nStyle\n\n\n11\n4\nIndigo Rain\nStyle\n\n\n12\n4\nGrey Fashion\nStyle\n\n\n13\n5\nWhite Tee\nStyle\n\n\n14\n5\nTeal Button Up\nStyle\n\n\n15\n5\nBlue Polo\nStyle\n\n\n16\n6\nNavy Solid\nStyle\n\n\n17\n6\nWhite Striped\nStyle\n\n\n18\n6\nPink Fluro Polkadot\nStyle\n\n\n\nbalanced_tree.product_prices\n\n\n\nid\nproduct_id\nprice\n\n\n\n\n7\nc4a632\n13\n\n\n8\ne83aa3\n32\n\n\n9\ne31d39\n10\n\n\n10\nd5e9a6\n23\n\n\n11\n72f5d4\n19\n\n\n12\n9ec847\n54\n\n\n13\n5d267b\n40\n\n\n14\nc8d436\n10\n\n\n15\n2a2353\n57\n\n\n16\nf084eb\n36\n\n\n17\nb9a74d\n17\n\n\n18\n2feb6b\n29"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#interactive-sql-session",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#interactive-sql-session",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "Interactive SQL Session",
    "text": "Interactive SQL Session\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_7_2-2.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#case-study-questions",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "Case Study Questions",
    "text": "Case Study Questions\nThe following questions can be considered key business questions and metrics that the Balanced Tree team requires for their monthly reports.\nEach question can be answered using a single query - but as you are writing the SQL to solve each individual problem, keep in mind how you would generate all of these metrics in a single SQL script which the Balanced Tree team can run each month.\n\nHigh Level Sales Analysis\n\nWhat was the total quantity sold for all products?\nWhat is the total generated revenue for all products before discounts?\nWhat was the total discount amount for all products?\n\n\n\nTransaction Analysis\n\nHow many unique transactions were there?\nWhat is the average unique products purchased in each transaction?\nWhat are the 25th, 50th and 75th percentile values for the revenue per transaction?\nWhat is the average discount value per transaction?\nWhat is the percentage split of all transactions for members vs non-members?\nWhat is the average revenue for member transactions and non-member transactions?\n\n\n\nProduct Analysis\n\nWhat are the top 3 products by total revenue before discount?\nWhat is the total quantity, revenue and discount for each segment?\nWhat is the top selling product for each segment?\nWhat is the total quantity, revenue and discount for each category?\nWhat is the top selling product for each category?\nWhat is the percentage split of revenue by product for each segment?\nWhat is the percentage split of revenue by segment for each category?\nWhat is the percentage split of total revenue by category?\nWhat is the total transaction “penetration” for each product? (hint: penetration = number of transactions where at least 1 quantity of a product was purchased divided by total number of transactions)\nWhat is the most common combination of at least 1 quantity of any 3 products in a 1 single transaction?\n\n\n\nReporting Challenge\nWrite a single SQL script that combines all of the previous questions into a scheduled report that the Balanced Tree team can run at the beginning of each month to calculate the previous month’s values.\nImagine that the Chief Financial Officer (which is also Danny) has asked for all of these questions at the end of every month.\nHe first wants you to generate the data for January only - but then he also wants you to demonstrate that you can easily run the samne analysis for February without many changes (if at all).\nFeel free to split up your final outputs into as many tables as you need - but be sure to explicitly reference which table outputs relate to which question for full marks :)\n\n\nBonus Challenge\nUse a single SQL query to transform the product_hierarchy and product_prices datasets to the product_details table.\nHint: you may want to consider using a recursive CTE to solve this problem!"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#lets-start-solving",
    "href": "portfolio/SQL_Portfolio/Balanced_Tree/Case_Study_7-Balanced_Tree_Clothing_Co..html#lets-start-solving",
    "title": "Case Study #7 - Balanced Tree Clothing Co.",
    "section": "Let’s start solving",
    "text": "Let’s start solving\n\nHigh Level Sales Analysis\n\n1. What was the total quantity sold for all products?\n\nExplanation:\nThis set of SQL queries provides a high-level sales analysis by calculating the total quantity sold for all products. Here’s a breakdown:\n1) Total Quantity Sold for All Products:\nSELECT SUM(qty) AS total_quantity_sold FROM sales;\nCalculates the overall total quantity sold for all products from the ‘sales’ table.\nFinal Concise Explanation: These SQL queries provide insights into the high-level sales analysis by calculating both the overall total quantity sold for all products and the total quantity sold for each individual product.\nPart 1\n\n\n\nCase_study_7_3-2.png\n\n\nPart 2\nExplanation:\n2) Total Quantity Sold for Each Product:\nSELECT PD.product_name, SUM(S.qty) AS total_quantity_sold\nFROM product_details AS PD \nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_quantity_sold DESC;\nCalculates the total quantity sold for each product by joining the ‘product_details’ and ‘sales’ tables on the product_id, grouping the results by product_name, and ordering them in descending order based on the total quantity sold.\nFinal Concise Explanation: The second query gives a breakdown of the quantity sold for each product, ordered from highest to lowest.\n\n\n\nCase_study_7_4.png\n\n\n\n2. What is the total generated revenue for all products before discounts?\n\nExplanation:\nThis set of SQL queries focuses on analyzing the total generated revenue for all products before discounts. Here’s a breakdown:\n1) Total Revenue Before Discounts:\nSELECT SUM(qty * price) AS total_revenue FROM sales;\nCalculates the overall total revenue before discounts by multiplying the quantity sold (qty) with the price for each product in the ‘sales’ table and then summing the results.\nFinal Concise Explanation: These SQL queries provide insights into the total generated revenue for all products before discounts. The first query gives the overall total revenue.\nPart 1\n\n\n\nCase_study_7_5.png\n\n\nPart 2\nExplanation:\n2) Total Revenue Before Discounts for Each Product:\nSELECT PD.product_name, SUM(S.qty * S.price) AS total_revenue\nFROM product_details AS PD \nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_revenue DESC;\nCalculates the total revenue before discounts for each product by joining the ‘product_details’ and ‘sales’ tables on the product_id, multiplying the quantity sold (qty) with the price, grouping the results by product_name, and ordering them in descending order based on the total revenue.\nFinal Concise Explanation: The second query breaks down the total revenue for each individual product, ordered from highest to lowest.\n\n\n\nCase_study_7_26.png\n\n\n\n3. What was the total discount amount for all products?\n\nExplanation:\nThis set of SQL queries focuses on analyzing the total discount amount for all products. Here’s a breakdown:\n1) Total Discount Amount:\nSELECT ROUND(SUM((qty * price * discount) / 100), 2) AS total_discount FROM sales;\nCalculates the overall total discount amount for all products by multiplying the quantity sold (qty) with the price, applying the discount percentage, summing the results, and rounding the total discount to two decimal places.\nFinal Concise Explanation: These SQL queries provide insights into the total discount amount for all products. The first query gives the overall total discount.\nPart 1\n\n\n\nCase_study_7_6.png\n\n\nPart 2\nExplanation:\n2) Total Discount Amount for Each Product:\nSELECT PD.product_name, ROUND(SUM((S.qty * S.price * S.discount) / 100), 2) AS total_discount\nFROM product_details AS PD \nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name\nORDER BY total_discount DESC;\nCalculates the total discount amount for each product by joining the ‘product_details’ and ‘sales’ tables on the product_id, applying the discount percentage, summing the results, rounding the total discount to two decimal places, grouping the results by product_name, and ordering them in descending order based on the total discount.\nFinal Concise Explanation: The second query breaks down the total discount for each individual product, ordered from highest to lowest.\n\n\n\nCase_study_7_27.png\n\n\n\n\nTransaction Analysis\n\n1. How many unique transactions were there?\n\nExplanation: This SQL query provides the count of unique transactions in the ‘sales’ table by counting the distinct values of the ‘txn_id’ column.\n\n\n\nCase_study_7_8.png\n\n\n\n2. What is the average unique products purchased in each transaction?\n\nExplanation:\nThis SQL query focuses on analyzing the average number of unique products purchased in each transaction.\n1) Counting Unique Products for Each Transaction:\nSELECT txn_id, COUNT(DISTINCT prod_id) AS unique_products_purchased\nFROM sales\nGROUP BY txn_id;\nCounts the number of unique products purchased for each transaction by applying the COUNT aggregation function to the distinct values of the ‘prod_id’ column and grouping the results by ‘txn_id’.\n2) Calculating the Average Unique Products Purchased:\nSELECT ROUND(AVG(unique_products_purchased), 0) AS avg_unique_products_purchased \nFROM (\n    -- Subquery to count unique products for each transaction\n    SELECT txn_id, COUNT(DISTINCT prod_id) AS unique_products_purchased\n    FROM sales\n    GROUP BY txn_id\n) AS unique_products_count;\nCalculates the average number of unique products purchased in each transaction by applying the AVG aggregation function to the results of the subquery (count of unique products for each transaction).\nFinal Concise Explanation: This SQL query provides the average number of unique products purchased in each transaction by counting the distinct values of ‘prod_id’ for each transaction and then calculating the average of these counts. The result is rounded to the nearest whole number.\n\n\n\nCase_study_7_9.png\n\n\n\n3. What are the 25th, 50th and 75th percentile values for the revenue per transaction?\n\nExplanation:\nThis SQL query analyzes the 25th, 50th, and 75th percentile values for the revenue per transaction. Here’s a concise breakdown:\n1) Calculating Revenue Per Transaction:\nWITH revenue_per_transaction AS (\nSELECT S.txn_id, SUM(S.qty * S.price) AS total_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY S.txn_id\nORDER BY S.txn_id)\nCreates a temporary table ‘revenue_per_transaction’ that calculates the total revenue for each transaction by joining ‘product_details’ and ‘sales’ tables on the product_id, grouping by txn_id, and ordering by txn_id.\n2) Calculating Percentile Values:\nSELECT\nMAX(CASE WHEN percentile_group = 1 THEN total_revenue END) AS percentile_25,\nMAX(CASE WHEN percentile_group = 2 THEN total_revenue END) AS percentile_50,\nMAX(CASE WHEN percentile_group = 3 THEN total_revenue END) AS percentile_75\nFROM \n    (SELECT txn_id, total_revenue, NTILE(4) OVER (ORDER BY total_revenue) as percentile_group\n     FROM revenue_per_transaction) AS percentile_groups;\nCalculates the 25th (percentile_25), 50th (percentile_50), and 75th (percentile_75) percentile values for the revenue per transaction using the NTILE window function. The results are obtained by grouping the revenue values into four equal parts (quartiles) and extracting the maximum value for each quartile.\nFinal Concise Explanation: This SQL query provides the 25th, 50th, and 75th percentile values for the revenue per transaction by calculating the total revenue for each transaction and then using the NTILE window function to group the transactions into quartiles. The results are extracted by selecting the maximum value for each quartile.\n\n\n\nCase_study_7_10.png\n\n\n\n4. What is the average discount value per transaction?\n\nExplanation:\nThis SQL query focuses on analyzing the average discount value per transaction. \n1) Calculating Discount Value for Each Transaction:\nSELECT txn_id, ROUND(SUM((price * qty * discount) / 100), 0) AS discount_value\nFROM sales\nGROUP BY txn_id;\nCalculates the discount value for each transaction by multiplying the price, quantity (qty), and discount percentage, summing the results, and rounding the total discount to the nearest whole number. The results are grouped by ‘txn_id’.\n2) Calculating Average Discount Value:\nSELECT ROUND(AVG(discount_value), 1) AS avg_discount_value\nFROM (\n    -- Subquery to calculate discount value for each transaction\n    SELECT txn_id, ROUND(SUM((price * qty * discount) / 100), 0) AS discount_value\n    FROM sales\n    GROUP BY txn_id\n) AS discount_table;\nCalculates the average discount value per transaction by applying the AVG aggregation function to the results of the subquery (discount value for each transaction) and rounding the average to one decimal place.\nFinal Concise Explanation: This SQL query provides the average discount value per transaction by calculating the discount value for each transaction and then finding the average of these values. The result is rounded to one decimal place.\n\n\n\nCase_study_7_11.png\n\n\n\n5. What is the percentage split of all transactions for members vs non-members?\n\nExplanation:\nThis SQL query analyzes the percentage split of all transactions for members vs. non-members.\n1) Calculating Member Transaction Percentage:\nROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 't' THEN txn_id ELSE 0 END))\n/ (SELECT COUNT(DISTINCT txn_id) FROM sales), 2) AS member_transaction_pct\nCalculates the percentage of transactions made by members by counting the distinct ‘txn_id’ values for member transactions (‘t’) and dividing it by the total count of distinct transactions. The result is rounded to two decimal places.\n2) Calculating Non-Member Transaction Percentage:\nROUND(100.0 * (COUNT(DISTINCT CASE WHEN member = 'f' THEN txn_id ELSE 0 END))\n/ (SELECT COUNT(DISTINCT txn_id) FROM sales), 2) AS non_member_transaction_pct\nCalculates the percentage of transactions made by non-members by counting the distinct ‘txn_id’ values for non-member transactions (‘f’) and dividing it by the total count of distinct transactions. The result is rounded to two decimal places.\nFinal Concise Explanation: This SQL query provides the percentage split of all transactions for members vs. non-members. It calculates the percentage of transactions made by members and non-members and presents the results rounded to two decimal places.\n\n\n\nCase_study_7_12.png\n\n\n\n6. What is the average revenue for member transactions and non-member transactions?\n\nExplanation:\nThis SQL query analyzes the average revenue for member transactions and non-member transactions.\n1) Calculating Average Revenue for Member Transactions:\nWITH member_transactions_cte AS (\nSELECT member, txn_id, SUM(qty * price) AS avg_revenue\nFROM sales \nGROUP BY member, txn_id)\nCreates a Common Table Expression (CTE) named ‘member_transactions_cte’ that calculates the revenue for each transaction and groups the results by member status (‘member’) and transaction ID (‘txn_id’).\n2) Calculating Average Revenue for Members and Non-Members:\nSELECT member, ROUND(AVG(avg_revenue), 2) AS avg_member_transactions\nFROM member_transactions_cte\nGROUP BY member;\nCalculates the average revenue for member transactions and non-member transactions by applying the AVG aggregation function to the revenue values within the ‘member_transactions_cte’ CTE. The results are rounded to two decimal places and grouped by member status.\nFinal Concise Explanation: This SQL query provides the average revenue for member transactions and non-member transactions by calculating the revenue for each transaction, grouping the results by member status, and then finding the average revenue for each group. The result is rounded to two decimal places.\n\n\n\nCase_study_7_13.png\n\n\n\n\nProduct Analysis\n\n1. What are the top 3 products by total revenue before discount?\n\nExplanation:\nThis SQL query analyzes the average revenue for member transactions and non-member transactions. It calculates the total revenue before discount for each product by joining the ‘product_details’ and ‘sales’ tables on the product_id, multiplying the price by the quantity (qty), summing the results, and grouping by product_name. Orders the results in descending order based on total revenue before discount and limits the output to the top 3 products.\n\n\n\nCase_study_7_14.png\n\n\n\n2. What is the total quantity, revenue and discount for each segment?\n\nExplanation:\nThis SQL query analyzes the total quantity, revenue before discount, and discount for each segment. Here’s a concise breakdown:\n1) Grouping Data by Segment:\nSELECT PD.segment_name,\nROUND(SUM(S.qty), 2) AS total_quantity,\nROUND(SUM(S.qty * S.price), 2) AS total_revenue_before_discount,\nROUND(SUM((S.qty * S.price * S.discount) / 100), 2) AS discount\nFROM product_details AS PD \nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.segment_name;\nGroups the data by ‘segment_name’ from the ‘product_details’ table and calculates the total quantity, total revenue before discount, and total discount for each segment by joining with the ‘sales’ table.\nFinal Concise Explanation: This SQL query provides the total quantity, revenue before discount, and discount for each segment by grouping the data based on the ‘segment_name’ from the ‘product_details’ table and performing the necessary calculations. The results are rounded to two decimal places for clarity.\n\n\n\nCase_study_7_15.png\n\n\n\n3. What is the top selling product for each segment?\n\nExplanation:\nThis SQL query identifies the top-selling product for each segment. Here’s a concise breakdown:\n1) Calculating Total Quantity Sold for Each Product and Segment:\nWITH segment_product_qty_sales_cte AS (\nSELECT PD.segment_name, PD.product_name, SUM(S.qty) AS total_qty_sold\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.segment_name, PD.product_name)\nCreates a Common Table Expression (CTE) named ‘segment_product_qty_sales_cte’ that calculates the total quantity sold for each product within each segment.\n2) Ranking Products within Each Segment::\ntop_selling_products_cte AS (\nSELECT segment_product_qty_sales_cte.segment_name, \n       segment_product_qty_sales_cte.product_name, \n       segment_product_qty_sales_cte.total_qty_sold, \n       ROW_NUMBER() OVER (PARTITION BY segment_product_qty_sales_cte.segment_name \n                         ORDER BY segment_product_qty_sales_cte.total_qty_sold DESC) \n                         AS row_num\nFROM segment_product_qty_sales_cte)\nCreates another CTE named ‘top_selling_products_cte’ that assigns a row number to each product within each segment based on the total quantity sold in descending order.\n3) Selecting the Top-Selling Product for Each Segment:\nSELECT top_selling_products_cte.segment_name, \n   top_selling_products_cte.product_name,\n   top_selling_products_cte.total_qty_sold\nFROM top_selling_products_cte\nWHERE row_num = 1;\nSelects the top-selling product for each segment by filtering only the rows where the row number is 1 within the ‘top_selling_products_cte’ CTE.\nFinal Concise Explanation:\nThis SQL query determines the top-selling product for each segment by first calculating the total quantity sold for each product within each segment and then ranking the products based on the total quantity sold. The final result selects the top-selling product for each segment based on the row number.\n\n\n\nCase_study_7_16.png\n\n\n\n4. What is the total quantity, revenue and discount for each category?\n\nExplanation:\nThis SQL query analyzes the total quantity, revenue before discount, and discount for each category. Here’s a concise breakdown:\n1) Grouping Data by Category:\nSELECT PD.category_name,\nROUND(SUM(S.qty), 2) AS total_quantity,\nROUND(SUM(S.qty * S.price), 2) AS total_revenue_before_discount,\nROUND(SUM((S.qty * S.price * S.discount) / 100), 2) AS discount\nFROM product_details AS PD \nJOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.category_name;\nGroups the data by ‘category_name’ from the ‘product_details’ table and calculates the total quantity, total revenue before discount, and total discount for each category by joining with the ‘sales’ table.\nFinal Concise Explanation: This SQL query provides the total quantity, revenue before discount, and discount for each category by grouping the data based on the ‘category_name’ from the ‘product_details’ table and performing the necessary calculations. The results are rounded to two decimal places for clarity.\n\n\n\nCase_study_7_17.png\n\n\n\n5. What is the top selling product for each category?\n\nExplanation:\nThis SQL query identifies the top-selling product for each category.\n1) Calculating Total Quantity Sold for Each Product and Category:\nWITH category_product_qty_sales_cte AS (\nSELECT PD.category_name, PD.product_name, SUM(S.qty) AS total_qty_sold\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.category_name, PD.product_name)\nCreates a Common Table Expression (CTE) named ‘category_product_qty_sales_cte’ that calculates the total quantity sold for each product within each category.\n2) Ranking Products within Each Category:\ntop_selling_products_cte AS (\nSELECT category_product_qty_sales_cte.category_name, \n       category_product_qty_sales_cte.product_name, \n       category_product_qty_sales_cte.total_qty_sold, \n       ROW_NUMBER() OVER (PARTITION BY category_product_qty_sales_cte.category_name \n                         ORDER BY category_product_qty_sales_cte.total_qty_sold DESC) \n                         AS row_num\nFROM category_product_qty_sales_cte)\nCreates another CTE named ‘top_selling_products_cte’ that assigns a row number to each product within each category based on the total quantity sold in descending order.\n3) Selecting the Top-Selling Product for Each Category:\nSELECT top_selling_products_cte.category_name, \n   top_selling_products_cte.product_name,\n   top_selling_products_cte.total_qty_sold\nFROM top_selling_products_cte\nWHERE row_num = 1;\nSelects the top-selling product for each category by filtering only the rows where the row number is 1 within the ‘top_selling_products_cte’ CTE.\nFinal Concise Explanation:\nThis SQL query determines the top-selling product for each category by first calculating the total quantity sold for each product within each category and then ranking the products based on the total quantity sold. The final result selects the top-selling product for each category based on the row number.\n\n\n\nCase_study_7_18.png\n\n\n\n6. What is the percentage split of revenue by product for each segment?\n\nExplanation:\nThis SQL query analyzes the percentage split of revenue by product for each segment. Here’s a concise breakdown:\n1) Calculating Segment-wise Product Revenue:\nWITH segment_product_revenue_cte AS (\nSELECT PD.segment_name, PD.product_name,\n       SUM(S.price * S.qty) AS segment_product_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.segment_name, PD.product_name)\nCreates a Common Table Expression (CTE) named ‘segment_product_revenue_cte’ that calculates the revenue for each product within each segment.\n2) Calculating the Percentage Split of Revenue:\nSELECT segment_name, product_name,\n   ROUND(100.0 * segment_product_revenue / (\n       SUM(segment_product_revenue) OVER (PARTITION BY segment_name)), 2)\n       AS revenue_pct\nFROM segment_product_revenue_cte\nORDER BY segment_name, product_name;\nCalculates the percentage split of revenue for each product within each segment by dividing the product’s revenue by the total revenue within the segment. The result is rounded to two decimal places.\nFinal Concise Explanation: This SQL query provides the percentage split of revenue by product for each segment by calculating the revenue for each product within each segment and then determining the percentage of each product’s revenue relative to the total revenue within the segment. The results are ordered by segment name and product name for clarity.\n\n\n\nCase_study_7_19.png\n\n\nThe output for the above query is:\n\n\n\nCase_study_7_20.png\n\n\n\n7. What is the percentage split of revenue by segment for each category?\n\nExplanation:\nThis SQL query analyzes the percentage split of revenue by segment for each category.\n1) Calculating Category-wise Segment Revenue:\nWITH category_segment_revenue_cte AS (\nSELECT PD.category_name, PD.segment_name,\n       SUM(S.price * S.qty) AS category_segment_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.category_name, PD.segment_name)\nCreates a Common Table Expression (CTE) named ‘category_segment_revenue_cte’ that calculates the revenue for each segment within each category.\n2) Calculating the Percentage Split of Revenue:\nSELECT category_name, segment_name,\n   ROUND(100.0 * category_segment_revenue / (\n       SUM(category_segment_revenue) OVER (PARTITION BY category_name)), 2)\n       AS revenue_pct\nFROM category_segment_revenue_cte\nORDER BY category_name, segment_name;\nCalculates the percentage split of revenue for each segment within each category by dividing the segment’s revenue by the total revenue within the category. The result is rounded to two decimal places.\nFinal Concise Explanation: This SQL query provides the percentage split of revenue by segment for each category by calculating the revenue for each segment within each category and then determining the percentage of each segment’s revenue relative to the total revenue within the category. The results are ordered by category name and segment name for clarity.\n\n\n\nCase_study_7_21.png\n\n\n\n8. What is the percentage split of total revenue by category?\n\nExplanation:\nThis SQL query analyzes the percentage split of total revenue by category.\n1) Calculating Category-wise Total Revenue:\nWITH category_revenue_cte AS (\nSELECT PD.category_name,\n       SUM(S.price * S.qty) AS category_revenue\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.category_name)\nCreates a Common Table Expression (CTE) named ‘category_revenue_cte’ that calculates the total revenue for each category.\n2) Calculating the Percentage Split of Total Revenue:\nSELECT category_name,\n   ROUND(100.0 * category_revenue / (\n       SUM(category_revenue) OVER()), 2)\n       AS revenue_pct\nFROM category_revenue_cte\nGROUP BY category_name\nORDER BY category_name;\nCalculates the percentage split of total revenue for each category by dividing the category’s revenue by the overall total revenue. The result is rounded to two decimal places.\nFinal Concise Explanation: This SQL query provides the percentage split of total revenue by category by calculating the total revenue for each category and then determining the percentage of each category’s revenue relative to the overall total revenue. The results are ordered by category name for clarity.\n\n\n\nCase_study_7_22.png\n\n\n\n9. What is the total transaction “penetration” for each product? (hint: penetration = number of transactions where at least 1 quantity of a product was purchased divided by total number of transactions)\n\nExplanation:\nThis SQL query calculates the total transaction “penetration” for each product.\n1) Calculating Product-wise Transaction Count:\nWITH product_transactions AS (\nSELECT PD.product_name,\n       COUNT(DISTINCT S.txn_id) AS product_transactions,\n       (SELECT COUNT(DISTINCT txn_id) FROM sales) AS total_number_of_transactions\nFROM product_details AS PD JOIN sales AS S ON PD.product_id = S.prod_id\nGROUP BY PD.product_name)\nCreates a Common Table Expression (CTE) named ‘product_transactions’ that counts the number of distinct transactions for each product and also calculates the total number of distinct transactions in the entire dataset.\n2) Calculating the Percentage Split of Total Revenue:\nSELECT product_name,\n   ROUND(100.0 * (product_transactions / total_number_of_transactions), 2) AS product_penetration\nFROM product_transactions\nORDER BY product_penetration DESC;\nCalculates the total transaction “penetration” for each product by dividing the product’s transaction count by the total number of transactions and multiplying by 100. The result is rounded to two decimal places and the output is ordered by product penetration in descending order.\nFinal Concise Explanation: This SQL query provides the total transaction “penetration” for each product by calculating the percentage of transactions where at least 1 quantity of a product was purchased relative to the total number of transactions. The results are ordered by product penetration in descending order for clarity.\n\n\n\nCase_study_7_23.png\n\n\nThe output of the above query is:\n\n\n\nCase_study_7_24.png\n\n\n\n10. What is the most common combination of at least 1 quantity of any 3 products in a 1 single transaction?\n\n\n\nReporting Challenge\nWrite a single SQL script that combines all of the previous questions into a scheduled report that the Balanced Tree team can run at the beginning of each month to calculate the previous month’s values.\nImagine that the Chief Financial Officer (which is also Danny) has asked for all of these questions at the end of every month.\nHe first wants you to generate the data for January only - but then he also wants you to demonstrate that you can easily run the samne analysis for February without many changes (if at all).\nFeel free to split up your final outputs into as many tables as you need - but be sure to explicitly reference which table outputs relate to which question for full marks :)\n\n\nBonus Challenge\nUse a single SQL query to transform the product_hierarchy and product_prices datasets to the product_details table.\nHint: you may want to consider using a recursive CTE to solve this problem!"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "1.png\n\n\n\n\n\nClique Bait is not like your regular online seafood store - the founder and CEO Danny, was also a part of a digital data analytics team and wanted to expand his knowledge into the seafood industry!\nIn this case study - you are required to support Danny’s vision and analyse his dataset and come up with creative solutions to calculate funnel fallout rates for the Clique Bait online store.\n\n\n\n\nFor this case study there is a total of 5 datasets which you will need to combine to solve all of the questions.\n\n\nCustomers who visit the Clique Bait website are tagged via their cookie_id.\n\n\n\nuser_id\ncookie_id\nstart_date\n\n\n\n\n397\n3759ff\n2020-03-30 00:00:00\n\n\n215\n863329\n2020-01-26 00:00:00\n\n\n191\neefca9\n2020-03-15 00:00:00\n\n\n89\n764796\n2020-01-07 00:00:00\n\n\n127\n17ccc5\n2020-01-22 00:00:00\n\n\n81\nb0b666\n2020-03-01 00:00:00\n\n\n260\na4f236\n2020-01-08 00:00:00\n\n\n203\nd1182f\n2020-04-18 00:00:00\n\n\n23\n12dbc8\n2020-01-18 00:00:00\n\n\n375\nf61d69\n2020-01-03 00:00:00\n\n\n\n\n\n\nCustomer visits are logged in this events table at a cookie_id level and the event_type and page_id values can be used to join onto relevant satellite tables to obtain further information about each event.\nThe sequence_number is used to order the events within each visit.\n\n\n\n\n\n\n\n\n\n\n\nvisit_id\ncookie_id\npage_id\nevent_type\nsequence_number\nevent_time\n\n\n\n\n719fd3\n3d83d3\n5\n1\n4\n2020-03-02 00:29:09.975502\n\n\nfb1eb1\nc5ff25\n5\n2\n8\n2020-01-22 07:59:16.761931\n\n\n23fe81\n1e8c2d\n10\n1\n9\n2020-03-21 13:14:11.745667\n\n\nad91aa\n648115\n6\n1\n3\n2020-04-27 16:28:09.824606\n\n\n5576d7\nac418c\n6\n1\n4\n2020-01-18 04:55:10.149236\n\n\n48308b\nc686c1\n8\n1\n5\n2020-01-29 06:10:38.702163\n\n\n46b17d\n78f9b3\n7\n1\n12\n2020-02-16 09:45:31.926407\n\n\n9fd196\nccf057\n4\n1\n5\n2020-02-14 08:29:12.922164\n\n\nedf853\nf85454\n1\n1\n1\n2020-02-22 12:59:07.652207\n\n\n3c6716\n02e74f\n3\n2\n5\n2020-01-31 17:56:20.777383\n\n\n\n\n\n\nThe event_identifier table shows the types of events which are captured by Clique Bait’s digital data systems.\n\n\n\nevent_type\nevent_name\n\n\n\n\n1\nPage View\n\n\n2\nAdd to Cart\n\n\n3\nPurchase\n\n\n4\nAd Impression\n\n\n5\nAd Click\n\n\n\n\n\n\nThis table shows information for the 3 campaigns that Clique Bait has ran on their website so far in 2020.\n\n\n\n\n\n\n\n\n\n\ncampaign_id\nproducts\ncampaign_name\nstart_date\nend_date\n\n\n\n\n1\n1-3\nBOGOF - Fishing For Compliments\n2020-01-01 00:00:00\n2020-01-14 00:00:00\n\n\n2\n4-5\n25% Off - Living The Lux Life\n2020-01-15 00:00:00\n2020-01-28 00:00:00\n\n\n3\n6-8\nHalf Off - Treat Your Shellf(ish)\n2020-02-01 00:00:00\n2020-03-31 00:00:00\n\n\n\n\n\n\nThis table lists all of the pages on the Clique Bait website which are tagged and have data passing through from user interaction events.\n\n\n\npage_id\npage_name\nproduct_category\nproduct_id\n\n\n\n\n1\nHome Page\nnull\nnull\n\n\n2\nAll Products\nnull\nnull\n\n\n3\nSalmon\nFish\n1\n\n\n4\nKingfish\nFish\n2\n\n\n5\nTuna\nFish\n3\n\n\n6\nRussian Caviar\nLuxury\n4\n\n\n7\nBlack Truffle\nLuxury\n5\n\n\n8\nAbalone\nShellfish\n6\n\n\n9\nLobster\nShellfish\n7\n\n\n10\nCrab\nShellfish\n8\n\n\n11\nOyster\nShellfish\n9\n\n\n12\nCheckout\nnull\nnull\n\n\n13\nConfirmation\nnull\nnull\n\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\n5.png\n\n\n\n\n\n\n\n\nUsing the following DDL schema details to create an ERD for all the Clique Bait datasets. Click_Here to access the DB Diagram tool to create the ERD.\n\n\n\n3.png\n\n\n\n\n\n4.png\n\n\n\n\n\nUsing the available datasets - answer the following questions using a single query for each one:\n\nHow many users are there?\nHow many cookies does each user have on average?\nWhat is the unique number of visits by all users per month?\nWhat is the number of events for each event type?\nWhat is the percentage of visits which have a purchase event?\nWhat is the percentage of visits which view the checkout page but do not have a purchase event?\nWhat are the top 3 pages by number of views?\nWhat is the number of views and cart adds for each product category?\nWhat are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nUse your 2 new output tables - answer the following questions:\n\nWhich product had the most views, cart adds and purchases?\nWhich product was most likely to be abandoned?\nWhich product had the highest view to purchase percentage?\nWhat is the average conversion rate from view to cart add?\nWhat is the average conversion rate from cart add to purchase?\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?\n\n\n\n\n\n\n\nUsing the available datasets - answer the following questions using a single query for each one:\n\n1. How many users are there?\n\nExplanation: This SQL query provides the count of unique users in the ‘users’ dataset.\n\n\n\n6.png\n\n\n\n2. How many cookies does each user have on average?\n\nExplanation:\nThis SQL query calculates the average number of cookies per user in the dataset:\n1) Calculating Cookie Count per User:\nWITH cookie AS (\nSELECT user_id, COUNT(cookie_id) AS cookie_count FROM users GROUP BY user_id)\nCreates a Common Table Expression (CTE) named ‘cookie’ that counts the number of cookies for each user in the ‘users’ table using the COUNT function and grouping by ‘user_id’.\n2) Calculating Average Cookie per User:\nSELECT ROUND(AVG(cookie_count), 0) AS average_cookie_per_user FROM cookie;\nCalculates the average number of cookies per user by taking the average of the ‘cookie_count’ column from the ‘cookie’ CTE. The result is rounded to zero decimal places for clarity.\nFinal Concise Explanation: This SQL query provides the average number of cookies per user in the dataset by counting the cookies for each user and then calculating the overall average.\n\n\n\n7.png\n\n\n\n3. What is the unique number of visits by all users per month?\n\nExplanation:\nThis SQL query provides the unique number of visits by all users per month in the ‘events’ dataset by extracting the month from the event timestamp and counting the distinct visit IDs for each month.\n\n\n\n8.png\n\n\n\n4. What is the number of events for each event type?\n\nExplantion: This SQL query provides the number of events for each event type by joining the ‘events’ table with the ‘event_identifier’ table, counting the events for each type, and presenting the results ordered by the count of events in descending order.\n\n\n\nCase_study_6_8.png\n\n\n\n5. What is the percentage of visits which have a purchase event?\n\nExplanation:\nThis SQL query calculates the percentage of visits that have a purchase event. Let’s break it down step by step:\n1) Joining Events with Event Identifiers:\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJoins the ‘events’ table (aliased as E) with the ‘event_identifier’ table (aliased as EI) based on the common column ‘event_type’. This allows us to associate event types with their corresponding names.\n2) Counting Visits with Purchase Events:\nCOUNT(DISTINCT E.visit_id)\nCounts the distinct visit IDs from the ‘events’ table (aliased as E) where the event type is associated with a purchase event. This gives the number of visits with at least one purchase event.\n3) Calculating the Percentage:\nROUND(100.0 * COUNT(DISTINCT E.visit_id) / (SELECT COUNT(DISTINCT visit_id) FROM events),2)\nCalculates the percentage by dividing the count of distinct visits with purchase events by the total count of distinct visits from the ‘events’ table. The result is multiplied by 100 and rounded to two decimal places.\n4) Filtering for Purchase Events:\nWHERE EI.event_name = 'Purchase'\nFilters the results to consider only events with the name ‘Purchase’. This ensures that only visits with purchase events are included in the calculation.\nFinal Concise Explanation: The SQL query calculates the percentage of visits that have a purchase event by counting the distinct visits with purchase events, dividing it by the total count of distinct visits, multiplying by 100, and rounding to two decimal places. The query only considers events with the name ‘Purchase’ for the calculation.\n\n\n\n10.png\n\n\n\n6. What is the percentage of visits which view the checkout page but do not have a purchase event?\n7. What are the top 3 pages by number of views?\n\nExplanation:\nThis SQL query identifies the top 3 pages by the number of views. Let’s break it down step by step:\n1) Filtering Relevant Events:\nWHERE event_type = '1'\nFilters events to consider only those with the event type equal to ‘1’. This assumes that event type ‘1’ represents page views.\n2) Grouping and Counting Views per Page:\nWITH top_3_pages AS\n(SELECT page_id, COUNT(DISTINCT visit_id) AS number_of_views\n FROM events\n WHERE event_type = '1'\n GROUP BY page_id\n ORDER BY number_of_views DESC\n LIMIT 3)\nCreates a Common Table Expression (CTE) named ‘top_3_pages’ that groups events by page_id, counts the distinct visit IDs for each page, orders the results by the number of views in descending order, and limits the output to the top 3 pages.\n3) Joining with Page Hierarchy:\nSELECT page_name, number_of_views\nFROM top_3_pages\nJOIN page_hierarchy ON top_3_pages.page_id = page_hierarchy.page_id;\nJoins the ‘top_3_pages’ CTE with the ‘page_hierarchy’ table based on the common column ‘page_id’ to get the corresponding page names for the top 3 pages.\nFinal Concise Explanation: This SQL query identifies the top 3 pages by the number of views by first filtering relevant events (assumed to be page views), grouping and counting views per page, and then joining with the page hierarchy to obtain the page names associated with the top 3 pages.\n\n\n\n11.png\n\n\n\n8. What is the number of views and cart adds for each product category?\n\nExplanation:\nThis SQL query calculates the number of views and cart adds for each product category.\n1) Joining Events with Page Hierarchy:\nFROM events as E JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nJoins the ‘events’ table (aliased as E) with the ‘page_hierarchy’ table (aliased as PH) based on the common column ‘page_id’.\n2) Filtering Relevant Rows:\nWHERE PH.product_category IS NOT NULL\nFilters the rows to include only those where the product category is not null. This ensures that only events related to products are considered.\n3) Counting Views and Cart Adds per Product Category:\nSUM(CASE WHEN E.event_type = '1' THEN 1 ELSE 0 END) AS number_of_views,\nSUM(CASE WHEN E.event_type = '2' THEN 1 ELSE 0 END) AS number_of_cart_adds\nUses conditional aggregation to count the number of views and cart adds for each product category. The CASE WHEN statement is used to determine if the event type is a view (event_type = ‘1’) or a cart add (event_type = ‘2’).\n4) Grouping by Product Category and Ordering:\nGROUP BY PH.product_category\nORDER BY PH.product_category;\nGroups the results by product category and orders them alphabetically by product category.\nFinal Concise Explanation: This SQL query calculates the number of views and cart adds for each product category by joining the ‘events’ table with the ‘page_hierarchy’ table, filtering relevant rows, and using conditional aggregation to count the events based on their types. The results are then grouped by product category and ordered alphabetically.\n\n\n\n12.png\n\n\n\n9. What are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nExplanation:\nThis SQL query is the first part of a product funnel analysis. It creates a temporary table (view_add_to_cart_cte) with details about how many times each product was viewed and added to the cart.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE view_add_to_cart_cte AS\nCreates a temporary table named view_add_to_cart_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name, \nPH.product_category,\nSUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\nSUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nSelects the product_id, product_name, product_category, and calculates the counts of page views and add-to-cart events using conditional aggregation.\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    PH.product_category IS NOT NULL\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the product category is not null, indicating relevant product-related events.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (view_add_to_cart_cte) with details about how many times each product was viewed and added to the cart. It calculates counts for page views and add-to-cart events, filtering for relevant product-related events. The results are grouped by product information.\n\n\n\n13.png\n\n\nExplanation:\nThis SQL query is the second part of a product funnel analysis. It creates a temporary table (products_abandoned_cte) with details about how many times each product was added to the cart but not purchased (abandoned).\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE products_abandoned_cte AS\nCreates a temporary table named products_abandoned_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name,\nPH.product_category, \nCOUNT(*) AS abandoned\nSelects the product_id, product_name, product_category, and calculates the count of abandoned events (add-to-cart events not leading to a purchase).\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type \nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    WHERE EI.event_name = 'Purchase')\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the event is an ‘Add to Cart’ event and the visit_id is not associated with a ‘Purchase’ event.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (products_abandoned_cte) with details about how many times each product was added to the cart but not purchased (abandoned). It calculates counts for abandoned events, filtering for add-to-cart events not associated with a purchase. The results are grouped by product information.\n\n\n\n14.png\n\n\nExplanation:\nThis SQL query is the third part of a product funnel analysis. It creates a temporary table (products_purchased_cte) with details about how many times each product was purchased.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE products_purchased_cte AS\nCreates a temporary table named products_purchased_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name,\nPH.product_category, \nCOUNT(*) AS purchased\nSelects the product_id, product_name, product_category, and calculates the count of purchased events (add-to-cart events leading to a purchase).\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type \nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase')\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the event is an ‘Add to Cart’ event and the visit_id is associated with a ‘Purchase’ event.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (products_purchased_cte) with details about how many times each product was purchased. It calculates counts for purchased events, filtering for add-to-cart events associated with a purchase. The results are grouped by product information.\n\n\n\n15.png\n\n\nExplanation:\nThis SQL query combines the results from the previous temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) into a final temporary table (product_information). It then selects and displays the contents of the final temporary table.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE product_information  AS\nCreates a temporary table named product_information to store the results.\n2) Selecting and Joining Tables:\nSELECT \nVATC.*,\nAB.abandoned, \nPP.purchased\nFROM \n    view_add_to_cart_cte AS VATC\n    JOIN products_abandoned_cte AS AB ON VATC.product_id = AB.product_id\n    JOIN products_purchased_cte AS PP ON VATC.product_id = PP.product_id;\n\nSelects columns from the view_add_to_cart_cte and joins it with the products_abandoned_cte and products_purchased_cte tables based on the common column product_id.\nCombines information about the number of views, cart additions, abandoned events, and purchased events for each product.\n\n3) Selecting from the Temporary Table:\nSELECT * FROM product_information\nORDER BY product_id;\n\nSelects and displays all columns from the final temporary table (product_information).\nOrders the results by product_id for better readability.\n\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\n5) Dropping Temporary Tables:\nDROP TEMPORARY TABLE IF EXISTS view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte;\nThis SQL query drops the temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) when they are no longer needed.\nFinal Concise Explanation: This query combines the results from the previous temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) into a final temporary table named product_information. It includes information about the number of views, cart additions, abandoned events, and purchased events for each product. The final results are then selected and displayed from the temporary table, ordered by product_id.\n\n\n\n16.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_view_add_to_cart_cte) to aggregate data for each product category instead of individual products.\n1) Aggregating Data by Product Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_view_add_to_cart_cte AS\nSELECT \n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    PH.product_category IS NOT NULL\nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_view_add_to_cart_cte: Initiates the creation of a temporary table named category_view_add_to_cart_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts: Calculates the total number of page views (view_counts) for each product category.\n- SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts: Calculates the total number of \"Add to Cart\" events (add_to_cart_counts) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Product Category:\n- WHERE PH.product_category IS NOT NULL: Filters out rows where the product category is null, focusing on valid product categories.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_view_add_to_cart_cte) that summarizes data for each product category. It calculates the total number of page views and “Add to Cart” events for each product category, providing a higher-level analysis of user interactions within different product categories.\n\n\n\n17.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_products_abandoned_cte) that counts the number of abandoned products for each product category.\n1) Creating Table for Abandoned Products by Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_products_abandoned_cte AS\nSELECT \n    PH.product_category, \n    COUNT(*) AS abandoned\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase'\n    ) \nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_products_abandoned_cte: Initiates the creation of a temporary table named category_products_abandoned_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS abandoned: Counts the number of rows (abandoned products) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Event Type and Excluding Purchased Visits:\n- WHERE EI.event_name = 'Add to Cart': Filters rows where the event is an \"Add to Cart\" event.\n- AND E.visit_id NOT IN (...): Excludes visits where a purchase event has occurred, preventing counting products that were eventually purchased.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_products_abandoned_cte) that counts the number of abandoned products for each product category. It focuses on “Add to Cart” events and excludes visits where a purchase event has occurred, providing insights into the abandonment behavior within different product categories.\n\n\n\n18.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_products_purchased_cte) that counts the number of purchased products for each product category.\n1) Creating Table for Purchased Products by Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_products_purchased_cte AS\nSELECT \n    PH.product_category, \n    COUNT(*) AS purchased\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase'\n    ) \nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_products_purchased_cte: Initiates the creation of a temporary table named category_products_purchased_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS purchased: Counts the number of rows (purchased products) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Event Type and Including Purchased Visits:\n- WHERE EI.event_name = 'Add to Cart': Filters rows where the event is an \"Add to Cart\" event.\n- AND E.visit_id IN (...): Includes visits where a purchase event has occurred, ensuring that only products added to the cart and later purchased are counted.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_products_purchased_cte) that counts the number of purchased products for each product category. It focuses on “Add to Cart” events and includes visits where a purchase event has occurred, providing insights into the products that were added to the cart and eventually purchased within different product categories.\n\n\n\n19.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_product_information) that aggregates information about product categories, including the number of views, additions to cart, abandoned products, and purchased products.\n1) Creating Table for Aggregated Category Product Information:\n-- Use temporary table instead of INTO for the final result\nCREATE TEMPORARY TABLE category_product_information AS\nSELECT \n    VATC.*,\n    AB.abandoned, \n    PP.purchased\nFROM \n    category_view_add_to_cart_cte AS VATC\n    JOIN category_products_abandoned_cte AS AB ON VATC.product_category = AB.product_category\n    JOIN category_products_purchased_cte AS PP ON VATC.product_category = PP.product_category;\n\n-- Select from the temporary table\nSELECT * FROM category_product_information\nORDER BY product_category;\n\n-- Drop the temporary tables when done\nDROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart_cte, category_products_abandoned_cte, \ncategory_products_purchased_cte;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_product_information: Initiates the creation of a temporary table named category_product_information.\n2) Data Selection and Joining Tables:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS purchased: Counts the number of rows (purchased products) for each product category.\n3) Table Joins:\n- FROM category_view_add_to_cart_cte AS VATC: Specifies the temporary table category_view_add_to_cart_cte with the alias VATC.\n- JOIN category_products_abandoned_cte AS AB ON VATC.product_category = AB.product_category: Joins the temporary table with the abandoned products table based on the product_category.\n- JOIN category_products_purchased_cte AS PP ON VATC.product_category = PP.product_category: Joins the temporary table with the purchased products table based on the product_category.\n4) Final Result:\n- The resulting temporary table (category_product_information) contains aggregated information for each product category, including the number of views, additions to cart, abandoned products, and purchased products.\n5) Selection from Temporary Table:\n- SELECT * FROM category_product_information ORDER BY product_category;: Retrieves all columns from the temporary table, ordering the results by product_category.\n6) Temporary Table Cleanup:\n- DROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart_cte, category_products_abandoned_cte, category_products_purchased_cte;: Drops the temporary tables used in the process.\nFinal Concise Explanation: This query creates a temporary table (category_product_information) that aggregates information about product categories, including the number of views, additions to cart, abandoned products, and purchased products. The temporary table is then selected, providing a summary of product category data, and the temporary tables are dropped when the process is completed.\n\n\n\n20.png\n\n\nUse your 2 new output tables - answer the following questions:\n\n1. Which product had the most views, cart adds and purchases?\n\nPart 1\n\n\n\n21.png\n\n\nPart 2\n\n\n\n22.png\n\n\nPart 3\n\n\n\n23.png\n\n\n\n2. Which product was most likely to be abandoned?\n\n\n\n\n24.png\n\n\n\n3. Which product had the highest view to purchase percentage?\n\n\n\n\n25.png\n\n\n\n4. What is the average conversion rate from view to cart add?\n\n\n\n\n26.png\n\n\n\n5. What is the average conversion rate from cart add to purchase?\n\n\n\n\n27.png\n\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#introduction",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#introduction",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "Clique Bait is not like your regular online seafood store - the founder and CEO Danny, was also a part of a digital data analytics team and wanted to expand his knowledge into the seafood industry!\nIn this case study - you are required to support Danny’s vision and analyse his dataset and come up with creative solutions to calculate funnel fallout rates for the Clique Bait online store."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#available-data",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#available-data",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "For this case study there is a total of 5 datasets which you will need to combine to solve all of the questions.\n\n\nCustomers who visit the Clique Bait website are tagged via their cookie_id.\n\n\n\nuser_id\ncookie_id\nstart_date\n\n\n\n\n397\n3759ff\n2020-03-30 00:00:00\n\n\n215\n863329\n2020-01-26 00:00:00\n\n\n191\neefca9\n2020-03-15 00:00:00\n\n\n89\n764796\n2020-01-07 00:00:00\n\n\n127\n17ccc5\n2020-01-22 00:00:00\n\n\n81\nb0b666\n2020-03-01 00:00:00\n\n\n260\na4f236\n2020-01-08 00:00:00\n\n\n203\nd1182f\n2020-04-18 00:00:00\n\n\n23\n12dbc8\n2020-01-18 00:00:00\n\n\n375\nf61d69\n2020-01-03 00:00:00\n\n\n\n\n\n\nCustomer visits are logged in this events table at a cookie_id level and the event_type and page_id values can be used to join onto relevant satellite tables to obtain further information about each event.\nThe sequence_number is used to order the events within each visit.\n\n\n\n\n\n\n\n\n\n\n\nvisit_id\ncookie_id\npage_id\nevent_type\nsequence_number\nevent_time\n\n\n\n\n719fd3\n3d83d3\n5\n1\n4\n2020-03-02 00:29:09.975502\n\n\nfb1eb1\nc5ff25\n5\n2\n8\n2020-01-22 07:59:16.761931\n\n\n23fe81\n1e8c2d\n10\n1\n9\n2020-03-21 13:14:11.745667\n\n\nad91aa\n648115\n6\n1\n3\n2020-04-27 16:28:09.824606\n\n\n5576d7\nac418c\n6\n1\n4\n2020-01-18 04:55:10.149236\n\n\n48308b\nc686c1\n8\n1\n5\n2020-01-29 06:10:38.702163\n\n\n46b17d\n78f9b3\n7\n1\n12\n2020-02-16 09:45:31.926407\n\n\n9fd196\nccf057\n4\n1\n5\n2020-02-14 08:29:12.922164\n\n\nedf853\nf85454\n1\n1\n1\n2020-02-22 12:59:07.652207\n\n\n3c6716\n02e74f\n3\n2\n5\n2020-01-31 17:56:20.777383\n\n\n\n\n\n\nThe event_identifier table shows the types of events which are captured by Clique Bait’s digital data systems.\n\n\n\nevent_type\nevent_name\n\n\n\n\n1\nPage View\n\n\n2\nAdd to Cart\n\n\n3\nPurchase\n\n\n4\nAd Impression\n\n\n5\nAd Click\n\n\n\n\n\n\nThis table shows information for the 3 campaigns that Clique Bait has ran on their website so far in 2020.\n\n\n\n\n\n\n\n\n\n\ncampaign_id\nproducts\ncampaign_name\nstart_date\nend_date\n\n\n\n\n1\n1-3\nBOGOF - Fishing For Compliments\n2020-01-01 00:00:00\n2020-01-14 00:00:00\n\n\n2\n4-5\n25% Off - Living The Lux Life\n2020-01-15 00:00:00\n2020-01-28 00:00:00\n\n\n3\n6-8\nHalf Off - Treat Your Shellf(ish)\n2020-02-01 00:00:00\n2020-03-31 00:00:00\n\n\n\n\n\n\nThis table lists all of the pages on the Clique Bait website which are tagged and have data passing through from user interaction events.\n\n\n\npage_id\npage_name\nproduct_category\nproduct_id\n\n\n\n\n1\nHome Page\nnull\nnull\n\n\n2\nAll Products\nnull\nnull\n\n\n3\nSalmon\nFish\n1\n\n\n4\nKingfish\nFish\n2\n\n\n5\nTuna\nFish\n3\n\n\n6\nRussian Caviar\nLuxury\n4\n\n\n7\nBlack Truffle\nLuxury\n5\n\n\n8\nAbalone\nShellfish\n6\n\n\n9\nLobster\nShellfish\n7\n\n\n10\nCrab\nShellfish\n8\n\n\n11\nOyster\nShellfish\n9\n\n\n12\nCheckout\nnull\nnull\n\n\n13\nConfirmation\nnull\nnull"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#interactive-sql-instance",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#interactive-sql-instance",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\n5.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#case-study-questions",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "Using the following DDL schema details to create an ERD for all the Clique Bait datasets. Click_Here to access the DB Diagram tool to create the ERD.\n\n\n\n3.png\n\n\n\n\n\n4.png\n\n\n\n\n\nUsing the available datasets - answer the following questions using a single query for each one:\n\nHow many users are there?\nHow many cookies does each user have on average?\nWhat is the unique number of visits by all users per month?\nWhat is the number of events for each event type?\nWhat is the percentage of visits which have a purchase event?\nWhat is the percentage of visits which view the checkout page but do not have a purchase event?\nWhat are the top 3 pages by number of views?\nWhat is the number of views and cart adds for each product category?\nWhat are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nUse your 2 new output tables - answer the following questions:\n\nWhich product had the most views, cart adds and purchases?\nWhich product was most likely to be abandoned?\nWhich product had the highest view to purchase percentage?\nWhat is the average conversion rate from view to cart add?\nWhat is the average conversion rate from cart add to purchase?\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#lets-start-solving-them.",
    "href": "portfolio/SQL_Portfolio/Clique_Bait/Case_Study_6-Clique_Bait.html#lets-start-solving-them.",
    "title": "Solving Case Study #6 - Clique Bait using SQL",
    "section": "",
    "text": "Using the available datasets - answer the following questions using a single query for each one:\n\n1. How many users are there?\n\nExplanation: This SQL query provides the count of unique users in the ‘users’ dataset.\n\n\n\n6.png\n\n\n\n2. How many cookies does each user have on average?\n\nExplanation:\nThis SQL query calculates the average number of cookies per user in the dataset:\n1) Calculating Cookie Count per User:\nWITH cookie AS (\nSELECT user_id, COUNT(cookie_id) AS cookie_count FROM users GROUP BY user_id)\nCreates a Common Table Expression (CTE) named ‘cookie’ that counts the number of cookies for each user in the ‘users’ table using the COUNT function and grouping by ‘user_id’.\n2) Calculating Average Cookie per User:\nSELECT ROUND(AVG(cookie_count), 0) AS average_cookie_per_user FROM cookie;\nCalculates the average number of cookies per user by taking the average of the ‘cookie_count’ column from the ‘cookie’ CTE. The result is rounded to zero decimal places for clarity.\nFinal Concise Explanation: This SQL query provides the average number of cookies per user in the dataset by counting the cookies for each user and then calculating the overall average.\n\n\n\n7.png\n\n\n\n3. What is the unique number of visits by all users per month?\n\nExplanation:\nThis SQL query provides the unique number of visits by all users per month in the ‘events’ dataset by extracting the month from the event timestamp and counting the distinct visit IDs for each month.\n\n\n\n8.png\n\n\n\n4. What is the number of events for each event type?\n\nExplantion: This SQL query provides the number of events for each event type by joining the ‘events’ table with the ‘event_identifier’ table, counting the events for each type, and presenting the results ordered by the count of events in descending order.\n\n\n\nCase_study_6_8.png\n\n\n\n5. What is the percentage of visits which have a purchase event?\n\nExplanation:\nThis SQL query calculates the percentage of visits that have a purchase event. Let’s break it down step by step:\n1) Joining Events with Event Identifiers:\nFROM events AS E\nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJoins the ‘events’ table (aliased as E) with the ‘event_identifier’ table (aliased as EI) based on the common column ‘event_type’. This allows us to associate event types with their corresponding names.\n2) Counting Visits with Purchase Events:\nCOUNT(DISTINCT E.visit_id)\nCounts the distinct visit IDs from the ‘events’ table (aliased as E) where the event type is associated with a purchase event. This gives the number of visits with at least one purchase event.\n3) Calculating the Percentage:\nROUND(100.0 * COUNT(DISTINCT E.visit_id) / (SELECT COUNT(DISTINCT visit_id) FROM events),2)\nCalculates the percentage by dividing the count of distinct visits with purchase events by the total count of distinct visits from the ‘events’ table. The result is multiplied by 100 and rounded to two decimal places.\n4) Filtering for Purchase Events:\nWHERE EI.event_name = 'Purchase'\nFilters the results to consider only events with the name ‘Purchase’. This ensures that only visits with purchase events are included in the calculation.\nFinal Concise Explanation: The SQL query calculates the percentage of visits that have a purchase event by counting the distinct visits with purchase events, dividing it by the total count of distinct visits, multiplying by 100, and rounding to two decimal places. The query only considers events with the name ‘Purchase’ for the calculation.\n\n\n\n10.png\n\n\n\n6. What is the percentage of visits which view the checkout page but do not have a purchase event?\n7. What are the top 3 pages by number of views?\n\nExplanation:\nThis SQL query identifies the top 3 pages by the number of views. Let’s break it down step by step:\n1) Filtering Relevant Events:\nWHERE event_type = '1'\nFilters events to consider only those with the event type equal to ‘1’. This assumes that event type ‘1’ represents page views.\n2) Grouping and Counting Views per Page:\nWITH top_3_pages AS\n(SELECT page_id, COUNT(DISTINCT visit_id) AS number_of_views\n FROM events\n WHERE event_type = '1'\n GROUP BY page_id\n ORDER BY number_of_views DESC\n LIMIT 3)\nCreates a Common Table Expression (CTE) named ‘top_3_pages’ that groups events by page_id, counts the distinct visit IDs for each page, orders the results by the number of views in descending order, and limits the output to the top 3 pages.\n3) Joining with Page Hierarchy:\nSELECT page_name, number_of_views\nFROM top_3_pages\nJOIN page_hierarchy ON top_3_pages.page_id = page_hierarchy.page_id;\nJoins the ‘top_3_pages’ CTE with the ‘page_hierarchy’ table based on the common column ‘page_id’ to get the corresponding page names for the top 3 pages.\nFinal Concise Explanation: This SQL query identifies the top 3 pages by the number of views by first filtering relevant events (assumed to be page views), grouping and counting views per page, and then joining with the page hierarchy to obtain the page names associated with the top 3 pages.\n\n\n\n11.png\n\n\n\n8. What is the number of views and cart adds for each product category?\n\nExplanation:\nThis SQL query calculates the number of views and cart adds for each product category.\n1) Joining Events with Page Hierarchy:\nFROM events as E JOIN page_hierarchy AS PH ON E.page_id = PH.page_id\nJoins the ‘events’ table (aliased as E) with the ‘page_hierarchy’ table (aliased as PH) based on the common column ‘page_id’.\n2) Filtering Relevant Rows:\nWHERE PH.product_category IS NOT NULL\nFilters the rows to include only those where the product category is not null. This ensures that only events related to products are considered.\n3) Counting Views and Cart Adds per Product Category:\nSUM(CASE WHEN E.event_type = '1' THEN 1 ELSE 0 END) AS number_of_views,\nSUM(CASE WHEN E.event_type = '2' THEN 1 ELSE 0 END) AS number_of_cart_adds\nUses conditional aggregation to count the number of views and cart adds for each product category. The CASE WHEN statement is used to determine if the event type is a view (event_type = ‘1’) or a cart add (event_type = ‘2’).\n4) Grouping by Product Category and Ordering:\nGROUP BY PH.product_category\nORDER BY PH.product_category;\nGroups the results by product category and orders them alphabetically by product category.\nFinal Concise Explanation: This SQL query calculates the number of views and cart adds for each product category by joining the ‘events’ table with the ‘page_hierarchy’ table, filtering relevant rows, and using conditional aggregation to count the events based on their types. The results are then grouped by product category and ordered alphabetically.\n\n\n\n12.png\n\n\n\n9. What are the top 3 products by purchases?\n\n\n\n\nUsing a single SQL query - create a new output table which has the following details:\n\nHow many times was each product viewed?\nHow many times was each product added to cart?\nHow many times was each product added to a cart but not purchased (abandoned)?\nHow many times was each product purchased?\n\nAdditionally, create another table which further aggregates the data for the above points but this time for each product category instead of individual products.\nExplanation:\nThis SQL query is the first part of a product funnel analysis. It creates a temporary table (view_add_to_cart_cte) with details about how many times each product was viewed and added to the cart.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE view_add_to_cart_cte AS\nCreates a temporary table named view_add_to_cart_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name, \nPH.product_category,\nSUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\nSUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nSelects the product_id, product_name, product_category, and calculates the counts of page views and add-to-cart events using conditional aggregation.\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type\nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    PH.product_category IS NOT NULL\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the product category is not null, indicating relevant product-related events.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (view_add_to_cart_cte) with details about how many times each product was viewed and added to the cart. It calculates counts for page views and add-to-cart events, filtering for relevant product-related events. The results are grouped by product information.\n\n\n\n13.png\n\n\nExplanation:\nThis SQL query is the second part of a product funnel analysis. It creates a temporary table (products_abandoned_cte) with details about how many times each product was added to the cart but not purchased (abandoned).\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE products_abandoned_cte AS\nCreates a temporary table named products_abandoned_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name,\nPH.product_category, \nCOUNT(*) AS abandoned\nSelects the product_id, product_name, product_category, and calculates the count of abandoned events (add-to-cart events not leading to a purchase).\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type \nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    WHERE EI.event_name = 'Purchase')\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the event is an ‘Add to Cart’ event and the visit_id is not associated with a ‘Purchase’ event.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (products_abandoned_cte) with details about how many times each product was added to the cart but not purchased (abandoned). It calculates counts for abandoned events, filtering for add-to-cart events not associated with a purchase. The results are grouped by product information.\n\n\n\n14.png\n\n\nExplanation:\nThis SQL query is the third part of a product funnel analysis. It creates a temporary table (products_purchased_cte) with details about how many times each product was purchased.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE products_purchased_cte AS\nCreates a temporary table named products_purchased_cte to store the results.\n2) Selecting Relevant Columns and Calculating Counts:\nSELECT \nPH.product_id, \nPH.page_name AS product_name,\nPH.product_category, \nCOUNT(*) AS purchased\nSelects the product_id, product_name, product_category, and calculates the count of purchased events (add-to-cart events leading to a purchase).\n3) Joining Tables and Filtering:\nFROM \nevents AS E \nJOIN event_identifier AS EI ON E.event_type = EI.event_type \nJOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase')\nJoins the ‘events’ table with ‘event_identifier’ and ‘page_hierarchy’ tables based on common columns. It filters only rows where the event is an ‘Add to Cart’ event and the visit_id is associated with a ‘Purchase’ event.\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\nFinal Concise Explanation: This part of the query creates a temporary table (products_purchased_cte) with details about how many times each product was purchased. It calculates counts for purchased events, filtering for add-to-cart events associated with a purchase. The results are grouped by product information.\n\n\n\n15.png\n\n\nExplanation:\nThis SQL query combines the results from the previous temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) into a final temporary table (product_information). It then selects and displays the contents of the final temporary table.\n1) Creating a Temporary Table:\nCREATE TEMPORARY TABLE product_information  AS\nCreates a temporary table named product_information to store the results.\n2) Selecting and Joining Tables:\nSELECT \nVATC.*,\nAB.abandoned, \nPP.purchased\nFROM \n    view_add_to_cart_cte AS VATC\n    JOIN products_abandoned_cte AS AB ON VATC.product_id = AB.product_id\n    JOIN products_purchased_cte AS PP ON VATC.product_id = PP.product_id;\n\nSelects columns from the view_add_to_cart_cte and joins it with the products_abandoned_cte and products_purchased_cte tables based on the common column product_id.\nCombines information about the number of views, cart additions, abandoned events, and purchased events for each product.\n\n3) Selecting from the Temporary Table:\nSELECT * FROM product_information\nORDER BY product_id;\n\nSelects and displays all columns from the final temporary table (product_information).\nOrders the results by product_id for better readability.\n\n4) Grouping by Product Information:\nGROUP BY \nPH.product_id, PH.page_name, PH.product_category;\nGroups the results by product_id, page_name, and product_category to obtain aggregated counts of abandoned events for each product.\n5) Dropping Temporary Tables:\nDROP TEMPORARY TABLE IF EXISTS view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte;\nThis SQL query drops the temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) when they are no longer needed.\nFinal Concise Explanation: This query combines the results from the previous temporary tables (view_add_to_cart_cte, products_abandoned_cte, products_purchased_cte) into a final temporary table named product_information. It includes information about the number of views, cart additions, abandoned events, and purchased events for each product. The final results are then selected and displayed from the temporary table, ordered by product_id.\n\n\n\n16.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_view_add_to_cart_cte) to aggregate data for each product category instead of individual products.\n1) Aggregating Data by Product Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_view_add_to_cart_cte AS\nSELECT \n    PH.product_category,\n    SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts,\n    SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type\n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    PH.product_category IS NOT NULL\nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_view_add_to_cart_cte: Initiates the creation of a temporary table named category_view_add_to_cart_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- SUM(CASE WHEN EI.event_name = 'Page View' THEN 1 ELSE 0 END) AS view_counts: Calculates the total number of page views (view_counts) for each product category.\n- SUM(CASE WHEN EI.event_name = 'Add to Cart' THEN 1 ELSE 0 END) AS add_to_cart_counts: Calculates the total number of \"Add to Cart\" events (add_to_cart_counts) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Product Category:\n- WHERE PH.product_category IS NOT NULL: Filters out rows where the product category is null, focusing on valid product categories.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_view_add_to_cart_cte) that summarizes data for each product category. It calculates the total number of page views and “Add to Cart” events for each product category, providing a higher-level analysis of user interactions within different product categories.\n\n\n\n17.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_products_abandoned_cte) that counts the number of abandoned products for each product category.\n1) Creating Table for Abandoned Products by Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_products_abandoned_cte AS\nSELECT \n    PH.product_category, \n    COUNT(*) AS abandoned\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id NOT IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase'\n    ) \nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_products_abandoned_cte: Initiates the creation of a temporary table named category_products_abandoned_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS abandoned: Counts the number of rows (abandoned products) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Event Type and Excluding Purchased Visits:\n- WHERE EI.event_name = 'Add to Cart': Filters rows where the event is an \"Add to Cart\" event.\n- AND E.visit_id NOT IN (...): Excludes visits where a purchase event has occurred, preventing counting products that were eventually purchased.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_products_abandoned_cte) that counts the number of abandoned products for each product category. It focuses on “Add to Cart” events and excludes visits where a purchase event has occurred, providing insights into the abandonment behavior within different product categories.\n\n\n\n18.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_products_purchased_cte) that counts the number of purchased products for each product category.\n1) Creating Table for Purchased Products by Category:\n-- Use temporary table instead of INTO for CTEs\nCREATE TEMPORARY TABLE category_products_purchased_cte AS\nSELECT \n    PH.product_category, \n    COUNT(*) AS purchased\nFROM \n    events AS E \n    JOIN event_identifier AS EI ON E.event_type = EI.event_type \n    JOIN page_hierarchy AS PH ON E.page_id = PH.page_id \nWHERE \n    EI.event_name = 'Add to Cart'\n    AND E.visit_id IN (\n        SELECT E.visit_id \n        FROM events AS E\n        JOIN event_identifier AS EI ON E.event_type = EI.event_type \n        WHERE EI.event_name = 'Purchase'\n    ) \nGROUP BY \n    PH.product_category;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_products_purchased_cte: Initiates the creation of a temporary table named category_products_purchased_cte.\n2) Data Selection and Aggregation:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS purchased: Counts the number of rows (purchased products) for each product category.\n3) Table Joins:\n- FROM events AS E: Specifies the events table with the alias E.\n- JOIN event_identifier AS EI ON E.event_type = EI.event_type: Joins the event_identifier table using the event_type.\n- JOIN page_hierarchy AS PH ON E.page_id = PH.page_id: Joins the page_hierarchy table using the page_id.\n4) Filtering by Event Type and Including Purchased Visits:\n- WHERE EI.event_name = 'Add to Cart': Filters rows where the event is an \"Add to Cart\" event.\n- AND E.visit_id IN (...): Includes visits where a purchase event has occurred, ensuring that only products added to the cart and later purchased are counted.\n5) Grouping by Product Category:\n- GROUP BY PH.product_category: Groups the results by product category, ensuring that each row represents a unique product category.\nFinal Concise Explanation: This query creates a temporary table (category_products_purchased_cte) that counts the number of purchased products for each product category. It focuses on “Add to Cart” events and includes visits where a purchase event has occurred, providing insights into the products that were added to the cart and eventually purchased within different product categories.\n\n\n\n19.png\n\n\nExplanation:\nThis SQL query creates a temporary table (category_product_information) that aggregates information about product categories, including the number of views, additions to cart, abandoned products, and purchased products.\n1) Creating Table for Aggregated Category Product Information:\n-- Use temporary table instead of INTO for the final result\nCREATE TEMPORARY TABLE category_product_information AS\nSELECT \n    VATC.*,\n    AB.abandoned, \n    PP.purchased\nFROM \n    category_view_add_to_cart_cte AS VATC\n    JOIN category_products_abandoned_cte AS AB ON VATC.product_category = AB.product_category\n    JOIN category_products_purchased_cte AS PP ON VATC.product_category = PP.product_category;\n\n-- Select from the temporary table\nSELECT * FROM category_product_information\nORDER BY product_category;\n\n-- Drop the temporary tables when done\nDROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart_cte, category_products_abandoned_cte, \ncategory_products_purchased_cte;\nStep-by-Step Explanation:\n1) Temporary Table Creation:\n- CREATE TEMPORARY TABLE category_product_information: Initiates the creation of a temporary table named category_product_information.\n2) Data Selection and Joining Tables:\n- SELECT PH.product_category: Selects the product_category column from the page_hierarchy table.\n- COUNT(*) AS purchased: Counts the number of rows (purchased products) for each product category.\n3) Table Joins:\n- FROM category_view_add_to_cart_cte AS VATC: Specifies the temporary table category_view_add_to_cart_cte with the alias VATC.\n- JOIN category_products_abandoned_cte AS AB ON VATC.product_category = AB.product_category: Joins the temporary table with the abandoned products table based on the product_category.\n- JOIN category_products_purchased_cte AS PP ON VATC.product_category = PP.product_category: Joins the temporary table with the purchased products table based on the product_category.\n4) Final Result:\n- The resulting temporary table (category_product_information) contains aggregated information for each product category, including the number of views, additions to cart, abandoned products, and purchased products.\n5) Selection from Temporary Table:\n- SELECT * FROM category_product_information ORDER BY product_category;: Retrieves all columns from the temporary table, ordering the results by product_category.\n6) Temporary Table Cleanup:\n- DROP TEMPORARY TABLE IF EXISTS category_view_add_to_cart_cte, category_products_abandoned_cte, category_products_purchased_cte;: Drops the temporary tables used in the process.\nFinal Concise Explanation: This query creates a temporary table (category_product_information) that aggregates information about product categories, including the number of views, additions to cart, abandoned products, and purchased products. The temporary table is then selected, providing a summary of product category data, and the temporary tables are dropped when the process is completed.\n\n\n\n20.png\n\n\nUse your 2 new output tables - answer the following questions:\n\n1. Which product had the most views, cart adds and purchases?\n\nPart 1\n\n\n\n21.png\n\n\nPart 2\n\n\n\n22.png\n\n\nPart 3\n\n\n\n23.png\n\n\n\n2. Which product was most likely to be abandoned?\n\n\n\n\n24.png\n\n\n\n3. Which product had the highest view to purchase percentage?\n\n\n\n\n25.png\n\n\n\n4. What is the average conversion rate from view to cart add?\n\n\n\n\n26.png\n\n\n\n5. What is the average conversion rate from cart add to purchase?\n\n\n\n\n27.png\n\n\n\n\n\nGenerate a table that has 1 single row for every unique visit_id record and has the following columns:\n\nuser_id\nvisit_id\nvisit_start_time: the earliest event_time for each visit\npage_views: count of page views for each visit\ncart_adds: count of product cart add events for each visit\npurchase: 1/0 flag if a purchase event exists for each visit\ncampaign_name: map the visit to a campaign if the visit_start_time falls between the start_date and end_date\nimpression: count of ad impressions for each visit\nclick: count of ad clicks for each visit\n(Optional column) cart_products: a comma separated text value with products added to the cart sorted by the order they were added to the cart (hint: use the sequence_number)\n\nUse the subsequent dataset to generate at least 5 insights for the Clique Bait team - bonus: prepare a single A4 infographic that the team can use for their management reporting sessions, be sure to emphasise the most important points from your findings.\nSome ideas you might want to investigate further include:\n\nIdentifying users who have received impressions during each campaign period and comparing each metric with other users who did not have an impression event\nDoes clicking on an impression lead to higher purchase rates?\nWhat is the uplift in purchase rate when comparing users who click on a campaign impression versus users who do not receive an impression? What if we compare them with users who just an impression but do not click?\nWhat metrics can you use to quantify the success or failure of each campaign compared to each other?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "Case_study_1.png\n\n\n\n\n\nDanny seriously loves Japanese food so in the beginning of 2021, he decides to embark upon a risky venture and opens up a cute little restaurant that sells his 3 favourite foods: sushi, curry and ramen.\nDanny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\n\n\n\n\nDanny wants to use the data to answer a few simple questions about his customers, especially about their visiting patterns, how much money they’ve spent and also which menu items are their favourite. Having this deeper connection with his customers will help him deliver a better and more personalised experience for his loyal customers.\nHe plans on using these insights to help him decide whether he should expand the existing customer loyalty program - additionally he needs help to generate some basic datasets so his team can easily inspect the data without needing to use SQL.\nDanny has provided you with a sample of his overall customer data due to privacy issues - but he hopes that these examples are enough for you to write fully functioning SQL queries to help him answer his questions!\nDanny has shared with you 3 key datasets for this case study:\n\nsales\nmenu\nmembers\n\nYou can inspect the entity relationship diagram and example data below.\n\n\n\n\n\n\n\nCase_study_2-2.png\n\n\n\n\n\n\nAll datasets exist within the dannys_diner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\n\ncustomer_id\norder_date\nproduct_id\n\n\n\n\nA\n2021-01-01\n1\n\n\nA\n2021-01-01\n2\n\n\nA\n2021-01-07\n2\n\n\nA\n2021-01-10\n3\n\n\nA\n2021-01-11\n3\n\n\nA\n2021-01-11\n3\n\n\nB\n2021-01-01\n2\n\n\nB\n2021-01-02\n2\n\n\nB\n2021-01-04\n1\n\n\nB\n2021-01-11\n1\n\n\nB\n2021-01-16\n3\n\n\nB\n2021-02-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-07\n3\n\n\n\n\n\n\nThe menu table maps the product_id to the actual product_name and price of each menu item.\n\n\n\nproduct_id\nproduct_name\nprice\n\n\n\n\n1\nsushi\n10\n\n\n2\ncurry\n15\n\n\n3\nramen\n12\n\n\n\n\n\n\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\n\ncustomer_id\njoin_date\n\n\n\n\nA\n2021-01-07\n\n\nB\n2021-01-09\n\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_3.png\n\n\n\n\n\nEach of the following case study questions can be answered using a single SQL statement:\n\n1. What is the total amount each customer spent at the restaurant?\n\nExplanation: This SQL query selects each customer’s ID and the total amount they spent at the restaurant by combining sales and menu data, grouping the results by customer, and ordering them by the total amount spent in ascending order.\n\n\n\nCase_study_4.png\n\n\n\n2. How many days has each customer visited the restaurant?\n\nExplanation: This SQL query counts the number of distinct days each customer has visited the restaurant by selecting customer IDs, counting unique order dates, grouping the results by customer, and ordering them by customer ID in ascending order.\n\n\n\nCase_study_5.png\n\n\n\n3. What was the first item from the menu purchased by each customer?\n\nExplanation:\nThis SQL query identifies the first item purchased by each customer from the menu. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘item_purchased_by_customer’:\nWITH item_purchased_by_customer AS (\nSELECT \n    SALES.customer_id as customer_id,\n    SALES.order_date as order_date,\n    MENU.product_name as product_name,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY SALES.order_date) as `Dense_Rank`\nFROM SALES\nLEFT JOIN MENU ON SALES.product_id = MENU.product_id)\nCreates a temporary table ‘item_purchased_by_customer’ with customer_id, order_date, product_name, and a Dense_Rank based on the order_date for each customer.\n2) Selecting Customer ID and Product Name for the First Purchase:\nSELECT customer_id, product_name\nFROM item_purchased_by_customer\nWHERE `Dense_Rank` = 1\nGROUP BY customer_id, product_name;\nSelects customer_id and product_name from the temporary table where the Dense_Rank is 1, indicating the first item purchased by each customer. The results are then grouped by customer_id and product_name for uniqueness.\nFinal Concise Explanation: This SQL query identifies the first item purchased by each customer by creating a temporary table with customer details, calculating a Dense_Rank based on the order date, and then selecting the records where Dense_Rank is 1. The final result is grouped by customer_id and product_name for uniqueness.\nThe following is the SQL query for the question:\n\n\n\nCase_study_6.png\n\n\nThe following is the output of the above SQL query:\n\n\n\nCase_study_7.png\n\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\nExplanation: This SQL query identifies the most purchased item on the menu by counting the occurrences of each product in the SALES table, grouping the results by product_name, and selecting the top result based on the highest purchase count.\n\n\n\nCase_study_8.png\n\n\n\n5. Which item was the most popular for each customer?\n\nExplanation:\nThis SQL query aims to determine the most popular item for each customer. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘most_popular’:\nWITH most_popular AS (\nSELECT SALES.customer_id,\n    MENU.product_name,\n    COUNT(MENU.product_name) AS order_cnt,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY COUNT(SALES.customer_id) DESC) as `Dense_Rank`\nFROM SALES\nJOIN MENU ON MENU.product_id = SALES.product_id\nGROUP BY SALES.customer_id, MENU.product_name)\nCreates a temporary table ‘most_popular’ with customer_id, product_name, the count of product_name occurrences (order_cnt), and a Dense_Rank based on the order count for each customer.\n2) Selecting Customer ID, Product Name, and Order Count:\nSELECT customer_id, product_name, order_cnt\nFROM most_popular\nWHERE `Dense_Rank` = 1;\nSelects customer_id, product_name, and order_cnt from the temporary table ‘most_popular’ where the Dense_Rank is 1, indicating the most popular item for each customer.\nFinal Concise Explanation: This SQL query identifies the most popular item for each customer by creating a temporary table with customer details, counting the occurrences of each product, calculating a Dense_Rank based on the order count, and then selecting the records where Dense_Rank is 1.\n\n\n\nCase_study_9.png\n\n\n\n6. Which item was purchased first by the customer after they became a member?\n\nExplanation:\nThis SQL query aims to find the first item purchased by each customer after they became a member. Here’s a step-by-step breakdown::\n1) Creating a Temporary Table ‘first_item_purchased’::\nWITH first_item_purchased AS (\nSELECT \n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date ASC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &gt; MEMBERS.join_date)\nCreates a temporary table ‘first_item_purchased’ with customer_id, product_id, and a Dense_Rank based on the order date for each customer, considering only sales that occurred after the customer joined as a member.\n2) Selecting Customer ID and Product Name:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects customer_id and product_name from the temporary table ‘first_item_purchased’ where the Dense_Rank is 1, indicating the first item purchased after becoming a member. The results are then joined with the MENU table to get the product names and ordered by customer_id.\nFinal Concise Explanation: This SQL query identifies the first item purchased by each customer after they became a member by creating a temporary table with customer details, calculating a Dense_Rank based on the order date, and selecting the records where Dense_Rank is 1. The results include customer_id and corresponding product_name, ordered by customer_id.\n\n\n\nCase_study_10.png\n\n\n\n7. Which item was purchased just before the customer became a member?\n\nExplanation:\nThis SQL query aims to identify the item purchased just before each customer became a member. Here’s a step-by-step breakdown::\n1) Creating a Temporary Table ‘first_item_purchased’:\nWITH first_item_purchased AS (\nSELECT \n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date DESC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date)\nCreates a temporary table ‘first_item_purchased’ with customer_id, product_id, and a Dense_Rank based on the order date for each customer just before they became a member.\n2) Selecting Customer ID and Product Name:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects customer_id and product_name from the temporary table ‘first_item_purchased’ where the Dense_Rank is 1, indicating the item purchased just before becoming a member. It then joins with the MENU table to retrieve the product names.\nFinal Concise Explanation: This SQL query identifies the item purchased just before each customer became a member by creating a temporary table with customer details, calculating a Dense_Rank based on the order date (in descending order), and then selecting the records where Dense_Rank is 1. It then joins with the MENU table to retrieve the product names.\n\n\n\nCase_study_11.png\n\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\nExplanation: This SQL query selects the customer_id, counts total_items_purchased, and calculates the amount_spent by joining SALES, MEMBERS, and MENU tables, filtering based on the join_date, grouping by customer_id, and ordering the results by customer_id.\n\n\n\nCase_study_12.png\n\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\nExplanation:\nThis SQL query calculates the total points each customer would have based on their purchases, considering a $1 spent as 10 points and applying a 2x points multiplier for sushi. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘cte’ with Points Calculation:\nWITH cte AS (\nSELECT \n    MENU.product_id,\n    (CASE \n        WHEN MENU.product_name = 'sushi' THEN 20 * MENU.price\n        ELSE 10 * MENU.price \n     END) AS Points\nFROM MENU)\nCreates a temporary table ‘cte’ with product_id and calculated Points based on the price, applying a 2x points multiplier for sushi.\n2) Selecting Customer ID and Calculating Total Points:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects the customer_id and calculates the Total_Points by summing the points from the temporary table ‘cte’ for each customer. The results are then grouped by customer_id and ordered by customer_id.\nFinal Concise Explanation: This SQL query calculates the total points each customer would have based on their purchases, considering a $1 spent as 10 points and applying a 2x points multiplier for sushi. It uses a temporary table for point calculations and then joins with the SALES table to sum the points for each customer. The results are grouped by customer_id and ordered by customer_id.\nThe following is the SQL query for the question:\n\n\n\nCase_study_13.png\n\n\nThe following is the output of the above SQL query:\n\n\n\nCase_study_14.png\n\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, not just sushi - how many points do customer A and B have at the end of January?\n\nExplanation:\nThis SQL query addresses the question of determining the total points earned by customers A and B at the end of January, with a special consideration for the first week after a customer joins the program. The points calculation involves different multipliers based on conditions related to the purchase date and product type. Here’s a step-by-step breakdown:\n1) Creating a Common Table Expression (CTE) with Relevant Dates:\nWITH dates_cte AS (\nSELECT \n    customer_id, \n    join_date,\n    DATE_ADD(join_date, INTERVAL 6 DAY) AS valid_date,\n    LAST_DAY('2021-01-01') AS month_end_date\nFROM members)\nCreates a CTE named dates_cte with customer_id, join_date, valid_date (one week after join_date), and the last day of January 2021 as month_end_date.\n2) Selecting Customer ID and Calculating Total Points:\nSELECT \nDC.customer_id,\nSUM(\n    CASE \n        WHEN S.order_date BETWEEN DC.join_date AND DC.valid_date THEN M.price * 20\n        WHEN M.product_name = 'sushi' THEN M.price * 20\n        ELSE M.price * 10 \n    END\n) AS total_points\nFROM dates_cte AS DC\nJOIN sales AS S ON DC.customer_id = S.customer_id\nJOIN menu AS M ON M.product_id = S.product_id\nWHERE S.order_date &lt;= DC.month_end_date\nGROUP BY DC.customer_id;\nSelects the customer_id and calculates the Total_Points by summing the points from the temporary table ‘cte’ for each customer. The results are then grouped by customer_id and ordered by customer_id.\nFinal Concise Explanation: This SQL query calculates the total points for customers A and B at the end of January, considering a special points multiplier for the first week after joining the program. Points are calculated based on purchase conditions such as order_date range and product type. The results provide the total_points for each customer, grouped by customer_id.\n\n\n\nCase_study_1_last.png\n\n\n\n\n\n\n\nThe following questions are related creating basic data tables that Danny and his team can use to quickly derive insights without needing to join the underlying tables using SQL.\nRecreate the following table output using the available data:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nmember\n\n\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\nThe following is the SQL query for the question:\n\n\n\nCase_study_15.png\n\n\nThe following is the output of the above executed SQL query:\nExplanation: This SQL query recreates a table with columns customer_id, order_date, product_name, price, and member (Y/N). It combines information from the SALES, MEMBERS, and MENU tables, calculating the membership status based on the order_date and join_date. The results are ordered by customer_id and order_date for clarity.\n\n\n\nCase_study_16.png\n\n\n\n\n\nDanny also requires further information about the ranking of customer products, but he purposely does not need the ranking for non-member purchases so he expects null ranking values for the records when customers are not yet part of the loyalty program.\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nmember\nranking\n\n\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nnull\n\n\nC\n2021-01-01\nramen\n12\nN\nnull\n\n\nC\n2021-01-07\nramen\n12\nN\nnull\n\n\n\nThe following is the SQL query to generate the above output:\n\n\n\nCase_study_17-2.png\n\n\nConfirming the output from the above query, if it matches with the expected output.\nExplanation:\nThis SQL query ranks customer products, providing null ranking values for records when customers are not yet part of the loyalty program. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘all_data’ with Additional Information:\nWITH all_data AS (\nSELECT \n    SALES.customer_id, SALES.order_date, MENU.product_name, MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES \nLEFT JOIN MENU ON SALES.product_id = MENU.product_id\nJOIN MEMBERS ON SALES.customer_id = MEMBERS.customer_id)\nCreates a temporary table ‘all_data’ with customer_id, order_date, product_name, price, and the membership status (‘Y’ for joined members, ‘N’ for non-members) using a CASE statement based on the order_date and join_date.\n2) Ranking Products for Members:\nSELECT all_data.*,\nCASE WHEN all_data.Member = 'Y' \n     THEN RANK() OVER (PARTITION BY all_data.customer_id, all_data.member ORDER BY all_data.order_date) \n     ELSE NULL \nEND AS 'ranking' \nFROM all_data;\nSelects all columns from the temporary table ‘all_data’ and calculates the ranking using the RANK() function over the order_date for each customer. If the customer is not a member (‘Y’), it assigns null to the ranking.\nFinal Concise Explanation: This SQL query ranks customer products, providing null ranking values for records when customers are not yet part of the loyalty program. It uses a temporary table to combine information from the SALES, MENU, and MEMBERS tables and applies the RANK() function over the order_date for members, assigning null to the ranking for non-members.\n\n\n\nCase_study_18-2.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#introduction",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#introduction",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "Danny seriously loves Japanese food so in the beginning of 2021, he decides to embark upon a risky venture and opens up a cute little restaurant that sells his 3 favourite foods: sushi, curry and ramen.\nDanny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#problem-statement",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#problem-statement",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "Danny wants to use the data to answer a few simple questions about his customers, especially about their visiting patterns, how much money they’ve spent and also which menu items are their favourite. Having this deeper connection with his customers will help him deliver a better and more personalised experience for his loyal customers.\nHe plans on using these insights to help him decide whether he should expand the existing customer loyalty program - additionally he needs help to generate some basic datasets so his team can easily inspect the data without needing to use SQL.\nDanny has provided you with a sample of his overall customer data due to privacy issues - but he hopes that these examples are enough for you to write fully functioning SQL queries to help him answer his questions!\nDanny has shared with you 3 key datasets for this case study:\n\nsales\nmenu\nmembers\n\nYou can inspect the entity relationship diagram and example data below."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#entity-relationship-diagram",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#entity-relationship-diagram",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "Case_study_2-2.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#example-datasets",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#example-datasets",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "All datasets exist within the dannys_diner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\n\ncustomer_id\norder_date\nproduct_id\n\n\n\n\nA\n2021-01-01\n1\n\n\nA\n2021-01-01\n2\n\n\nA\n2021-01-07\n2\n\n\nA\n2021-01-10\n3\n\n\nA\n2021-01-11\n3\n\n\nA\n2021-01-11\n3\n\n\nB\n2021-01-01\n2\n\n\nB\n2021-01-02\n2\n\n\nB\n2021-01-04\n1\n\n\nB\n2021-01-11\n1\n\n\nB\n2021-01-16\n3\n\n\nB\n2021-02-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-01\n3\n\n\nC\n2021-01-07\n3\n\n\n\n\n\n\nThe menu table maps the product_id to the actual product_name and price of each menu item.\n\n\n\nproduct_id\nproduct_name\nprice\n\n\n\n\n1\nsushi\n10\n\n\n2\ncurry\n15\n\n\n3\nramen\n12\n\n\n\n\n\n\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\n\ncustomer_id\njoin_date\n\n\n\n\nA\n2021-01-07\n\n\nB\n2021-01-09"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#interactive-sql-session",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#interactive-sql-session",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_3.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#case-study-questions",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "Each of the following case study questions can be answered using a single SQL statement:\n\n1. What is the total amount each customer spent at the restaurant?\n\nExplanation: This SQL query selects each customer’s ID and the total amount they spent at the restaurant by combining sales and menu data, grouping the results by customer, and ordering them by the total amount spent in ascending order.\n\n\n\nCase_study_4.png\n\n\n\n2. How many days has each customer visited the restaurant?\n\nExplanation: This SQL query counts the number of distinct days each customer has visited the restaurant by selecting customer IDs, counting unique order dates, grouping the results by customer, and ordering them by customer ID in ascending order.\n\n\n\nCase_study_5.png\n\n\n\n3. What was the first item from the menu purchased by each customer?\n\nExplanation:\nThis SQL query identifies the first item purchased by each customer from the menu. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘item_purchased_by_customer’:\nWITH item_purchased_by_customer AS (\nSELECT \n    SALES.customer_id as customer_id,\n    SALES.order_date as order_date,\n    MENU.product_name as product_name,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY SALES.order_date) as `Dense_Rank`\nFROM SALES\nLEFT JOIN MENU ON SALES.product_id = MENU.product_id)\nCreates a temporary table ‘item_purchased_by_customer’ with customer_id, order_date, product_name, and a Dense_Rank based on the order_date for each customer.\n2) Selecting Customer ID and Product Name for the First Purchase:\nSELECT customer_id, product_name\nFROM item_purchased_by_customer\nWHERE `Dense_Rank` = 1\nGROUP BY customer_id, product_name;\nSelects customer_id and product_name from the temporary table where the Dense_Rank is 1, indicating the first item purchased by each customer. The results are then grouped by customer_id and product_name for uniqueness.\nFinal Concise Explanation: This SQL query identifies the first item purchased by each customer by creating a temporary table with customer details, calculating a Dense_Rank based on the order date, and then selecting the records where Dense_Rank is 1. The final result is grouped by customer_id and product_name for uniqueness.\nThe following is the SQL query for the question:\n\n\n\nCase_study_6.png\n\n\nThe following is the output of the above SQL query:\n\n\n\nCase_study_7.png\n\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\nExplanation: This SQL query identifies the most purchased item on the menu by counting the occurrences of each product in the SALES table, grouping the results by product_name, and selecting the top result based on the highest purchase count.\n\n\n\nCase_study_8.png\n\n\n\n5. Which item was the most popular for each customer?\n\nExplanation:\nThis SQL query aims to determine the most popular item for each customer. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘most_popular’:\nWITH most_popular AS (\nSELECT SALES.customer_id,\n    MENU.product_name,\n    COUNT(MENU.product_name) AS order_cnt,\n    DENSE_RANK() OVER (PARTITION BY SALES.customer_id ORDER BY COUNT(SALES.customer_id) DESC) as `Dense_Rank`\nFROM SALES\nJOIN MENU ON MENU.product_id = SALES.product_id\nGROUP BY SALES.customer_id, MENU.product_name)\nCreates a temporary table ‘most_popular’ with customer_id, product_name, the count of product_name occurrences (order_cnt), and a Dense_Rank based on the order count for each customer.\n2) Selecting Customer ID, Product Name, and Order Count:\nSELECT customer_id, product_name, order_cnt\nFROM most_popular\nWHERE `Dense_Rank` = 1;\nSelects customer_id, product_name, and order_cnt from the temporary table ‘most_popular’ where the Dense_Rank is 1, indicating the most popular item for each customer.\nFinal Concise Explanation: This SQL query identifies the most popular item for each customer by creating a temporary table with customer details, counting the occurrences of each product, calculating a Dense_Rank based on the order count, and then selecting the records where Dense_Rank is 1.\n\n\n\nCase_study_9.png\n\n\n\n6. Which item was purchased first by the customer after they became a member?\n\nExplanation:\nThis SQL query aims to find the first item purchased by each customer after they became a member. Here’s a step-by-step breakdown::\n1) Creating a Temporary Table ‘first_item_purchased’::\nWITH first_item_purchased AS (\nSELECT \n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date ASC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &gt; MEMBERS.join_date)\nCreates a temporary table ‘first_item_purchased’ with customer_id, product_id, and a Dense_Rank based on the order date for each customer, considering only sales that occurred after the customer joined as a member.\n2) Selecting Customer ID and Product Name:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects customer_id and product_name from the temporary table ‘first_item_purchased’ where the Dense_Rank is 1, indicating the first item purchased after becoming a member. The results are then joined with the MENU table to get the product names and ordered by customer_id.\nFinal Concise Explanation: This SQL query identifies the first item purchased by each customer after they became a member by creating a temporary table with customer details, calculating a Dense_Rank based on the order date, and selecting the records where Dense_Rank is 1. The results include customer_id and corresponding product_name, ordered by customer_id.\n\n\n\nCase_study_10.png\n\n\n\n7. Which item was purchased just before the customer became a member?\n\nExplanation:\nThis SQL query aims to identify the item purchased just before each customer became a member. Here’s a step-by-step breakdown::\n1) Creating a Temporary Table ‘first_item_purchased’:\nWITH first_item_purchased AS (\nSELECT \n    MEMBERS.customer_id AS customer_id,\n    SALES.product_id AS product_id,\n    DENSE_RANK() OVER (PARTITION BY MEMBERS.customer_id ORDER BY SALES.order_date DESC) AS DRank\nFROM MEMBERS\nJOIN SALES ON SALES.customer_id = MEMBERS.customer_id AND SALES.order_date &lt; MEMBERS.join_date)\nCreates a temporary table ‘first_item_purchased’ with customer_id, product_id, and a Dense_Rank based on the order date for each customer just before they became a member.\n2) Selecting Customer ID and Product Name:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects customer_id and product_name from the temporary table ‘first_item_purchased’ where the Dense_Rank is 1, indicating the item purchased just before becoming a member. It then joins with the MENU table to retrieve the product names.\nFinal Concise Explanation: This SQL query identifies the item purchased just before each customer became a member by creating a temporary table with customer details, calculating a Dense_Rank based on the order date (in descending order), and then selecting the records where Dense_Rank is 1. It then joins with the MENU table to retrieve the product names.\n\n\n\nCase_study_11.png\n\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\nExplanation: This SQL query selects the customer_id, counts total_items_purchased, and calculates the amount_spent by joining SALES, MEMBERS, and MENU tables, filtering based on the join_date, grouping by customer_id, and ordering the results by customer_id.\n\n\n\nCase_study_12.png\n\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\nExplanation:\nThis SQL query calculates the total points each customer would have based on their purchases, considering a $1 spent as 10 points and applying a 2x points multiplier for sushi. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘cte’ with Points Calculation:\nWITH cte AS (\nSELECT \n    MENU.product_id,\n    (CASE \n        WHEN MENU.product_name = 'sushi' THEN 20 * MENU.price\n        ELSE 10 * MENU.price \n     END) AS Points\nFROM MENU)\nCreates a temporary table ‘cte’ with product_id and calculated Points based on the price, applying a 2x points multiplier for sushi.\n2) Selecting Customer ID and Calculating Total Points:\nSELECT first_item_purchased.customer_id AS customer_id, MENU.product_name \nFROM first_item_purchased\nJOIN MENU ON first_item_purchased.product_id = MENU.product_id\nWHERE Drank = 1\nORDER BY customer_id ASC;\nSelects the customer_id and calculates the Total_Points by summing the points from the temporary table ‘cte’ for each customer. The results are then grouped by customer_id and ordered by customer_id.\nFinal Concise Explanation: This SQL query calculates the total points each customer would have based on their purchases, considering a $1 spent as 10 points and applying a 2x points multiplier for sushi. It uses a temporary table for point calculations and then joins with the SALES table to sum the points for each customer. The results are grouped by customer_id and ordered by customer_id.\nThe following is the SQL query for the question:\n\n\n\nCase_study_13.png\n\n\nThe following is the output of the above SQL query:\n\n\n\nCase_study_14.png\n\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, not just sushi - how many points do customer A and B have at the end of January?\n\nExplanation:\nThis SQL query addresses the question of determining the total points earned by customers A and B at the end of January, with a special consideration for the first week after a customer joins the program. The points calculation involves different multipliers based on conditions related to the purchase date and product type. Here’s a step-by-step breakdown:\n1) Creating a Common Table Expression (CTE) with Relevant Dates:\nWITH dates_cte AS (\nSELECT \n    customer_id, \n    join_date,\n    DATE_ADD(join_date, INTERVAL 6 DAY) AS valid_date,\n    LAST_DAY('2021-01-01') AS month_end_date\nFROM members)\nCreates a CTE named dates_cte with customer_id, join_date, valid_date (one week after join_date), and the last day of January 2021 as month_end_date.\n2) Selecting Customer ID and Calculating Total Points:\nSELECT \nDC.customer_id,\nSUM(\n    CASE \n        WHEN S.order_date BETWEEN DC.join_date AND DC.valid_date THEN M.price * 20\n        WHEN M.product_name = 'sushi' THEN M.price * 20\n        ELSE M.price * 10 \n    END\n) AS total_points\nFROM dates_cte AS DC\nJOIN sales AS S ON DC.customer_id = S.customer_id\nJOIN menu AS M ON M.product_id = S.product_id\nWHERE S.order_date &lt;= DC.month_end_date\nGROUP BY DC.customer_id;\nSelects the customer_id and calculates the Total_Points by summing the points from the temporary table ‘cte’ for each customer. The results are then grouped by customer_id and ordered by customer_id.\nFinal Concise Explanation: This SQL query calculates the total points for customers A and B at the end of January, considering a special points multiplier for the first week after joining the program. Points are calculated based on purchase conditions such as order_date range and product type. The results provide the total_points for each customer, grouped by customer_id.\n\n\n\nCase_study_1_last.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#bonus-questions",
    "href": "portfolio/SQL_Portfolio/Danny'_Diner/Case_Study_1-Danny's_Diner.html#bonus-questions",
    "title": "Case Study #1 - Danny’s Diner",
    "section": "",
    "text": "The following questions are related creating basic data tables that Danny and his team can use to quickly derive insights without needing to join the underlying tables using SQL.\nRecreate the following table output using the available data:\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nmember\n\n\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY\n\n\nB\n2021-01-16\nramen\n12\nY\n\n\nB\n2021-02-01\nramen\n12\nY\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-01\nramen\n12\nN\n\n\nC\n2021-01-07\nramen\n12\nN\n\n\n\nThe following is the SQL query for the question:\n\n\n\nCase_study_15.png\n\n\nThe following is the output of the above executed SQL query:\nExplanation: This SQL query recreates a table with columns customer_id, order_date, product_name, price, and member (Y/N). It combines information from the SALES, MEMBERS, and MENU tables, calculating the membership status based on the order_date and join_date. The results are ordered by customer_id and order_date for clarity.\n\n\n\nCase_study_16.png\n\n\n\n\n\nDanny also requires further information about the ranking of customer products, but he purposely does not need the ranking for non-member purchases so he expects null ranking values for the records when customers are not yet part of the loyalty program.\n\n\n\ncustomer_id\norder_date\nproduct_name\nprice\nmember\nranking\n\n\n\n\nA\n2021-01-01\ncurry\n15\nN\nnull\n\n\nA\n2021-01-01\nsushi\n10\nN\nnull\n\n\nA\n2021-01-07\ncurry\n15\nY\n1\n\n\nA\n2021-01-10\nramen\n12\nY\n2\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nA\n2021-01-11\nramen\n12\nY\n3\n\n\nB\n2021-01-01\ncurry\n15\nN\nnull\n\n\nB\n2021-01-02\ncurry\n15\nN\nnull\n\n\nB\n2021-01-04\nsushi\n10\nN\nnull\n\n\nB\n2021-01-11\nsushi\n10\nY\n1\n\n\nB\n2021-01-16\nramen\n12\nY\n2\n\n\nB\n2021-02-01\nramen\n12\nY\n3\n\n\nC\n2021-01-01\nramen\n12\nN\nnull\n\n\nC\n2021-01-01\nramen\n12\nN\nnull\n\n\nC\n2021-01-07\nramen\n12\nN\nnull\n\n\n\nThe following is the SQL query to generate the above output:\n\n\n\nCase_study_17-2.png\n\n\nConfirming the output from the above query, if it matches with the expected output.\nExplanation:\nThis SQL query ranks customer products, providing null ranking values for records when customers are not yet part of the loyalty program. Here’s a step-by-step breakdown:\n1) Creating a Temporary Table ‘all_data’ with Additional Information:\nWITH all_data AS (\nSELECT \n    SALES.customer_id, SALES.order_date, MENU.product_name, MENU.price,\n    CASE WHEN SALES.order_date &gt;= MEMBERS.join_date THEN 'Y' ELSE 'N' END AS `Member`\nFROM SALES \nLEFT JOIN MENU ON SALES.product_id = MENU.product_id\nJOIN MEMBERS ON SALES.customer_id = MEMBERS.customer_id)\nCreates a temporary table ‘all_data’ with customer_id, order_date, product_name, price, and the membership status (‘Y’ for joined members, ‘N’ for non-members) using a CASE statement based on the order_date and join_date.\n2) Ranking Products for Members:\nSELECT all_data.*,\nCASE WHEN all_data.Member = 'Y' \n     THEN RANK() OVER (PARTITION BY all_data.customer_id, all_data.member ORDER BY all_data.order_date) \n     ELSE NULL \nEND AS 'ranking' \nFROM all_data;\nSelects all columns from the temporary table ‘all_data’ and calculates the ranking using the RANK() function over the order_date for each customer. If the customer is not a member (‘Y’), it assigns null to the ranking.\nFinal Concise Explanation: This SQL query ranks customer products, providing null ranking values for records when customers are not yet part of the loyalty program. It uses a temporary table to combine information from the SALES, MENU, and MEMBERS tables and applies the RANK() function over the order_date for members, assigning null to the ranking for non-members.\n\n\n\nCase_study_18-2.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html",
    "title": "Case Study #4 - Data Bank",
    "section": "",
    "text": "Case_study_4_1.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#introduction",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#introduction",
    "title": "Case Study #4 - Data Bank",
    "section": "Introduction",
    "text": "Introduction\nThere is a new innovation in the financial industry called Neo-Banks: new aged digital only banks without physical branches.\nDanny thought that there should be some sort of intersection between these new age banks, cryptocurrency and the data world…so he decides to launch a new initiative - Data Bank!\nData Bank runs just like any other digital bank - but it isn’t only for banking activities, they also have the world’s most secure distributed data storage platform!\nCustomers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. There are a few interesting caveats that go with this business model, and this is where the Data Bank team need your help!\nThe management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need.\nThis case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#available-data",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#available-data",
    "title": "Case Study #4 - Data Bank",
    "section": "Available Data",
    "text": "Available Data\nThe Data Bank team have prepared a data model for this case study as well as a few example rows from the complete dataset below to get you familiar with their tables.\n\n\n\nCase_study_4_2.png\n\n\n\nTable 1: Regions\nJust like popular cryptocurrency platforms - Data Bank is also run off a network of nodes where both money and data is stored across the globe. In a traditional banking sense - you can think of these nodes as bank branches or stores that exist around the world.\nThis regions table contains the region_id and their respective region_name values\n\n\n\nregion_id\nregion_name\n\n\n\n\n1\nAfrica\n\n\n2\nAmerica\n\n\n3\nAsia\n\n\n4\nEurope\n\n\n5\nOceania\n\n\n\n\n\nTable 2: Customer Nodes\nCustomers are randomly distributed across the nodes according to their region - this also specifies exactly which node contains both their cash and data.\nThis random distribution changes frequently to reduce the risk of hackers getting into Data Bank’s system and stealing customer’s money and data!\nBelow is a sample of the top 10 rows of the data_bank.customer_nodes:\n\n\n\ncustomer_id\nregion_id\nnode_id\nstart_date\nend_date\n\n\n\n\n1\n3\n4\n2020-01-02\n2020-01-03\n\n\n2\n3\n5\n2020-01-03\n2020-01-17\n\n\n3\n5\n4\n2020-01-27\n2020-02-18\n\n\n4\n5\n4\n2020-01-07\n2020-01-19\n\n\n5\n3\n3\n2020-01-15\n2020-01-23\n\n\n6\n1\n1\n2020-01-11\n2020-02-06\n\n\n7\n2\n5\n2020-01-20\n2020-02-04\n\n\n8\n1\n2\n2020-01-15\n2020-01-28\n\n\n9\n4\n5\n2020-01-21\n2020-01-25\n\n\n10\n3\n4\n2020-01-13\n2020-01-14\n\n\n\n\n\nTable 3: Customer Transactions\nThis table stores all customer deposits, withdrawals and purchases made using their Data Bank debit card.\n\n\n\ncustomer_id\ntxn_date\ntxn_type\ntxn_amount\n\n\n\n\n429\n2020-01-21\ndeposit\n82\n\n\n155\n2020-01-10\ndeposit\n712\n\n\n398\n2020-01-01\ndeposit\n196\n\n\n255\n2020-01-14\ndeposit\n563\n\n\n185\n2020-01-29\ndeposit\n626\n\n\n309\n2020-01-13\ndeposit\n995\n\n\n312\n2020-01-20\ndeposit\n485\n\n\n376\n2020-01-03\ndeposit\n706\n\n\n188\n2020-01-13\ndeposit\n601\n\n\n138\n2020-01-11\ndeposit\n520"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#interactive-sql-instance",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#interactive-sql-instance",
    "title": "Case Study #4 - Data Bank",
    "section": "Interactive SQL Instance",
    "text": "Interactive SQL Instance\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_4_3.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#case-study-questions",
    "title": "Case Study #4 - Data Bank",
    "section": "Case Study Questions",
    "text": "Case Study Questions\nThe following case study questions include some general data exploration analysis for the nodes and transactions before diving right into the core business questions and finishes with a challenging final request!\n\nA. Customer Nodes Exploration\n\nHow many unique nodes are there on the Data Bank system?\nWhat is the number of nodes per region?\nHow many customers are allocated to each region?\nHow many days on average are customers reallocated to a different node?\nWhat is the median, 80th and 95th percentile for this same reallocation days metric for each region?\n\n\n\nB. Customer Transactions\n\nWhat is the unique count and total amount for each transaction type?\nWhat is the average total historical deposit counts and amounts for all customers?\nFor each month - how many Data Bank customers make more than 1 deposit and either 1 purchase or 1 withdrawal in a single month?\nWhat is the closing balance for each customer at the end of the month?\nWhat is the percentage of customers who increase their closing balance by more than 5%?\n\n\n\nC. Data Allocation Challenge\nTo test out a few different hypotheses - the Data Bank team wants to run an experiment where different groups of customers would be allocated data using 3 different options:\n\nOption 1: data is allocated based off the amount of money at the end of the previous month\nOption 2: data is allocated on the average amount of money kept in the account in the previous 30 days\nOption 3: data is updated real-time\n\nFor this multi-part challenge question - you have been requested to generate the following data elements to help the Data Bank team estimate how much data will need to be provisioned for each option:\n\nrunning customer balance column that includes the impact each transaction\ncustomer balance at the end of each month\nminimum, average and maximum values of the running balance for each customer\n\nUsing all of the data available - how much data would have been required for each option on a monthly basis?\n\n\nD. Extra Challenge\nData Bank wants to try another option which is a bit more difficult to implement - they want to calculate data growth using an interest calculation, just like in a traditional savings account you might have with a bank.\nIf the annual interest rate is set at 6% and the Data Bank team wants to reward its customers by increasing their data allocation based off the interest calculated on a daily basis at the end of each day, how much data would be required for this option on a monthly basis?\nSpecial notes:\n\nData Bank wants an initial calculation which does not allow for compounding interest, however they may also be interested in a daily compounding interest calculation so you can try to perform this calculation if you have the stamina!\n\n\n\nExtension Request\nThe Data Bank team wants you to use the outputs generated from the above sections to create a quick Powerpoint presentation which will be used as marketing materials for both external investors who might want to buy Data Bank shares and new prospective customers who might want to bank with Data Bank.\n\nUsing the outputs generated from the customer node questions, generate a few headline insights which Data Bank might use to market it’s world-leading security features to potential investors and customers.\nWith the transaction analysis - prepare a 1 page presentation slide which contains all the relevant information about the various options for the data provisioning so the Data Bank management team can make an informed decision."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#lets-start-solving",
    "href": "portfolio/SQL_Portfolio/Data_Bank/Case_Study_4-Data_Bank.html#lets-start-solving",
    "title": "Case Study #4 - Data Bank",
    "section": "Let’s start solving",
    "text": "Let’s start solving\n\nA. Customer Nodes Exploration\n1. How many unique nodes are there on the Data Bank system? \n2. What is the number of nodes per region? \n3. How many customers are allocated to each region? \n4. How many days on average are customers reallocated to a different node?\n5. What is the median, 80th and 95th percentile for this same reallocation days metric for each region?\n\n\nB. Customer Transactions\n1. What is the unique count and total amount for each transaction type? \n2. What is the average total historical deposit counts and amounts for all customers? \n3. For each month - how many Data Bank customers make more than 1 deposit and either 1 purchase or 1 withdrawal in a single month? \n4. What is the closing balance for each customer at the end of the month?\n5. What is the percentage of customers who increase their closing balance by more than 5%?\n\n\nC. Data Allocation Challenge\nTo test out a few different hypotheses - the Data Bank team wants to run an experiment where different groups of customers would be allocated data using 3 different options:\n\nOption 1: data is allocated based off the amount of money at the end of the previous month\nOption 2: data is allocated on the average amount of money kept in the account in the previous 30 days\nOption 3: data is updated real-time\n\nFor this multi-part challenge question - you have been requested to generate the following data elements to help the Data Bank team estimate how much data will need to be provisioned for each option:\n\nrunning customer balance column that includes the impact each transaction\ncustomer balance at the end of each month\nminimum, average and maximum values of the running balance for each customer\n\nUsing all of the data available - how much data would have been required for each option on a monthly basis?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html",
    "title": "Case Study #5 - Data Mart",
    "section": "",
    "text": "Case_study_5_1.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#introduction",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#introduction",
    "title": "Case Study #5 - Data Mart",
    "section": "Introduction",
    "text": "Introduction\nData Mart is Danny’s latest venture and after running international operations for his online supermarket that specialises in fresh produce - Danny is asking for your support to analyse his sales performance.\nIn June 2020 - large scale supply changes were made at Data Mart. All Data Mart products now use sustainable packaging methods in every single step from the farm all the way to the customer.\nDanny needs your help to quantify the impact of this change on the sales performance for Data Mart and it’s separate business areas.\nThe key business question he wants you to help him answer are the following:\n\nWhat was the quantifiable impact of the changes introduced in June 2020?\nWhich platform, region, segment and customer types were the most impacted by this change?\nWhat can we do about future introduction of similar sustainability updates to the business to minimise impact on sales?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#available-data",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#available-data",
    "title": "Case Study #5 - Data Mart",
    "section": "Available Data",
    "text": "Available Data\nFor this case study there is only a single table: data_mart.weekly_sales\nThe Entity Relationship Diagram is shown below with the data types made clear, please note that there is only this one table - hence why it looks a little bit lonely!\n\n\n\nCase_study_5_2.png\n\n\n\nColumn Dictionary\nThe columns are pretty self-explanatory based on the column names but here are some further details about the dataset:\n\nData Mart has international operations using a multi-region strategy\nData Mart has both, a retail and online platform in the form of a Shopify store front to serve their customers\nCustomer segment and customer_type data relates to personal age and demographics information that is shared with Data Mart.\ntransactions is the count of unique purchases made through Data Mart and sales is the actual dollar amount of purchases\n\nEach record in the dataset is related to a specific aggregated slice of the underlying sales data rolled up into a week_date value which represents the start of the sales week.\n\n\nExample Rows\n10 random rows are shown in the table output below from data_mart.weekly_sales:\n\n\n\n\n\n\n\n\n\n\n\n\nweek_date\nregion\nplatform\nsegment\ncustomer_type\ntransactions\nsales\n\n\n\n\n9/9/20\nOCEANIA\nShopify\nC3\nNew\n610\n110033.89\n\n\n29/7/20\nAFRICA\nRetail\nC1\nNew\n110692\n3053771.19\n\n\n22/7/20\nEUROPE\nShopify\nC4\nExisting\n24\n8101.54\n\n\n13/5/20\nAFRICA\nShopify\nnull\nGuest\n5287\n1003301.37\n\n\n24/7/19\nASIA\nRetail\nC1\nNew\n127342\n3151780.41\n\n\n10/7/19\nCANADA\nShopify\nF3\nNew\n51\n8844.93\n\n\n26/6/19\nOCEANIA\nRetail\nC3\nNew\n152921\n5551385.36\n\n\n29/5/19\nSOUTH AMERICA\nShopify\nnull\nNew\n53\n10056.2\n\n\n22/8/18\nAFRICA\nRetail\nnull\nExisting\n31721\n1718863.58\n\n\n25/7/18\nSOUTH AMERICA\nRetail\nnull\nNew\n2136\n81757.91"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#interactive-sql-session",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#interactive-sql-session",
    "title": "Case Study #5 - Data Mart",
    "section": "Interactive SQL Session",
    "text": "Interactive SQL Session\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_5_3.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#case-study-questions",
    "title": "Case Study #5 - Data Mart",
    "section": "Case Study Questions",
    "text": "Case Study Questions\nThe following case study questions require some data cleaning steps before we start to unpack Danny’s key business questions in more depth.\n\n1. Data Cleansing Steps\nIn a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record\n\n\n\n2. Data Exploration\n\nWhat day of the week is used for each week_date value?\nWhat range of week numbers are missing from the dataset?\nHow many total transactions were there for each year in the dataset?\nWhat is the total sales for each region for each month?\nWhat is the total count of transactions for each platform\nWhat is the percentage of sales for Retail vs Shopify for each month?\nWhat is the percentage of sales by demographic for each year in the dataset?\nWhich age_band and demographic values contribute the most to Retail sales?\nCan we use the avg_transaction column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?\n\n\n\n3. Before & After Analysis\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n\nWhat is the total sales for the 4 weeks before and after 2020-06-15? What is the growth or reduction rate in actual values and percentage of sales?\nWhat about the entire 12 weeks before and after?\nHow do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019?\n\n\n\n4. Bonus Question\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#lets-start-solving-them.",
    "href": "portfolio/SQL_Portfolio/Data_Mart/Case_Study_5-Data_Mart.html#lets-start-solving-them.",
    "title": "Case Study #5 - Data Mart",
    "section": "Let’s start solving them.",
    "text": "Let’s start solving them.\n\n1. Data Cleansing Steps\nIn a single query, perform the following operations and generate a new table in the data_mart schema named clean_weekly_sales:\n\nConvert the week_date to a DATE format\nAdd a week_number as the second column for each week_date value, for example any value from the 1st of January to 7th of January will be 1, 8th to 14th will be 2 etc.\nAdd a month_number with the calendar month for each week_date value as the 3rd column.\nAdd a calendar_year column as the 4th column containing either 2018, 2019 or 2020 values.\nAdd a new column called age_band after the original segment column using the following mapping on the number inside the segment value.\n\n\n\n\nsegment\nage_band\n\n\n\n\n1\nYoung Adults\n\n\n2\nMiddle Aged\n\n\n3 or 4\nRetirees\n\n\n\n\nAdd a new demographic column using the following mapping for the first letter in the segment values:\n\n\n\n\nsegment\ndemographic\n\n\n\n\nC\nCouples\n\n\nF\nFamilies\n\n\n\n\nEnsure all null string values with an \"unknown\" string value in the original segment column as well as the new age_band and demographic columns\nGenerate a new avg_transaction column as the sales value divided by transactions rounded to 2 decimal places for each record\n\nPart 1\nBelow is the SQL query to perform all the data cleaning steps asked in the above question:\n\n\n\nCase_study_5_4.png\n\n\nBelow is the outut of the above query looks like:\n\n\n\nCase_study_5_5.png\n\n\n\n\n2. Data Exploration\n1. What day of the week is used for each week_date value?\n\n\n\nCase_study_5_6.png\n\n\n2. What range of week numbers are missing from the dataset?\n\n\n\nCase_study_5_7.png\n\n\n3. How many total transactions were there for each year in the dataset?*\n\n\n\nCase_study_5_8.png\n\n\n4. What is the total sales for each region for each month?\n\n\n\nCase_study_5_9.png\n\n\n5. What is the total count of transactions for each platform.\n\n\n\ncase_study_5_10.png\n\n\n6. What is the percentage of sales for Retail vs Shopify for each month?\n\n\n\ncase_study_5_11.png\n\n\nBelow is the result of the above query:\n\n\n\ncase_study_5_12.png\n\n\n7. What is the percentage of sales by demographic for each year in the dataset?\n\n\n\ncase_study_5_13.png\n\n\n8. Which age_band and demographic values contribute the most to Retail sales?\n\n\n\ncase_study_5_14.png\n\n\n9. Can we use the avg_transaction column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?\n\n\n\nCase_study_5_15.png\n\n\n\n\n3. Before & After Analysis\nThis technique is usually used when we inspect an important event and want to inspect the impact before and after a certain point in time.\nTaking the week_date value of 2020-06-15 as the baseline week where the Data Mart sustainable packaging changes came into effect.\nWe would include all week_date values for 2020-06-15 as the start of the period after the change and the previous week_date values would be before\nUsing this analysis approach - answer the following questions:\n1. What is the total sales for the 4 weeks before and after 2020-06-15? What is the growth or reduction rate in actual values and percentage of sales?\n\n\n\nCase_study_5_16.png\n\n\n2. What about the entire 12 weeks before and after?\n\n\n\nCase_study_5_17.png\n\n\nBelow is the output of the above query:\n\n\n\nCase_study_5_18.png\n\n\n3. How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019?\n\n\n4. Bonus Question\nWhich areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?\n\nregion\nplatform\nage_band\ndemographic\ncustomer_type\n\nDo you have any further recommendations for Danny’s team at Data Mart or any interesting insights based off this analysis?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "",
    "text": "Case_study_3_1.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#introduction",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#introduction",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Introduction",
    "text": "Introduction\nSubscription based businesses are super popular and Danny realised that there was a large gap in the market - he wanted to create a new streaming service that only had food related content - something like Netflix but with only cooking shows!\nDanny finds a few smart friends to launch his new startup Foodie-Fi in 2020 and started selling monthly and annual subscriptions, giving their customers unlimited on-demand access to exclusive food videos from around the world!\nDanny created Foodie-Fi with a data driven mindset and wanted to ensure all future investment decisions and new features were decided using data. This case study focuses on using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#available-data",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#available-data",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Available Data",
    "text": "Available Data\nDanny has shared the data design for Foodie-Fi and also short descriptions on each of the database tables - our case study focuses on only 2 tables but there will be a challenge to create a new table for the Foodie-Fi team.\nAll datasets exist within the foodie_fi database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#entity-relationship-diagram",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#entity-relationship-diagram",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\n\nCase_study_3_2.png\n\n\n\nTable 1: plans\nCustomers can choose which plans to join Foodie-Fi when they first sign up.\nBasic plan customers have limited access and can only stream their videos and is only available monthly at $9.90\nPro plan customers have no watch time limits and are able to download videos for offline viewing. Pro plans start at \\$19.90 a month or \\$199 for an annual subscription.\nCustomers can sign up to an initial 7 day free trial will automatically continue with the pro monthly subscription plan unless they cancel, downgrade to basic or upgrade to an annual pro plan at any point during the trial.\nWhen customers cancel their Foodie-Fi service - they will have a churn plan record with a null price but their plan will continue until the end of the billing period.\n\n\n\nplan_id\nplan_name\nprice\n\n\n\n\n0\ntrial\n0\n\n\n1\nbasic monthly\n9.90\n\n\n2\npro monthly\n19.90\n\n\n3\npro annual\n199\n\n\n4\nchurn\nnull\n\n\n\n\n\nTable 2: subscriptions\nCustomer subscriptions show the exact date where their specific plan_id starts.\nIf customers downgrade from a pro plan or cancel their subscription - the higher plan will remain in place until the period is over - the start_date in the subscriptions table will reflect the date that the actual plan changes.\nWhen customers upgrade their account from a basic plan to a pro or annual pro plan - the higher plan will take effect straightaway.\nWhen customers churn - they will keep their access until the end of their current billing period but the start_date will be technically the day they decided to cancel their service.\n\n\n\ncustomer_id\nplan_id\nstart_date\n\n\n\n\n1\n0\n2020-08-01\n\n\n1\n1\n2020-08-08\n\n\n2\n0\n2020-09-20\n\n\n2\n3\n2020-09-27\n\n\n11\n0\n2020-11-19\n\n\n11\n4\n2020-11-26\n\n\n13\n0\n2020-12-15\n\n\n13\n1\n2020-12-22\n\n\n13\n2\n2021-03-29\n\n\n15\n0\n2020-03-17\n\n\n15\n2\n2020-03-24\n\n\n15\n4\n2020-04-29\n\n\n16\n0\n2020-05-31\n\n\n16\n1\n2020-06-07\n\n\n16\n3\n2020-10-21\n\n\n18\n0\n2020-07-06\n\n\n18\n2\n2020-07-13\n\n\n19\n0\n2020-06-22\n\n\n19\n2\n2020-06-29\n\n\n19\n3\n2020-08-29"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#interactive-sql-session",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#interactive-sql-session",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Interactive SQL Session",
    "text": "Interactive SQL Session\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_3_3.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#case-study-questions",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Case Study Questions",
    "text": "Case Study Questions\nThis case study is split into an initial data understanding question before diving straight into data analysis questions before finishing with 1 single extension challenge.\n\nA. Customer Journey\nBased off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\n\n\nB. Data Analysis Questions\n\nHow many customers has Foodie-Fi ever had?\nWhat is the monthly distribution of trial plan start_date values for our dataset - use the start of the month as the group by value.\nWhat plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name.\nWhat is the customer count and percentage of customers who have churned rounded to 1 decimal place?\nHow many customers have churned straight after their initial free trial - what percentage is this rounded to the nearest whole number?\nWhat is the number and percentage of customer plans after their initial free trial?\nWhat is the customer count and percentage breakdown of all 5 plan_name values at 2020-12-31?\nHow many customers have upgraded to an annual plan in 2020?\nHow many days on average does it take for a customer to an annual plan from the day they join Foodie-Fi?\nCan you further breakdown this average value into 30 day periods (i.e. 0-30 days, 31-60 days etc)\nHow many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\n\n\nC. Challenge Payment Question\nThe Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\n\n\nD. Outside The Box Questions\nThe following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\nHow would you calculate the rate of growth for Foodie-Fi?\nWhat key metrics would you recommend Foodie-Fi management to track over time to assess performance of their overall business?\nWhat are some key customer journeys or experiences that you would analyse further to improve customer retention?\nIf the Foodie-Fi team were to create an exit survey shown to customers who wish to cancel their subscription, what questions would you include in the survey?\nWhat business levers could the Foodie-Fi team use to reduce the customer churn rate? How would you validate the effectiveness of your ideas?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#lets-start-solving-them.",
    "href": "portfolio/SQL_Portfolio/Foodie_Fi/Case_Study_3-Foodie-Fi.html#lets-start-solving-them.",
    "title": "Case Study #3 - Foodie-Fi",
    "section": "Let’s start solving them.",
    "text": "Let’s start solving them.\n\nA. Customer Journey\nBased off the 8 sample customers provided in the sample from the subscriptions table, write a brief description about each customer’s onboarding journey.\nTry to keep it as short as possible - you may also want to run some sort of join to make your explanations a bit easier!\n\n\nB. Data Analysis Questions\n\n1. How many customers has Foodie-Fi ever had?\n\n\n\n\nCase_study_3_4.png\n\n\n\n2. What is the monthly distribution of trial plan start_date values for our dataset - use the start of the month as the group by value.\n\n\n\n\nCase_study_3_5.png\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name.\n\n\n\n\nCase_study_3_6.png\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\n\n\n\nCase_study_3_7.png\n\n\n\n5. How many customers have churned straight after their initial free trial - what percentage is this rounded to the nearest whole number?\n\n\n\n\nCase_study_3_8.png\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\n\n\nCase_study_3_9.png\n\n\n\n7. What is the customer count and percentage breakdown of all 5 plan_name values at 2020-12-31?\n8. How many customers have upgraded to an annual plan in 2020?\n\n\n\n\nCase_study_3_10.png\n\n\n\n9. How many days on average does it take for a customer to an annual plan from the day they join Foodie-Fi?\n\n\n\n\nCase_study_3_11.png\n\n\n\n10. Can you further breakdown this average value into 30 day periods (i.e. 0-30 days, 31-60 days etc)\n11. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\n\n\n\nCase_study_3_12.png\n\n\n\n\nC. Challenge Payment Question\nThe Foodie-Fi team wants you to create a new payments table for the year 2020 that includes amounts paid by each customer in the subscriptions table with the following requirements:\n\nmonthly payments always occur on the same day of month as the original start_date of any monthly paid plan\nupgrades from basic to monthly or pro plans are reduced by the current paid amount in that month and start immediately\nupgrades from pro monthly to pro annual are paid at the end of the current billing period and also starts at the end of the month period\nonce a customer churns they will no longer make payments\n\nExample outputs for this table might look like the following:\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nplan_id\nplan_name\npayment_date\namount\npayment_order\n\n\n\n\n1\n1\nbasic monthly\n2020-08-08\n9.90\n1\n\n\n1\n1\nbasic monthly\n2020-09-08\n9.90\n2\n\n\n1\n1\nbasic monthly\n2020-10-08\n9.90\n3\n\n\n1\n1\nbasic monthly\n2020-11-08\n9.90\n4\n\n\n1\n1\nbasic monthly\n2020-12-08\n9.90\n5\n\n\n2\n3\npro annual\n2020-09-27\n199.00\n1\n\n\n13\n1\nbasic monthly\n2020-12-22\n9.90\n1\n\n\n15\n2\npro monthly\n2020-03-24\n19.90\n1\n\n\n15\n2\npro monthly\n2020-04-24\n19.90\n2\n\n\n16\n1\nbasic monthly\n2020-06-07\n9.90\n1\n\n\n16\n1\nbasic monthly\n2020-07-07\n9.90\n2\n\n\n16\n1\nbasic monthly\n2020-08-07\n9.90\n3\n\n\n16\n1\nbasic monthly\n2020-09-07\n9.90\n4\n\n\n16\n1\nbasic monthly\n2020-10-07\n9.90\n5\n\n\n16\n3\npro annual\n2020-10-21\n189.10\n6\n\n\n18\n2\npro monthly\n2020-07-13\n19.90\n1\n\n\n18\n2\npro monthly\n2020-08-13\n19.90\n2\n\n\n18\n2\npro monthly\n2020-09-13\n19.90\n3\n\n\n18\n2\npro monthly\n2020-10-13\n19.90\n4\n\n\n18\n2\npro monthly\n2020-11-13\n19.90\n5\n\n\n18\n2\npro monthly\n2020-12-13\n19.90\n6\n\n\n19\n2\npro monthly\n2020-06-29\n19.90\n1\n\n\n19\n2\npro monthly\n2020-07-29\n19.90\n2\n\n\n19\n3\npro annual\n2020-08-29\n199.00\n3\n\n\n\nThe following is the SQL query to achieve the above result.\n\n\nD. Outside The Box Questions\nThe following are open ended questions which might be asked during a technical interview for this case study - there are no right or wrong answers, but answers that make sense from both a technical and a business perspective make an amazing impression!\n\n1. How would you calculate the rate of growth for Foodie-Fi?\n2. What key metrics would you recommend Foodie-Fi management to track over time to assess performance of their overall business?\n3. What are some key customer journeys or experiences that you would analyse further to improve customer retention?\n4. If the Foodie-Fi team were to create an exit survey shown to customers who wish to cancel their subscription, what questions would you include in the survey?\n5. What business levers could the Foodie-Fi team use to reduce the customer churn rate? How would you validate the effectiveness of your ideas?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "Case_study_2_3.png\n\n\n\n\n\nDid you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)\nDanny was scrolling through his Instagram feed when something really caught his eye - “80s Retro Styling and Pizza Is The Future!”\nDanny was sold on the idea, but he knew that pizza alone was not going to help him get seed funding to expand his new Pizza Empire - so he had one more genius idea to combine with it - he was going to Uberize it - and so Pizza Runner was launched!\nDanny started by recruiting “runners” to deliver fresh pizza from Pizza Runner Headquarters (otherwise known as Danny’s house) and also maxed out his credit card to pay freelance developers to build a mobile app to accept orders from customers.\n\n\n\n\nBecause Danny had a few years of experience as a data scientist - he was very aware that data collection was going to be critical for his business’ growth.\nHe has prepared for us an entity relationship diagram of his database design but requires further assistance to clean his data and apply some basic calculations so he can better direct his runners and optimise Pizza Runner’s operations.\nAll datasets exist within the pizza_runner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions.\n\n\n\n\n\n\n\nCase_study_2_1.png\n\n\n\n\nThe runners table shows the registration_date for each new runner\n\n\n\nrunner_id\nregistration_date\n\n\n\n\n1\n2021-01-01\n\n\n2\n2021-01-03\n\n\n3\n2021-01-08\n\n\n4\n2021-01-15\n\n\n\n\n\n\nCustomer pizza orders are captured in the customer_orders table with 1 row for each individual pizza that is part of the order.\nThe pizza_id relates to the type of pizza which was ordered whilst the exclusions are the ingredient_id values which should be removed from the pizza and the extras are the ingredient_id values which need to be added to the pizza.\nNote that customers can order multiple pizzas in a single order with varying exclusions and extras values even if the pizza is the same type!\nThe exclusions and extras columns will need to be cleaned up before using them in your queries.\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2021-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2021-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2021-01-02 23:51:23\n\n\n3\n102\n2\nNaN\n\n2021-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2021-01-04 13:23:46\n\n\n5\n104\n1\nnull\n1\n2021-01-08 21:00:29\n\n\n6\n101\n2\nnull\nnull\n2021-01-08 21:03:13\n\n\n7\n105\n2\nnull\n1\n2021-01-08 21:20:29\n\n\n8\n102\n1\nnull\nnull\n2021-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2021-01-10 11:22:59\n\n\n10\n104\n1\nnull\nnull\n2021-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2021-01-11 18:34:49\n\n\n\n\n\n\nAfter each orders are received through the system - they are assigned to a runner - however not all orders are fully completed and can be cancelled by the restaurant or the customer.\nThe pickup_time is the timestamp at which the runner arrives at the Pizza Runner headquarters to pick up the freshly cooked pizzas. The distance and duration fields are related to how far and long the runner had to travel to deliver the order to the respective customer.\nThere are some known data issues with this table so be careful when using this in your queries - make sure to check the data types for each column in the schema SQL!\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2021-01-01 18:15:34\n20km\n32 minutes\n\n\n\n2\n1\n2021-01-01 19:10:54\n20km\n27 minutes\n\n\n\n3\n1\n2021-01-03 00:12:37\n13.4km\n20 mins\nNaN\n\n\n4\n2\n2021-01-04 13:53:03\n23.4\n40\nNaN\n\n\n5\n3\n2021-01-08 21:10:57\n10\n15\nNaN\n\n\n6\n3\nnull\nnull\nnull\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25km\n25mins\nnull\n\n\n8\n2\n2020-01-10 00:15:02\n23.4 km\n15 minute\nnull\n\n\n9\n2\nnull\nnull\nnull\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10 minutes\nnull\n\n\n\n\n\n\nAt the moment - Pizza Runner only has 2 pizzas available the Meat Lovers or Vegetarian!\n\n\n\npizza_id\npizza_name\n\n\n\n\n1\nMeat Lovers\n\n\n2\nVegetarian\n\n\n\n\n\n\nEach pizza_id has a standard set of toppings which are used as part of the pizza recipe.\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n\n\n\n\nThis table contains all of the topping_name values with their corresponding topping_id value\n\n\n\ntopping_id\ntopping_name\n\n\n\n\n1\nBacon\n\n\n2\nBBQ Sauce\n\n\n3\nBeef\n\n\n4\nCheese\n\n\n5\nChicken\n\n\n6\nMushrooms\n\n\n7\nOnions\n\n\n8\nPepperoni\n\n\n9\nPeppers\n\n\n10\nSalami\n\n\n11\nTomatoes\n\n\n12\nTomato Sauce\n\n\n\n\n\n\n\nThe Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_2_2.png\n\n\n\n\n\nThis case study has LOTS of questions - they are broken up by area of focus including:\n\nPizza Metrics\nRunner and Customer Experience\nIngredient Optimisation\nPricing and Ratings\nBonus DML Challenges (DML = Data Manipulation Language)\n\nEach of the following case study questions can be answered using a single SQL statement.\nAgain, there are many questions in this case study - please feel free to pick and choose which ones you’d like to try!\nBefore you start writing your SQL queries however - you might want to investigate the data, you may want to do something with some of those null values and data types in the customer_orders and runner_orders tables!\n\n\n\nHow many pizzas were ordered?\nHow many unique customer orders were made?\nHow many successful orders were delivered by each runner?\nHow many of each type of pizza was delivered?\nHow many Vegetarian and Meatlovers were ordered by each customer?\nWhat was the maximum number of pizzas delivered in a single order?\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\nHow many pizzas were delivered that had both exclusions and extras?\nWhat was the total volume of pizzas ordered for each hour of the day?\nWhat was the volume of orders for each day of the week?\n\n\n\n\n\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\nWhat was the average distance travelled for each customer?\nWhat was the difference between the longest and shortest delivery times for all orders?\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\nWhat is the successful delivery percentage for each runner?\n\n\n\n\n\nWhat are the standard ingredients for each pizza?\nWhat was the most commonly added extra?\nWhat was the most common exclusion?\nGenerate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nGenerate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients\n\nFor example: \"Meat Lovers: 2xBacon, Beef, ... , Salami\".\n\nWhat is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\nIf a Meat Lovers pizza costs \\$12 and Vegetarian costs \\$10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\nWhat if there was an additional \\$1 charge for any pizza extras?\n\nAdd cheese is \\$1 extra\n\nThe Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\nUsing your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\nIf a Meat Lovers pizza was \\$12 and Vegetarian \\$10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?\n\n\n\n\n\n\nCleaning the Customer_Orders table\nExplanation:\nThe provided SQL script is focused on cleaning and transforming the customer_orders table in the pizza_runner database. The script involves creating a temporary table named customer_orders_temp with specific modifications to handle cases where certain columns may have null or ‘null’ values.\n1) Cleaning Customer_Orders Table:\n-- Cleaning Customer_Orders Table\nUSE pizza_runner;\n\n-- Drop the temporary table if it exists\nDROP TABLE IF EXISTS customer_orders_temp;\n\n-- Create a temporary table with cleaned data\nCREATE TEMPORARY TABLE customer_orders_temp AS\nSELECT \n    order_id,\n    customer_id,\n    pizza_id,\n    CASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN ''\n        ELSE exclusions END AS exclusions,\n    CASE WHEN extras IS NULL OR extras LIKE 'null' THEN ''\n        ELSE extras END AS extras,\n    order_time\nFROM customer_orders;\nStep-by-Step Explanation:\n1) Database Selection:\n- `USE pizza_runner`: Specifies the `pizza_runner` database for subsequent operations.\n2) Drop Existing Temporary Table:\n- `DROP TABLE IF EXISTS customer_orders_temp`: Drops the temporary table `customer_orders_temp` if it already exists. This ensures a clean slate for creating the table.\n3) Temporary Table Creation:\n- `CREATE TEMPORARY TABLE customer_orders_temp AS`: Initiates the creation of a new temporary table named `customer_orders_temp`\n4) Data Transformation with CASE Statements:\n- `CASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN '' ELSE exclusions END AS exclusions`: Replaces null or 'null' values in the `exclusions` column with an empty string.\n- `CASE WHEN extras IS NULL OR extras LIKE 'null' THEN '' ELSE extras END AS extras`: Replaces null or 'null' values in the `extras column with an empty string.\n5) Column Selection:\n- `SELECT order_id, customer_id, pizza_id, exclusions, extras, order_time FROM customer_orders_temp`: Selects specific columns from the customer_orders_temp table.\nFinal Concise Explanation: This script is useful for cleaning up potential inconsistencies or issues in the customer_orders table related to the exclusions and extras columns, making the data more consistent and ready for further analysis. The use of a temporary table helps avoid permanent changes to the original data.\n\n\n\nCase_study_2_4.png\n\n\nThe output of the above query looks like:\n\n\n\nCase_study_2_22.png\n\n\nCleaning the Runner_Orders table\nExplanation:\nThe provided SQL script is focused on cleaning and transforming the runner_orders table in the pizza_runner database. The script involves creating a temporary table named runner_orders_temp with specific modifications to handle cases where certain columns may have ‘null’ or inconsistent values.\n1) Cleaning Runner_Orders Table:\n-- Cleaning Runner_Orders Table\nDROP TABLE IF EXISTS runner_orders_temp;\n\nCREATE TEMPORARY TABLE runner_orders_temp AS\nSELECT \n    order_id,\n    runner_id,\n    CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time,\n    CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance)\n        ELSE distance END AS FLOAT) AS distance,\n    CAST(CASE WHEN duration LIKE \"null\" THEN NULL\n        WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration)\n        WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration)\n        WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration)\n        ELSE duration END AS FLOAT) AS duration,\n    CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL\n        ELSE cancellation END AS cancellation\n    FROM runner_orders;\nStep-by-Step Explanation:\n1) Drop Existing Temporary Table:\n- DROP TABLE IF EXISTS runner_orders_temp: Drops the temporary table `runner_orders_temp` if it already exists. This ensures a clean slate for creating the table.\n2) Temporary Table Creation:\n- CREATE TEMPORARY TABLE runner_orders_temp AS: Initiates the creation of a new temporary table named `runner_orders_temp`\n3) Data Transformation with CASE Statements:\n- CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time: Converts the pickup_time column to a DATETIME type, handling cases where the value is 'null'.\n- CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance) ELSE distance END AS FLOAT) AS distance: Converts the distance column to FLOAT, handling cases where the value is 'null' or includes 'km'.\n- CAST(CASE WHEN duration LIKE \"null\" THEN NULL WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration) WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration) WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration) ELSE duration END AS FLOAT) AS duration: Converts the duration column to FLOAT, handling cases where the value is 'null' or includes various forms of minutes.\n4) Handling Cancellation Values:\n- CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL ELSE cancellation END AS cancellation: Replaces empty, 'null', or 'NaN' values in the cancellation column with NULL.\n5) Column Selection:\n- SELECT order_id, runner_id, pickup_time, distance, duration, cancellation FROM runner_orders_temp: Selects specific columns from the runner_orders_temp table.\nFinal Concise Explanation: This script aims to handle inconsistencies and ‘null’ values in the runner_orders table, making the data more consistent and suitable for further analysis. The use of a temporary table ensures that the original data remains unchanged.\n\n\n\nCase_study_2_22.png\n\n\nThe output of the above query looks like:\n\n\n\nCase_study_2_29.png\n\n\nChanging the Data Types of columns in runner_orders TABLE\nExplanation: This query modifies the data types in the runner_orders_temp table:\n\npickup_time is changed to a DATETIME type for date and time values.\ndistance and duration are changed to FLOAT types for decimal number representation.\n\n\n\n\nCase_study_2_6.png\n\n\n\n\n\n\n1. How many pizzas were ordered?\n\nExplanation: The provided SQL query is aimed at calculating the total number of pizzas that were ordered.\n\n\n\nCase_study_2_7.png\n\n\n\n2. How many unique customer orders were made?\n\nExplnation: This query provides insight into the number of distinct customer orders, offering a measure of the variety and uniqueness of orders within the dataset. The use of DISTINCT ensures that each order_id is counted only once in the calculation.\n\n\n\nCase_study_2_7.png\n\n\n\n3. How many successful orders were delivered by each runner?\n\nExplanation: The provided SQL query is intended to determine the number of successful orders delivered by each runner. The query produces a list of runner_id values along with the count of successful deliveries for each runner, where successful deliveries are identified by a non-zero distance.\nThis information provides insights into the performance of each runner in terms of successfully delivering orders.\n\n\n\nCase_study_2_9.png\n\n\n\n4. How many of each type of pizza was delivered?\n\nExplnation: The provided SQL query is intended to determine the number of each type of pizza that was delivered.The query produces a list of pizza_name values along with the count of successful deliveries for each type of pizza, where successful deliveries are identified by a non-zero distance.\nThis information provides insights into the popularity or demand for each type of pizza among delivered orders.\n\n\n\nCase_study_2_10.png\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\nExplanation: The provided SQL query is intended to determine the count of Vegetarian and Meatlovers pizzas ordered by each customer.\nThe query produces a list of customer_id, pizza_name, and the count of orders (order_cnt) for each combination of customer and pizza type. This information provides insights into the ordering preferences of each customer, specifically regarding the number of Vegetarian and Meatlovers pizzas ordered.\n\n\n\nCase_study_2_11.png\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\nExplanation: The provided SQL query aims to determine the maximum number of pizzas delivered in a single order.\nThe query produces a list of order_id and the count of pizzas (pizza_cnt) for each order. The results are ordered in descending order of pizza count, allowing you to identify the order with the maximum number of pizzas delivered.\n\n\n\nCase_study_2_12.png\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many had no changes?\n\nExplanation: The given query is analyzing customer orders, specifically focusing on delivered pizzas, to understand how many pizzas for each customer had at least one change (exclusions or extras) and how many had no changes.\nThis query helps identify customer preferences for customized pizzas. Customers with higher counts in the ‘Change’ column may prefer pizzas with modifications, while those with higher counts in the ‘No_Change’ column prefer pizzas without any changes. It offers insights into the distribution of delivered pizzas based on customization preferences among customers.\n\n\n\nCase_study_2_30.png\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\nExplanation: This query analyzes delivered pizzas to find out how many had both exclusions and extras.\nThe query provides insights into the number of delivered pizzas for each customer that had both exclusions and extras. It helps understand how often customers choose to customize their pizzas with both exclusions and extras in a single order.\n\n\n\nCase_study_2_31.png\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\nExplanation: The query provides insights into the total volume of pizzas ordered for each hour of the day. It helps identify peak hours of pizza orders, allowing businesses to optimize staffing and resources accordingly.\n\n\n\nCase_study_2_15.png\n\n\n\n10. What was the volume of orders for each day of the week?\n\nExplanation: The query provides insights into the volume of orders for each day of the week. It helps identify trends and patterns in customer ordering behavior throughout the week, aiding businesses in adjusting operations and promotions accordingly.\n\n\n\nCase_study_2_16.png\n\n\n\n\n\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\n\nExplanation: The query provides insights into the number of runners who signed up for each one-week period. This information can be useful for understanding the trends in runner sign-ups over time and evaluating the effectiveness of marketing or recruitment efforts.\n\n\n\nCase_study_2_32.png\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\n\nExplanation: This query calculates the average time in minutes it took for each runner to arrive at Pizza Runner HQ to pick up the order. Here’s a step-by-step explanation:\n1) Creation of Temporary Table (Common Table Expression - CTE):\n- WITH runners_pick_cte AS (...): Defines a CTE named runners_pick_cte to calculate the average pickup time for each runner.\n- Subquery inside CTE:\n    - SELECT runner_id, ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time: Calculates the average time difference in minutes between the order_time and pickup_time for each runner.\n    - FROM runner_orders_temp JOIN customer_orders_temp ON ...: Joins the runner_orders_temp and customer_orders_temp tables based on the order ID to get relevant information.\n    - WHERE runner_orders_temp.distance != 0: Filters out cases where the distance is 0, ensuring only successful deliveries are considered.\n    - GROUP BY runner_id: Groups the results by runner_id.\n    \n2) Main Query:\n- SELECT ROUND(AVG(avg_time),0) AS avg_pick_time FROM runners_pick_cte: Calculates the overall average pickup time by rounding the average pickup times from the CTE to the nearest whole number.\nFinal Concise Explanation: The query provides insights into the average time it took for each runner to arrive at Pizza Runner HQ for order pickups. The result is the overall average pickup time rounded to the nearest whole number. This information can be valuable for evaluating the efficiency of runners in fulfilling orders.\n\n\n\nCase_study_2_17.png\n\n\n\n3. Is there any relationship between the number of pizzas and how long the order takes to prepare?\n\nExplanation: This query aims to investigate the relationship between the number of pizzas in an order and the time it takes to prepare that order.\n1) Creation of Temporary Table (Common Table Expression - CTE):\n- WITH order_count_cte AS (...): Defines a CTE named order_count_cte to calculate the count of pizzas in each order (pizza_order_count) and the average time to prepare each order (avg_time_to_prepare).\n- Subquery inside CTE:\n    - SELECT customer_orders_temp.order_id, COUNT(customer_orders_temp.order_id) AS pizza_order_count, ...: Counts the number of pizzas in each order and calculates the average time to prepare each order.\n    - FROM runner_orders_temp JOIN customer_orders_temp ON ...: Joins the runner_orders_temp and customer_orders_temp tables based on the order ID to get relevant information.\n    - WHERE pickup_time IS NOT NULL: Filters out cases where the pickup time is not available, ensuring only successfully delivered orders are considered.\n    - GROUP BY customer_orders_temp.order_id: Groups the results by order ID.\n    \n2) Main Query:\n- SELECT pizza_order_count, ROUND(AVG(avg_time_to_prepare),2) AS avg_time_to_prepare FROM order_count_cte GROUP BY pizza_order_count;: Calculates the average time to prepare orders for each count of pizzas (pizza_order_count).\nFinal Concise Explanation: The query helps explore whether there is any correlation between the number of pizzas in an order and the time it takes to prepare that order. The result provides insights into the average preparation time based on the count of pizzas in the order. Analyzing this information can be valuable for understanding the efficiency of order preparation processes based on the order size.\n\n\n\nCase_study_2_18.png\n\n\n\n4. What was the average distance travelled for each customer?\n\n\n\n\nCase_study_2_20.png\n\n\n\n5. What was the difference between the longest and shortest delivery times for all orders?\n\n\n\n\nCase_study_2_33.png\n\n\n\n6. What was the average speed for each runner for each delivery and do you notice any trend for these values?\n7. What is the successful delivery percentage for each runner?\n\n\n\n\nCase_study_2_34.png\n\n\n\n\n\nExplanation:\nThis query is performing data cleaning on the pizza_recipes table and creating a temporary table pizza_recipes_temp. It’s extracting individual topping IDs for each pizza from the comma-separated toppings column using the SUBSTRING_INDEX function. Let’s break down the steps:\n1) Subquery to Generate Numbers:\n- Subquery to Generate Numbers\n  SELECT 1 AS n\n  UNION SELECT 2\n  UNION SELECT 3\n  UNION SELECT 4\n  UNION SELECT 5\n  UNION SELECT 6\n  UNION SELECT 7\n  UNION SELECT 8\n  UNION SELECT 9\n  UNION SELECT 10;\nThis subquery generates numbers from 1 to 10. The UNION operator is used to combine the results into a single column.\n2) JOIN with pizza_recipes:\n- Joining with pizza_recipes to Extract Toppings\n  SELECT\n      pizza_id,\n      SUBSTRING_INDEX(SUBSTRING_INDEX(toppings, ',', n), ',', -1) AS topping_id\n  FROM\n      pizza_recipes\n  JOIN (\n      -- Subquery with Numbers\n      SELECT 1 AS n\n      UNION SELECT 2\n      UNION SELECT 3\n      UNION SELECT 4\n      UNION SELECT 5\n      UNION SELECT 6\n      UNION SELECT 7\n      UNION SELECT 8\n      UNION SELECT 9\n      UNION SELECT 10\n   ) AS numbers ON CHAR_LENGTH(toppings) - CHAR_LENGTH(REPLACE(toppings, ',', '')) &gt;= n - 1\n    ORDER BY\n      pizza_id;\nHere, we are joining the pizza_recipes table with the subquery (numbers). The SUBSTRING_INDEX function is used to extract individual toppings based on the generated numbers.\n3) Create Temporary Table:\n- CREATE TEMPORARY TABLE pizza_recipes_temp AS \nFinally, we use the results from the previous query to create a temporary table named pizza_recipes_temp.\nFinal Concise Explanation: The goal is to break down the toppings of each pizza in the original pizza_recipes table into individual rows, associating each row with the corresponding pizza ID.\n\n\n\nCase_study_2_35.png\n\n\nThe output of the above query is shown as below:\n\n\n\nCase_study_2_25.png\n\n\n\n1) What are the standard ingredients for each pizza?\n\nExplanation:\nThe query aims to retrieve the standard ingredients for each pizza by joining the pizza_names, pizza_recipes_temp, and pizza_toppings tables.\n1) SELECT Statement:\n- SELECT \n    pizza_names.pizza_id,\n    pizza_names.pizza_name,\n    -- Uses the GROUP_CONCAT function to concatenate distinct topping names into a comma-separated list for each pizza.\n    GROUP_CONCAT(DISTINCT topping_name) AS topping_name_\n2) JOIN Conditions:\n- JOIN pizza_recipes_temp ON pizza_names.pizza_id = pizza_recipes_temp.pizza_id\n- JOIN pizza_toppings ON pizza_recipes_temp.topping_id = pizza_toppings.topping_id\nHere, we’re joining the pizza_names table with the temporary table on the pizza ID and temporary table with the pizza_toppings table on the topping ID.\nFinal Concise Explanation: The final result is a list of pizzas with their respective standard ingredients (topping names) presented as a comma-separated list. The query aggregates the toppings for each pizza, providing an overview of the standard ingredients for all pizzas in the dataset.\n\n\n\nCase_study_2_36.png\n\n\n\n2. What was the most commonly added extra?\n\nExplanation:\nThis query aims to identify the most commonly added extra topping to pizza orders. \n1) Common Table Expression (CTE):\n- WITH cte AS (\nSELECT\n    order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM\n    customer_orders\nJOIN (\n    SELECT 1 AS n\n    UNION SELECT 2\n    -- Add more numbers if needed\n) AS numbers ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n - 1\nWHERE extras IS NOT NULL)\nA CTE named cte is defined to extract individual topping IDs from the extras column in the customer_orders table. It uses the SUBSTRING_INDEX function to split the extras string, and TRIM is used to remove leading or trailing spaces.\n2) Main Query:\n- SELECT topping_name, COUNT(order_id) AS most_common_extras\n  FROM cte\n  JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\n  GROUP BY topping_name\n  LIMIT 1;\nThe main query selects the topping_name and counts the occurrences of each extra topping using the CTE cte. It joins the CTE with the pizza_toppings table to retrieve the names of the extra toppings. The results are grouped by topping_name, and COUNT(order_id) calculates the number of orders containing each extra topping. Finally, LIMIT 1 is applied to get only the most common extra topping.\nFinal Concise Explanation: The overall purpose of the query is to find and report the most commonly added extra topping to pizza orders based on the available dataset.\n\n\n\nCase_study_2_27.png\n\n\n\n3. What was the most common exclusion?\n\nExplanation:\nThis query aims to identify the most common exclusion (topping omitted) from pizza orders.\n1) Common Table Expression (CTE):\n- WITH cte AS (\nSELECT\n    order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM\n    customer_orders\nJOIN (\n    SELECT 1 AS n\n    UNION SELECT 2\n    -- Add more numbers if needed\n) AS numbers ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n - 1\nWHERE exclusions IS NOT NULL)\nA CTE named cte is defined to extract individual topping IDs from the exclusions column in the customer_orders table. It uses the SUBSTRING_INDEX function to split the exclusions string, and TRIM is used to remove leading or trailing spaces.\n2) Main Query:\n- SELECT topping_name, COUNT(order_id) AS most_common_exclusions\n  FROM cte\n  JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\n  GROUP BY topping_name\n  LIMIT 1;\nThe main query selects the topping_name and counts the occurrences of each exclusion topping using the CTE cte. It joins the CTE with the pizza_toppings table to retrieve the names of the exclusion toppings. The results are grouped by topping_name, and COUNT(order_id) calculates the number of orders excluding each topping. Finally, LIMIT 1 is applied to get only the most common exclusion topping.\nFinal Concise Explanation: The overall purpose of the query is to find and report the most commonly excluded topping from pizza orders based on the available dataset.\n\n\n\nCase_study_2_28.png\n\n\n\n4. Generate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\n5. Generate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients For example: “Meat Lovers: 2xBacon, Beef, … , Salami”\n6. What is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\n1. If a Meat Lovers pizza costs \\$12 and Vegetarian costs \\$10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\n2. What if there was an additional \\$1 charge for any pizza extras?\n\nAdd cheese is \\$1 extra\n\n3. The Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\n4. Using your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\n5. If a Meat Lovers pizza was \\$12 and Vegetarian \\$10 fixed prices with no cost for extras and each runner is paid \\$0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#introduction",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#introduction",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "Did you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)\nDanny was scrolling through his Instagram feed when something really caught his eye - “80s Retro Styling and Pizza Is The Future!”\nDanny was sold on the idea, but he knew that pizza alone was not going to help him get seed funding to expand his new Pizza Empire - so he had one more genius idea to combine with it - he was going to Uberize it - and so Pizza Runner was launched!\nDanny started by recruiting “runners” to deliver fresh pizza from Pizza Runner Headquarters (otherwise known as Danny’s house) and also maxed out his credit card to pay freelance developers to build a mobile app to accept orders from customers."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#available-data",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#available-data",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "Because Danny had a few years of experience as a data scientist - he was very aware that data collection was going to be critical for his business’ growth.\nHe has prepared for us an entity relationship diagram of his database design but requires further assistance to clean his data and apply some basic calculations so he can better direct his runners and optimise Pizza Runner’s operations.\nAll datasets exist within the pizza_runner database schema - be sure to include this reference within your SQL scripts as you start exploring the data and answering the case study questions."
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#entity-relationship-diagram",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#entity-relationship-diagram",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "Case_study_2_1.png\n\n\n\n\nThe runners table shows the registration_date for each new runner\n\n\n\nrunner_id\nregistration_date\n\n\n\n\n1\n2021-01-01\n\n\n2\n2021-01-03\n\n\n3\n2021-01-08\n\n\n4\n2021-01-15\n\n\n\n\n\n\nCustomer pizza orders are captured in the customer_orders table with 1 row for each individual pizza that is part of the order.\nThe pizza_id relates to the type of pizza which was ordered whilst the exclusions are the ingredient_id values which should be removed from the pizza and the extras are the ingredient_id values which need to be added to the pizza.\nNote that customers can order multiple pizzas in a single order with varying exclusions and extras values even if the pizza is the same type!\nThe exclusions and extras columns will need to be cleaned up before using them in your queries.\n\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2021-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2021-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2021-01-02 23:51:23\n\n\n3\n102\n2\nNaN\n\n2021-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2021-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2021-01-04 13:23:46\n\n\n5\n104\n1\nnull\n1\n2021-01-08 21:00:29\n\n\n6\n101\n2\nnull\nnull\n2021-01-08 21:03:13\n\n\n7\n105\n2\nnull\n1\n2021-01-08 21:20:29\n\n\n8\n102\n1\nnull\nnull\n2021-01-09 23:54:33\n\n\n9\n103\n1\n4\n1, 5\n2021-01-10 11:22:59\n\n\n10\n104\n1\nnull\nnull\n2021-01-11 18:34:49\n\n\n10\n104\n1\n2, 6\n1, 4\n2021-01-11 18:34:49\n\n\n\n\n\n\nAfter each orders are received through the system - they are assigned to a runner - however not all orders are fully completed and can be cancelled by the restaurant or the customer.\nThe pickup_time is the timestamp at which the runner arrives at the Pizza Runner headquarters to pick up the freshly cooked pizzas. The distance and duration fields are related to how far and long the runner had to travel to deliver the order to the respective customer.\nThere are some known data issues with this table so be careful when using this in your queries - make sure to check the data types for each column in the schema SQL!\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2021-01-01 18:15:34\n20km\n32 minutes\n\n\n\n2\n1\n2021-01-01 19:10:54\n20km\n27 minutes\n\n\n\n3\n1\n2021-01-03 00:12:37\n13.4km\n20 mins\nNaN\n\n\n4\n2\n2021-01-04 13:53:03\n23.4\n40\nNaN\n\n\n5\n3\n2021-01-08 21:10:57\n10\n15\nNaN\n\n\n6\n3\nnull\nnull\nnull\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25km\n25mins\nnull\n\n\n8\n2\n2020-01-10 00:15:02\n23.4 km\n15 minute\nnull\n\n\n9\n2\nnull\nnull\nnull\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10 minutes\nnull\n\n\n\n\n\n\nAt the moment - Pizza Runner only has 2 pizzas available the Meat Lovers or Vegetarian!\n\n\n\npizza_id\npizza_name\n\n\n\n\n1\nMeat Lovers\n\n\n2\nVegetarian\n\n\n\n\n\n\nEach pizza_id has a standard set of toppings which are used as part of the pizza recipe.\n\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n\n\n\n\nThis table contains all of the topping_name values with their corresponding topping_id value\n\n\n\ntopping_id\ntopping_name\n\n\n\n\n1\nBacon\n\n\n2\nBBQ Sauce\n\n\n3\nBeef\n\n\n4\nCheese\n\n\n5\nChicken\n\n\n6\nMushrooms\n\n\n7\nOnions\n\n\n8\nPepperoni\n\n\n9\nPeppers\n\n\n10\nSalami\n\n\n11\nTomatoes\n\n\n12\nTomato Sauce"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#interactive-sql-session",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#interactive-sql-session",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "The Dataset for this case study can be accessed from here. I will be using MySQL to solve this case study. In order to solve yourself this case study, simply go to the above link and choose MySQL Dialect (version &gt; 8, if using MySQL version higher than 8 locally), copy & paste the Database schema into MySQL.\nHere is the snapshot of it.\n\n\n\nCase_study_2_2.png"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#case-study-questions",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#case-study-questions",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "This case study has LOTS of questions - they are broken up by area of focus including:\n\nPizza Metrics\nRunner and Customer Experience\nIngredient Optimisation\nPricing and Ratings\nBonus DML Challenges (DML = Data Manipulation Language)\n\nEach of the following case study questions can be answered using a single SQL statement.\nAgain, there are many questions in this case study - please feel free to pick and choose which ones you’d like to try!\nBefore you start writing your SQL queries however - you might want to investigate the data, you may want to do something with some of those null values and data types in the customer_orders and runner_orders tables!\n\n\n\nHow many pizzas were ordered?\nHow many unique customer orders were made?\nHow many successful orders were delivered by each runner?\nHow many of each type of pizza was delivered?\nHow many Vegetarian and Meatlovers were ordered by each customer?\nWhat was the maximum number of pizzas delivered in a single order?\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\nHow many pizzas were delivered that had both exclusions and extras?\nWhat was the total volume of pizzas ordered for each hour of the day?\nWhat was the volume of orders for each day of the week?\n\n\n\n\n\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\nWhat was the average distance travelled for each customer?\nWhat was the difference between the longest and shortest delivery times for all orders?\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\nWhat is the successful delivery percentage for each runner?\n\n\n\n\n\nWhat are the standard ingredients for each pizza?\nWhat was the most commonly added extra?\nWhat was the most common exclusion?\nGenerate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\nGenerate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients\n\nFor example: \"Meat Lovers: 2xBacon, Beef, ... , Salami\".\n\nWhat is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\nIf a Meat Lovers pizza costs \\$12 and Vegetarian costs \\$10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\nWhat if there was an additional \\$1 charge for any pizza extras?\n\nAdd cheese is \\$1 extra\n\nThe Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\nUsing your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\nIf a Meat Lovers pizza was \\$12 and Vegetarian \\$10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?"
  },
  {
    "objectID": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#lets-start-solving-them.",
    "href": "portfolio/SQL_Portfolio/Pizza_Runner/Case_Study_2-Pizza_Runner.html#lets-start-solving-them.",
    "title": "Solving Case Study #2 - Pizza Runner using SQL",
    "section": "",
    "text": "Cleaning the Customer_Orders table\nExplanation:\nThe provided SQL script is focused on cleaning and transforming the customer_orders table in the pizza_runner database. The script involves creating a temporary table named customer_orders_temp with specific modifications to handle cases where certain columns may have null or ‘null’ values.\n1) Cleaning Customer_Orders Table:\n-- Cleaning Customer_Orders Table\nUSE pizza_runner;\n\n-- Drop the temporary table if it exists\nDROP TABLE IF EXISTS customer_orders_temp;\n\n-- Create a temporary table with cleaned data\nCREATE TEMPORARY TABLE customer_orders_temp AS\nSELECT \n    order_id,\n    customer_id,\n    pizza_id,\n    CASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN ''\n        ELSE exclusions END AS exclusions,\n    CASE WHEN extras IS NULL OR extras LIKE 'null' THEN ''\n        ELSE extras END AS extras,\n    order_time\nFROM customer_orders;\nStep-by-Step Explanation:\n1) Database Selection:\n- `USE pizza_runner`: Specifies the `pizza_runner` database for subsequent operations.\n2) Drop Existing Temporary Table:\n- `DROP TABLE IF EXISTS customer_orders_temp`: Drops the temporary table `customer_orders_temp` if it already exists. This ensures a clean slate for creating the table.\n3) Temporary Table Creation:\n- `CREATE TEMPORARY TABLE customer_orders_temp AS`: Initiates the creation of a new temporary table named `customer_orders_temp`\n4) Data Transformation with CASE Statements:\n- `CASE WHEN exclusions IS NULL OR exclusions LIKE 'null' THEN '' ELSE exclusions END AS exclusions`: Replaces null or 'null' values in the `exclusions` column with an empty string.\n- `CASE WHEN extras IS NULL OR extras LIKE 'null' THEN '' ELSE extras END AS extras`: Replaces null or 'null' values in the `extras column with an empty string.\n5) Column Selection:\n- `SELECT order_id, customer_id, pizza_id, exclusions, extras, order_time FROM customer_orders_temp`: Selects specific columns from the customer_orders_temp table.\nFinal Concise Explanation: This script is useful for cleaning up potential inconsistencies or issues in the customer_orders table related to the exclusions and extras columns, making the data more consistent and ready for further analysis. The use of a temporary table helps avoid permanent changes to the original data.\n\n\n\nCase_study_2_4.png\n\n\nThe output of the above query looks like:\n\n\n\nCase_study_2_22.png\n\n\nCleaning the Runner_Orders table\nExplanation:\nThe provided SQL script is focused on cleaning and transforming the runner_orders table in the pizza_runner database. The script involves creating a temporary table named runner_orders_temp with specific modifications to handle cases where certain columns may have ‘null’ or inconsistent values.\n1) Cleaning Runner_Orders Table:\n-- Cleaning Runner_Orders Table\nDROP TABLE IF EXISTS runner_orders_temp;\n\nCREATE TEMPORARY TABLE runner_orders_temp AS\nSELECT \n    order_id,\n    runner_id,\n    CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time,\n    CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance)\n        ELSE distance END AS FLOAT) AS distance,\n    CAST(CASE WHEN duration LIKE \"null\" THEN NULL\n        WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration)\n        WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration)\n        WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration)\n        ELSE duration END AS FLOAT) AS duration,\n    CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL\n        ELSE cancellation END AS cancellation\n    FROM runner_orders;\nStep-by-Step Explanation:\n1) Drop Existing Temporary Table:\n- DROP TABLE IF EXISTS runner_orders_temp: Drops the temporary table `runner_orders_temp` if it already exists. This ensures a clean slate for creating the table.\n2) Temporary Table Creation:\n- CREATE TEMPORARY TABLE runner_orders_temp AS: Initiates the creation of a new temporary table named `runner_orders_temp`\n3) Data Transformation with CASE Statements:\n- CAST(CASE WHEN pickup_time LIKE \"null\" THEN NULL ELSE pickup_time END AS DATETIME) AS pickup_time: Converts the pickup_time column to a DATETIME type, handling cases where the value is 'null'.\n- CAST(CASE WHEN distance LIKE \"null\" THEN NULL WHEN distance LIKE '%km' THEN TRIM('km' FROM distance) ELSE distance END AS FLOAT) AS distance: Converts the distance column to FLOAT, handling cases where the value is 'null' or includes 'km'.\n- CAST(CASE WHEN duration LIKE \"null\" THEN NULL WHEN duration LIKE '%minutes' THEN TRIM('minutes' FROM duration) WHEN duration LIKE '%minute' THEN TRIM('minute' FROM duration) WHEN duration LIKE '%mins' THEN TRIM('mins' FROM duration) ELSE duration END AS FLOAT) AS duration: Converts the duration column to FLOAT, handling cases where the value is 'null' or includes various forms of minutes.\n4) Handling Cancellation Values:\n- CASE WHEN cancellation IN ('', 'null', 'NaN') THEN NULL ELSE cancellation END AS cancellation: Replaces empty, 'null', or 'NaN' values in the cancellation column with NULL.\n5) Column Selection:\n- SELECT order_id, runner_id, pickup_time, distance, duration, cancellation FROM runner_orders_temp: Selects specific columns from the runner_orders_temp table.\nFinal Concise Explanation: This script aims to handle inconsistencies and ‘null’ values in the runner_orders table, making the data more consistent and suitable for further analysis. The use of a temporary table ensures that the original data remains unchanged.\n\n\n\nCase_study_2_22.png\n\n\nThe output of the above query looks like:\n\n\n\nCase_study_2_29.png\n\n\nChanging the Data Types of columns in runner_orders TABLE\nExplanation: This query modifies the data types in the runner_orders_temp table:\n\npickup_time is changed to a DATETIME type for date and time values.\ndistance and duration are changed to FLOAT types for decimal number representation.\n\n\n\n\nCase_study_2_6.png\n\n\n\n\n\n\n1. How many pizzas were ordered?\n\nExplanation: The provided SQL query is aimed at calculating the total number of pizzas that were ordered.\n\n\n\nCase_study_2_7.png\n\n\n\n2. How many unique customer orders were made?\n\nExplnation: This query provides insight into the number of distinct customer orders, offering a measure of the variety and uniqueness of orders within the dataset. The use of DISTINCT ensures that each order_id is counted only once in the calculation.\n\n\n\nCase_study_2_7.png\n\n\n\n3. How many successful orders were delivered by each runner?\n\nExplanation: The provided SQL query is intended to determine the number of successful orders delivered by each runner. The query produces a list of runner_id values along with the count of successful deliveries for each runner, where successful deliveries are identified by a non-zero distance.\nThis information provides insights into the performance of each runner in terms of successfully delivering orders.\n\n\n\nCase_study_2_9.png\n\n\n\n4. How many of each type of pizza was delivered?\n\nExplnation: The provided SQL query is intended to determine the number of each type of pizza that was delivered.The query produces a list of pizza_name values along with the count of successful deliveries for each type of pizza, where successful deliveries are identified by a non-zero distance.\nThis information provides insights into the popularity or demand for each type of pizza among delivered orders.\n\n\n\nCase_study_2_10.png\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\nExplanation: The provided SQL query is intended to determine the count of Vegetarian and Meatlovers pizzas ordered by each customer.\nThe query produces a list of customer_id, pizza_name, and the count of orders (order_cnt) for each combination of customer and pizza type. This information provides insights into the ordering preferences of each customer, specifically regarding the number of Vegetarian and Meatlovers pizzas ordered.\n\n\n\nCase_study_2_11.png\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\nExplanation: The provided SQL query aims to determine the maximum number of pizzas delivered in a single order.\nThe query produces a list of order_id and the count of pizzas (pizza_cnt) for each order. The results are ordered in descending order of pizza count, allowing you to identify the order with the maximum number of pizzas delivered.\n\n\n\nCase_study_2_12.png\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many had no changes?\n\nExplanation: The given query is analyzing customer orders, specifically focusing on delivered pizzas, to understand how many pizzas for each customer had at least one change (exclusions or extras) and how many had no changes.\nThis query helps identify customer preferences for customized pizzas. Customers with higher counts in the ‘Change’ column may prefer pizzas with modifications, while those with higher counts in the ‘No_Change’ column prefer pizzas without any changes. It offers insights into the distribution of delivered pizzas based on customization preferences among customers.\n\n\n\nCase_study_2_30.png\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\nExplanation: This query analyzes delivered pizzas to find out how many had both exclusions and extras.\nThe query provides insights into the number of delivered pizzas for each customer that had both exclusions and extras. It helps understand how often customers choose to customize their pizzas with both exclusions and extras in a single order.\n\n\n\nCase_study_2_31.png\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\nExplanation: The query provides insights into the total volume of pizzas ordered for each hour of the day. It helps identify peak hours of pizza orders, allowing businesses to optimize staffing and resources accordingly.\n\n\n\nCase_study_2_15.png\n\n\n\n10. What was the volume of orders for each day of the week?\n\nExplanation: The query provides insights into the volume of orders for each day of the week. It helps identify trends and patterns in customer ordering behavior throughout the week, aiding businesses in adjusting operations and promotions accordingly.\n\n\n\nCase_study_2_16.png\n\n\n\n\n\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01).\n\nExplanation: The query provides insights into the number of runners who signed up for each one-week period. This information can be useful for understanding the trends in runner sign-ups over time and evaluating the effectiveness of marketing or recruitment efforts.\n\n\n\nCase_study_2_32.png\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\n\nExplanation: This query calculates the average time in minutes it took for each runner to arrive at Pizza Runner HQ to pick up the order. Here’s a step-by-step explanation:\n1) Creation of Temporary Table (Common Table Expression - CTE):\n- WITH runners_pick_cte AS (...): Defines a CTE named runners_pick_cte to calculate the average pickup time for each runner.\n- Subquery inside CTE:\n    - SELECT runner_id, ROUND(AVG(TIMESTAMPDIFF(MINUTE, order_time, pickup_time)),2) AS avg_time: Calculates the average time difference in minutes between the order_time and pickup_time for each runner.\n    - FROM runner_orders_temp JOIN customer_orders_temp ON ...: Joins the runner_orders_temp and customer_orders_temp tables based on the order ID to get relevant information.\n    - WHERE runner_orders_temp.distance != 0: Filters out cases where the distance is 0, ensuring only successful deliveries are considered.\n    - GROUP BY runner_id: Groups the results by runner_id.\n    \n2) Main Query:\n- SELECT ROUND(AVG(avg_time),0) AS avg_pick_time FROM runners_pick_cte: Calculates the overall average pickup time by rounding the average pickup times from the CTE to the nearest whole number.\nFinal Concise Explanation: The query provides insights into the average time it took for each runner to arrive at Pizza Runner HQ for order pickups. The result is the overall average pickup time rounded to the nearest whole number. This information can be valuable for evaluating the efficiency of runners in fulfilling orders.\n\n\n\nCase_study_2_17.png\n\n\n\n3. Is there any relationship between the number of pizzas and how long the order takes to prepare?\n\nExplanation: This query aims to investigate the relationship between the number of pizzas in an order and the time it takes to prepare that order.\n1) Creation of Temporary Table (Common Table Expression - CTE):\n- WITH order_count_cte AS (...): Defines a CTE named order_count_cte to calculate the count of pizzas in each order (pizza_order_count) and the average time to prepare each order (avg_time_to_prepare).\n- Subquery inside CTE:\n    - SELECT customer_orders_temp.order_id, COUNT(customer_orders_temp.order_id) AS pizza_order_count, ...: Counts the number of pizzas in each order and calculates the average time to prepare each order.\n    - FROM runner_orders_temp JOIN customer_orders_temp ON ...: Joins the runner_orders_temp and customer_orders_temp tables based on the order ID to get relevant information.\n    - WHERE pickup_time IS NOT NULL: Filters out cases where the pickup time is not available, ensuring only successfully delivered orders are considered.\n    - GROUP BY customer_orders_temp.order_id: Groups the results by order ID.\n    \n2) Main Query:\n- SELECT pizza_order_count, ROUND(AVG(avg_time_to_prepare),2) AS avg_time_to_prepare FROM order_count_cte GROUP BY pizza_order_count;: Calculates the average time to prepare orders for each count of pizzas (pizza_order_count).\nFinal Concise Explanation: The query helps explore whether there is any correlation between the number of pizzas in an order and the time it takes to prepare that order. The result provides insights into the average preparation time based on the count of pizzas in the order. Analyzing this information can be valuable for understanding the efficiency of order preparation processes based on the order size.\n\n\n\nCase_study_2_18.png\n\n\n\n4. What was the average distance travelled for each customer?\n\n\n\n\nCase_study_2_20.png\n\n\n\n5. What was the difference between the longest and shortest delivery times for all orders?\n\n\n\n\nCase_study_2_33.png\n\n\n\n6. What was the average speed for each runner for each delivery and do you notice any trend for these values?\n7. What is the successful delivery percentage for each runner?\n\n\n\n\nCase_study_2_34.png\n\n\n\n\n\nExplanation:\nThis query is performing data cleaning on the pizza_recipes table and creating a temporary table pizza_recipes_temp. It’s extracting individual topping IDs for each pizza from the comma-separated toppings column using the SUBSTRING_INDEX function. Let’s break down the steps:\n1) Subquery to Generate Numbers:\n- Subquery to Generate Numbers\n  SELECT 1 AS n\n  UNION SELECT 2\n  UNION SELECT 3\n  UNION SELECT 4\n  UNION SELECT 5\n  UNION SELECT 6\n  UNION SELECT 7\n  UNION SELECT 8\n  UNION SELECT 9\n  UNION SELECT 10;\nThis subquery generates numbers from 1 to 10. The UNION operator is used to combine the results into a single column.\n2) JOIN with pizza_recipes:\n- Joining with pizza_recipes to Extract Toppings\n  SELECT\n      pizza_id,\n      SUBSTRING_INDEX(SUBSTRING_INDEX(toppings, ',', n), ',', -1) AS topping_id\n  FROM\n      pizza_recipes\n  JOIN (\n      -- Subquery with Numbers\n      SELECT 1 AS n\n      UNION SELECT 2\n      UNION SELECT 3\n      UNION SELECT 4\n      UNION SELECT 5\n      UNION SELECT 6\n      UNION SELECT 7\n      UNION SELECT 8\n      UNION SELECT 9\n      UNION SELECT 10\n   ) AS numbers ON CHAR_LENGTH(toppings) - CHAR_LENGTH(REPLACE(toppings, ',', '')) &gt;= n - 1\n    ORDER BY\n      pizza_id;\nHere, we are joining the pizza_recipes table with the subquery (numbers). The SUBSTRING_INDEX function is used to extract individual toppings based on the generated numbers.\n3) Create Temporary Table:\n- CREATE TEMPORARY TABLE pizza_recipes_temp AS \nFinally, we use the results from the previous query to create a temporary table named pizza_recipes_temp.\nFinal Concise Explanation: The goal is to break down the toppings of each pizza in the original pizza_recipes table into individual rows, associating each row with the corresponding pizza ID.\n\n\n\nCase_study_2_35.png\n\n\nThe output of the above query is shown as below:\n\n\n\nCase_study_2_25.png\n\n\n\n1) What are the standard ingredients for each pizza?\n\nExplanation:\nThe query aims to retrieve the standard ingredients for each pizza by joining the pizza_names, pizza_recipes_temp, and pizza_toppings tables.\n1) SELECT Statement:\n- SELECT \n    pizza_names.pizza_id,\n    pizza_names.pizza_name,\n    -- Uses the GROUP_CONCAT function to concatenate distinct topping names into a comma-separated list for each pizza.\n    GROUP_CONCAT(DISTINCT topping_name) AS topping_name_\n2) JOIN Conditions:\n- JOIN pizza_recipes_temp ON pizza_names.pizza_id = pizza_recipes_temp.pizza_id\n- JOIN pizza_toppings ON pizza_recipes_temp.topping_id = pizza_toppings.topping_id\nHere, we’re joining the pizza_names table with the temporary table on the pizza ID and temporary table with the pizza_toppings table on the topping ID.\nFinal Concise Explanation: The final result is a list of pizzas with their respective standard ingredients (topping names) presented as a comma-separated list. The query aggregates the toppings for each pizza, providing an overview of the standard ingredients for all pizzas in the dataset.\n\n\n\nCase_study_2_36.png\n\n\n\n2. What was the most commonly added extra?\n\nExplanation:\nThis query aims to identify the most commonly added extra topping to pizza orders. \n1) Common Table Expression (CTE):\n- WITH cte AS (\nSELECT\n    order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(extras, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM\n    customer_orders\nJOIN (\n    SELECT 1 AS n\n    UNION SELECT 2\n    -- Add more numbers if needed\n) AS numbers ON CHAR_LENGTH(extras) - CHAR_LENGTH(REPLACE(extras, ',', '')) &gt;= n - 1\nWHERE extras IS NOT NULL)\nA CTE named cte is defined to extract individual topping IDs from the extras column in the customer_orders table. It uses the SUBSTRING_INDEX function to split the extras string, and TRIM is used to remove leading or trailing spaces.\n2) Main Query:\n- SELECT topping_name, COUNT(order_id) AS most_common_extras\n  FROM cte\n  JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\n  GROUP BY topping_name\n  LIMIT 1;\nThe main query selects the topping_name and counts the occurrences of each extra topping using the CTE cte. It joins the CTE with the pizza_toppings table to retrieve the names of the extra toppings. The results are grouped by topping_name, and COUNT(order_id) calculates the number of orders containing each extra topping. Finally, LIMIT 1 is applied to get only the most common extra topping.\nFinal Concise Explanation: The overall purpose of the query is to find and report the most commonly added extra topping to pizza orders based on the available dataset.\n\n\n\nCase_study_2_27.png\n\n\n\n3. What was the most common exclusion?\n\nExplanation:\nThis query aims to identify the most common exclusion (topping omitted) from pizza orders.\n1) Common Table Expression (CTE):\n- WITH cte AS (\nSELECT\n    order_id,\n    CAST(TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(exclusions, ',', n), ',', -1)) AS UNSIGNED) AS topping_id\nFROM\n    customer_orders\nJOIN (\n    SELECT 1 AS n\n    UNION SELECT 2\n    -- Add more numbers if needed\n) AS numbers ON CHAR_LENGTH(exclusions) - CHAR_LENGTH(REPLACE(exclusions, ',', '')) &gt;= n - 1\nWHERE exclusions IS NOT NULL)\nA CTE named cte is defined to extract individual topping IDs from the exclusions column in the customer_orders table. It uses the SUBSTRING_INDEX function to split the exclusions string, and TRIM is used to remove leading or trailing spaces.\n2) Main Query:\n- SELECT topping_name, COUNT(order_id) AS most_common_exclusions\n  FROM cte\n  JOIN pizza_toppings ON pizza_toppings.topping_id = cte.topping_id\n  GROUP BY topping_name\n  LIMIT 1;\nThe main query selects the topping_name and counts the occurrences of each exclusion topping using the CTE cte. It joins the CTE with the pizza_toppings table to retrieve the names of the exclusion toppings. The results are grouped by topping_name, and COUNT(order_id) calculates the number of orders excluding each topping. Finally, LIMIT 1 is applied to get only the most common exclusion topping.\nFinal Concise Explanation: The overall purpose of the query is to find and report the most commonly excluded topping from pizza orders based on the available dataset.\n\n\n\nCase_study_2_28.png\n\n\n\n4. Generate an order item for each record in the customers_orders table in the format of one of the following:\n\nMeat Lovers\nMeat Lovers - Exclude Beef\nMeat Lovers - Extra Bacon\nMeat Lovers - Exclude Cheese, Bacon - Extra Mushroom, Peppers\n\n5. Generate an alphabetically ordered comma separated ingredient list for each pizza order from the customer_orders table and add a 2x in front of any relevant ingredients For example: “Meat Lovers: 2xBacon, Beef, … , Salami”\n6. What is the total quantity of each ingredient used in all delivered pizzas sorted by most frequent first?\n\n\n\n\n\n1. If a Meat Lovers pizza costs \\$12 and Vegetarian costs \\$10 and there were no charges for changes - how much money has Pizza Runner made so far if there are no delivery fees?\n2. What if there was an additional \\$1 charge for any pizza extras?\n\nAdd cheese is \\$1 extra\n\n3. The Pizza Runner team now wants to add an additional ratings system that allows customers to rate their runner, how would you design an additional table for this new dataset - generate a schema for this new table and insert your own data for ratings for each successful customer order between 1 to 5.\n4. Using your newly generated table - can you join all of the information together to form a table which has the following information for successful deliveries?\n\ncustomer_id\norder_id\nrunner_id\nrating\norder_time\npickup_time\nTime between order and pickup\nDelivery duration\nAverage speed\nTotal number of pizzas\n\n5. If a Meat Lovers pizza was \\$12 and Vegetarian \\$10 fixed prices with no cost for extras and each runner is paid \\$0.30 per kilometre traveled - how much money does Pizza Runner have left over after these deliveries?\n\n\n\n\nIf Danny wants to expand his range of pizzas - how would this impact the existing data design? Write an INSERT statement to demonstrate what would happen if a new Supreme pizza with all the toppings was added to the Pizza Runner menu?"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "SQL Portfolio\n\nSQL Case Studies Projects\nThis series contains the SQL case studies I solved as part of the 8 Week SQL Challenge from 8weekssqlchallenge.com.\nThis series contains 8 case studies that have been solved using MySQL RDBMS, with explanations.\n\n\n\nNLP & GenerativeAI Portfolio\n\nNLP & GenerativeAI Projects\nThis series contains all my NLP & GenerativeAI Projects.\n\n\n\nComputer Vision Portfolio\n\nComputer Vision Projects\nThis series contains all my Computer Vision Projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #7 - Balanced Tree Clothing Co.\n\n\n36 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #4 - Data Bank\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #5 - Data Mart\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study #3 - Foodie-Fi\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving Case Study #6 - Clique Bait using SQL\n\n\n36 min\n\n\n\nSQL\n\n\nMySQL\n\n\n\nIn this post, we will solve an end-to-end an analytical case study using SQL.\n\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\n\n\n\nSolving Case Study #2 - Pizza Runner using SQL\n\n\n36 min\n\n\n\nSQL\n\n\nMySQL\n\n\n\nIn this post, I will solve an end-to-end an analytical case study using SQL.\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\nCase Study #1 - Danny’s Diner\n\n\n23 min\n\n\n\nMySQL\n\n\nSQL\n\n\n\nIn this post, we’ll tackle the first case study using MySQL from 8WEEKSSQLCHALLENGE.com. We’ll help Danny’s Diner by figuring out when people visit, what they like, and how…\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\nMelanoma Skin Cancer Detection using Transfer Learning with PyTorch\n\n\n12 min\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nPyTorch\n\n\nGradio\n\n\nHuggingFace Spaces\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\nMultilingual Toxic Comment Classification using Tensorflow and TPUs\n\n\n14 min\n\n\n\nNatural Language Processing\n\n\nDeep Learning\n\n\nTensorflow\n\n\nTPUs\n\n\nGradio\n\n\nHuggingFace Spaces\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "",
    "text": "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\nIn this project, you’ll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.\n\nUnderstanding the building blocks of working with Multimodal AI projects\nWorking with some of the fundamental concepts of LangChain\n\nHow to use the Whisper API to transcribe audio to text\nHow to combine both LangChain and Whisper API to create ask questions of any YouTube video"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#objectives",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#objectives",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "",
    "text": "Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\nIn this project, you’ll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.\n\nUnderstanding the building blocks of working with Multimodal AI projects\nWorking with some of the fundamental concepts of LangChain\n\nHow to use the Whisper API to transcribe audio to text\nHow to combine both LangChain and Whisper API to create ask questions of any YouTube video"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#before-you-begin",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#before-you-begin",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Before you begin",
    "text": "Before you begin\nYou’ll need a developer account with OpenAI and a create API Key. The API secret key will be stored in your ‘Environment Variables’ on the side menu. See the getting-started.ipynb notebook for details on setting this up."
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-0-setup",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-0-setup",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 0: Setup",
    "text": "Task 0: Setup\nThe project requires several packages that need to be installed into Workspace.\n\nlangchain is a framework for developing generative AI applications.\nyt_dlp lets you download YouTube videos.\ntiktoken converts text into tokens.\ndocarray makes it easier to work with multi-model data (in this case mixing audio and text).\n\n\nInstructions\nRun the following code to install the packages.\n\n# Install langchain\n!pip install langchain==0.0.228 yt_dlp==2023.7.6 tiktoken==0.5.1 docarray==0.38.0 chromadb==0.4.19 --quiet\n\n\n%%writefile .env\nOPENAI_API_KEY = \"PASTE_YOUR_OPENAI_API_KEY\"\n\nOverwriting .env\n\n\n\nfrom dotenv import load_dotenv, find_dotenv\n## Loading the Secrets from the .env file\nprint(load_dotenv())\n\nTrue"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-1-import-the-required-libraries",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-1-import-the-required-libraries",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 1: Import The Required Libraries",
    "text": "Task 1: Import The Required Libraries\nFor this project we need the os and the yt_dlp packages to download the YouTube video of your choosing, convert it to an .mp3 and save the file. We will also be using the openai package to make easy calls to the OpenAI models we will use.\nImport the following packages.\n\nImport os\nImport openai\nImport yt_dlp with the alias youtube_dl\nFrom the yt_dlp package, import DowloadError\nAssign openai_api_key to os.getenv(\"OPENAI_API_KEY\")\n\n\n# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\n# Import the os package \nimport os\n# Import Glob package\nimport glob\n# Import the openai package \nimport openai\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl\n# Import DownloadError from yt_dlp \nfrom yt_dlp import DownloadError\n# Import DocArray\nimport docarray\n\nWe will also assign the variable openai_api_key to the environment variable “OPEN_AI_KEY”. This will help keep our key secure and remove the need to write it in the code here.\n\n# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-2-download-the-youtube-video",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-2-download-the-youtube-video",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 2: Download the YouTube Video",
    "text": "Task 2: Download the YouTube Video\nAfter creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3).\nWe’ll download a DataCamp tutorial about machine learning in Python.\nWe will do this by setting a variable to store the youtube_url and the output_dir that we want the file to be stored.\nThe yt_dlp allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you.\nLastly, we will create a loop that looks in the output_dir to find any .mp3 files. Then we will store those in a list called audio_files that will be used later to send each file to the Whisper model for transcription.\nCreate the following: - Two variables - youtube_url to store the Video URL and output_dir that will be the directory where the audio files will be saved. - For this tutorial, we can set the youtube_url to the following \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"and the output_dirto files/audio/. In the future, you can change these values. - Use the ydl_config that is provided to you\n\n# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\n# Check if the output directory exists, if not create it\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n# Print a message indicating which video is being downloaded\nprint(f\"Downloading Video from the url : {youtube_url}\")\n# Attempt to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\nDownloading Video from the url : https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n[youtube] aqzxYofJ_ck: Downloading webpage\n[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n[youtube] aqzxYofJ_ck: Downloading android player API JSON\n[youtube] aqzxYofJ_ck: Downloading m3u8 information\n[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n[download] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\n[download] 100% of   10.43MiB in 00:00:07 at 1.45MiB/s   \n[ExtractAudio] Destination: files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\nDeleting original file files\\audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n\n\nTo find the audio files that we will use the globmodule that looks in the output_dir to find any .mp3 files. Then we will append the file to a list called audio_files. This will be used later to send each file to the Whisper model for transcription.\nCreate the following: - A variable called audio_filesthat uses the glob module to find all matching files with the .mp3 file extension - Select the first first file in the list and assign it to audio_filename - To verify the filename, print audio_filename\n\n# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio_file = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename = audio_file[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)\n\nfiles/audio\\Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-3-transcribe-the-video-using-whisper",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-3-transcribe-the-video-using-whisper",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 3: Transcribe the Video using Whisper",
    "text": "Task 3: Transcribe the Video using Whisper\nIn this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the audio_file, for the output_file and the model.\nUsing these variables we will: - create a list to store the transcripts - Read the Audio File - Send the file to the Whisper Model using the OpenAI package\nTo complete this step, create the following: - A variable named audio_filethat is assigned the audio_filename we created in the last step - A variable named output_file that is assigned the value \"files/transcripts/transcript.txt\" - A variable named model that is assigned the value \"whisper-1\" - An empty list called transcripts - A variable named audio that uses the open method and \"rb\" modifier on the audio_file - A variable to store the response from the openai.Audio.transcribe method that takes in the modeland audio variables - Append the response[\"text\"]to the transcripts list.\n\nimport openai\n\n# Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Set the API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"Converting Audio to Text.....\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\n# Extract the transcript from the response\ntranscript = response['text']\n\nConverting Audio to Text.....\n\n\nTo save the transcripts to text files we will use the below provided code:\n\n# If an output file is specified, save the transcript to a .txt file\nif output_file is not None:\n    # Create the directory for the output file if it doesn't exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    # Write the transcript to the output file\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\n# Print the transcript to the console to verify it worked \nprint(transcript)\n\nHi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating, because it's going to make your model appear to perform better than it actually does. So it's giving you a false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as PD. That's the standard alias for it. And then we actually, we purchased one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train test split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan applications. And we're going to use PD.read CSV because it is in a CSV file. And the file is called loan underscore data dot CSV. Let me just check and see if I got that correct. Loan underscore data dot CSV. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, this is a categorical column that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column. And then I'm just going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications, with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test. And then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default, the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And because we set the random state, this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice, and so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So if we're going to look at features train 2, and you see, even though it's random, we have exactly the same results. It's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful."
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-4-create-a-textloader-using-langchain",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-4-create-a-textloader-using-langchain",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 4: Create a TextLoader using LangChain",
    "text": "Task 4: Create a TextLoader using LangChain\nIn order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the TextLoader that will take the text from our transcript and load it into a document.\nTo complete this step, do the following: - Import TextLoader from langchain.document_loaders - Create a variable called loader that uses the TextLoader method which takes in the directory of the transcripts \"./files/text\" - Create a variable called docs that is assigned the result of calling the loader.load() method.\n\n# Import the TextLoader class from the langchain.document_loaders module\nfrom langchain.document_loaders import TextLoader \n\n# Create a new instance of the TextLoader class, specifying the directory containing the text files\nloader = TextLoader(\"./files/transcripts/transcript.txt\")\n\n# Load the documents from the specified directory using the TextLoader instance\ndocs = loader.load()\n\n\n# Show the first element of docs to verify it has been loaded \ndocs[0]\n\nDocument(page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating, because it's going to make your model appear to perform better than it actually does. So it's giving you a false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using DataCamp Workspace here, and this is one of the data sets that is available as standard with DataCamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as PD. That's the standard alias for it. And then we actually, we purchased one function from scikit-learn. So this is in the scikit-learn model selection sub-module. And the function for splitting into training and testing sets is called train test split. So let's run that. All right. So this data is about loan applications. So I'm just going to call it loan applications. And we're going to use PD.read CSV because it is in a CSV file. And the file is called loan underscore data dot CSV. Let me just check and see if I got that correct. Loan underscore data dot CSV. Yes, it did. Okay. So let me just copy and paste this variable name so we can print out the results. Okay. So here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right. So now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, this is a categorical column that's going to become important how we deal with that in a moment. All right. So first of all, we'll just concentrate on the response. So the response variable is called credit dot policy. And so each row is an application. So when that application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column. And then I'm just going to copy and paste this variable name again, so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right. So we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications, with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And I'm going to print this out. So, so far, this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right. So now the crux of this. So we're going to split the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test. And then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both training testing sets. What I mean by that is that by default, the training and testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And because we set the random state, this code is going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice, and so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So if we're going to look at features train 2, and you see, even though it's random, we have exactly the same results. It's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful.\", metadata={'source': './files/transcripts/transcript.txt'})"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-4-creating-an-in-memory-vector-store",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-4-creating-an-in-memory-vector-store",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 4: Creating an In-Memory Vector Store",
    "text": "Task 4: Creating an In-Memory Vector Store\nNow that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space.\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the docarray package.\nWe will also tokenize our queries using the tiktoken package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model “understand” the text and relationships with other tokens.\n\nInstructions\n\nImport the tiktoken package.\n\n\n# Import the tiktoken package\nimport tiktoken"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-5-create-the-document-search",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-5-create-the-document-search",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 5: Create the Document Search",
    "text": "Task 5: Create the Document Search\nWe will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing:\n\nImport RetrievalQA from langchain.chains - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents\nImport ChatOpenAI from langchain.chat_models - this imports the ChatOpenAI model that we will use to query the data\nImport DocArrayInMemorySearch from langchain.vectorstores - this gives the ability to search over the vector store we have created.\nImport OpenAIEmbeddings from langchain.embeddings - this will create embeddings for the data store in the vector store.\nImport display and Markdownfrom IPython.display - this will create formatted responses to the queries. (\n\n\n# Import the RetrievalQA class from the langchain.chains module\nfrom langchain.chains import RetrievalQA\n# Import the ChatOpenAI class from the langchain.chat_models module\nfrom langchain.chat_models import ChatOpenAI\n# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\nfrom langchain.vectorstores import DocArrayInMemorySearch\n# Import the OpenAIEmbeddings class from the langchain.embeddings module\nfrom langchain.embeddings import OpenAIEmbeddings \n\nNow we will create a vector store that will use the DocArrayInMemory search methods which will search through the created embeddings created by the OpenAI Embeddings function.\nTo complete this step: - Create a variable called db - Assign the db variable to store the result of the method DocArrayInMemorySearch.from_documents - In the DocArrayInMemorySearch method, pass in the docs and a function call to OpenAIEmbeddings()\n\n# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(\n    docs, OpenAIEmbeddings()\n)\n\nWe will now create a retriever from the db we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the ChatOpenAI model, will assigned that as our LLM.\nCreate the following: - A variable called retriever that is assigned db.as_retriever() - A variable called llm that creates the ChatOpenAI method with a set temperatureof 0.0. This will controle the variability in the responses we receive from the LLM.\n\n# Convert the DocArrayInMemorySearch instance to a retriever\nretriever = db.as_retriever()\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm = ChatOpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\")\n\nOur last step before starting to ask questions is to create the RetrievalQA chain. This chain takes in the:\n- The llm we want to use - The chain_type which is how the model retrieves the data - The retriever that we have created - An option called verbose that allows use to see the seperate steps of the chain\nCreate a variable called qa_stuff. This variable will be assigned the method RetrievalQA.from_chain_type.\nUse the following settings inside this method: - llm=llm - chain_type=\"stuff\" - retriever=retriever - verbose=True\n\n# Create a new RetrievalQA instance with the specified parameters\nqa_stuff = RetrievalQA.from_chain_type(\nllm=llm,\nchain_type=\"stuff\",\nretriever=retriever,\nverbose=True,\n)\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation"
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-5-create-the-queries",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#task-5-create-the-queries",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "Task 5: Create the Queries",
    "text": "Task 5: Create the Queries\nNow we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query.\nTo create the questions to ask the model complete the following steps: - Create a variable call query and assigned it a string value of \"What is this tutorial about?\" - Create a response variable that will store the result of qa_stuff.run(query) - Show the resposnse\n\n# Set the query to be used for the QA system\nquery = \"What is this tutorial about?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThis tutorial is about a data pre-processing technique for machine learning called splitting your data. It explains why it is important to split your data set into a training set and a testing set, and when in the machine learning workflow this should be done. The tutorial also provides code examples using pandas and scikit-learn to demonstrate how to split the data set.\n\n\nWe can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds.\n\n# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThe training set is a subset of the data that is used to train a machine learning model. It is used to teach the model the patterns and relationships in the data so that it can make accurate predictions or classifications. The test set, on the other hand, is a subset of the data that is used to evaluate the performance of the trained model. It is used to assess how well the model generalizes to new, unseen data. The test set is independent of the training set and should not be used during the training process to avoid biasing the model's performance.\n\n\n\n# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nThis lesson is suitable for anyone who is interested in data pre-processing techniques for machine learning, specifically the technique of splitting data into training and testing sets. It is especially relevant for individuals who are new to machine learning and want to understand the importance of splitting data and how to implement it in their workflow.\n\n\n\n# Set the query to be used for the QA system\nquery =\"Who is the greatest football team on earth?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nI don't know the answer to that question as it is subjective and can vary depending on personal opinions and criteria used to determine greatness.\n\n\n\n# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n# Print the response to the console\nprint(response)\n\n\n\n&gt; Entering new  chain...\n\n&gt; Finished chain.\nI don't know the answer to that question."
  },
  {
    "objectID": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#thats-all.",
    "href": "posts/Chat_With_YouTube_Videos/Building a Simple Chatbot that talks with YouTube Videos using Langchain & OpenAI API.html#thats-all.",
    "title": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.",
    "section": "That’s all.",
    "text": "That’s all."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html",
    "href": "posts/Dropout_Explained/Dropout_Explained.html",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Why should we use Dropout?\nWhat Is a Dropout?\nWhy does Dropout works?\nTips for Using Dropout.\nReferences\n\n\n\n\nBefore jumping into what is dropout, let’s try to answer the question of why should we use Dropout. Deep Neural Networks which are trained on smaller datasets can overfit the training data. Now, what is Overfitting? Overfitting happens when your model is too powerful to memorize the training data. But who cares about your training data, we want to take the model and put it into production for the use case. We want the model to perform well on the test/unseen dataset. Usually, Convolutional Neural Networks are less prone to Overfitting because of the idea of weight sharing i.e., you use the same filter/kernel and slide over the image, and therefore, you end up having fewer parameters in CNNs. The problem primarily occurs when you have a Deep Fully Connected Neural Network that tends to have a lot of parameters & they are likely to overfit. So, in short, Dropout is a regularization technique that prevents your Deep Neural Networks to overfit.\nDoes that mean you cannot apply to Convolutional Neural Networks? The answer is No, You can apply to CNNs and even to LSTMs and GRUs.\n\n\n\n\n\nLet’s try to Understand the dropout in the context of Fully Connected Neural Networks since they are more prone to Overfit. Look at the below image: \nIn a Nutshell, Dropout is about killing/turning off the nodes in a network at a layer. Suppose you have an MLP like in figure(a), then during the training stage at layer l, you are essentially ignoring the neurons and that is done randomly with a certain probability, p. This probability, p, is the dropout rate that is a HyperParameter that you set before training the Network. Say you choose the dropout rate p=0.5 then it means you will delete each node with a 50% probability during each forward pass of your minibatch. In each forward pass of your minibatch, you will randomly drop the nodes and these nodes will be not going to update during backpropagation. \nIn the above Image, Consider in the first forward pass in the Middle layer dropout may disable the 2nd & 4th nodes but in the second forward pass maybe 1st & 4th nodes will get disabled. In each iteration, during backpropagation, the weights corresponding to these nodes will not be updated.\nHow do we drop nodes practically? Let’s compare the standard network and a dropout network. \nWe can observe the inputs of layer l, y^l, are multiplied with the weights, wi^(l+1) and bias bi^(l+1) is added to get net inputs for one neuron, and later a non-linearity (activation function) is applied on it to generate the value of the neuron in layer (l+1).\n\n - But in Dropout Network, first, a boolean vector r^(l) is generated which will contain the same number of elements as the number of neurons in layer l, then this vector is multiplied by the input vectors of layer l to get the net input. The net input is finally passed into an activation function. But the question is how do you arrive at this boolean vector? Let’s take an example to illustrate this point, in the above image we have 3 neurons in layer l, suppose you set the dropout rate to p=0.5 then one way to create the boolean vector is first to generate the random number vector containing numbers between 0 & 1. How can you do that? You can sample 3 numbers from a Uniform distribution with an interval [0,1]. Assume that you get a vector of random numbers as [0.1, 0.6, 0.8], then you will set the value of the vector to be zero when the random value &lt; dropout rate(0.5 in our case) else you set 1. So, our random vector becomes [0,1,1] which represents that neuron1 is dropped and the rest of the neurons and not. Now, This vector we are calling r^(l) & the rest of the things are the same as usual.\n\nSo far we talked about the Training stage, But how does dropout work in the Inference stage? Since dropout is dropping neurons randomly in every forward pass of the minibatch, the authors of the paper: A Simple Way to Prevent Neural Networks from Overfitting uses a trick to avoid this randomization, and your model should be deterministic during Inference otherwise your predictions and accuracy computed on the test set will not be deterministic. For instance, we are creating a credit scoring model then due to dropping neurons randomly a customer might get different scores on different dates because the neurons will be dropped randomly. That’s why Dropout is applied to training the model only. During Inference, before finalizing the network the weights are first scaled by the chosen dropout rate (p) i.e., Wtest^(l) = pW^(l). Now the network can be used as normal to make predictions on unseen data.\nNote: The rescaling of the weights can be performed at the training time instead, after each weight update at the end of the min-batch. This is sometimes called an “Inverse Dropout” and it does not require any modification of weights during training. Both Keras & PyTorch deep learning Libraries Dropout in this way.\n\n\n\n\n\nOne interpretation is the Co-adaptation theory: It refers to the fact that now the network doesn’t learn to rely on particular connections too heavily. So, if we have a Fully Connected Neural Network with Dropout applied to a particular layer then the network will rely on neuron1 & neuron4 only if that layer has 4 neurons in total while training. Consider a real-life example where your teacher forms groups for projects and you are randomly assigned to a group. So that you don’t rely on your best friend (let’s say). This way your teacher is making you independent and this is called Co-adaptation & you want to prevent your network from it.\n\n\n\n\n\n\nDropout is a general idea and it can be used with any Network either LSTMs, GRUs, MLPs, CNNs.\n\n\nA good dropout rate is between 0.5 to 0.8 in the hidden layers. But we can use different dropout rates in different hidden layers.\n\n\nMake your network complex by increasing its capacity until it overfits and then add dropout to the layers to prevent overfitting.\n\n\nTo find the optimal dropout rate we can use cross-validation to find it. Since, it is not always possible to do cross-validation because your dataset might be very big. Therefore, the dropout rate = 0.5 works very well for most problems.\n\n\n\n\n\n\nhttps://www.youtube.com/watch?v=FKTWKZU3l6k&list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4&index=5\nPart-1, Part-2, Part-3 : By Professor Sebastian Raschka"
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#why-should-we-use-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#why-should-we-use-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Before jumping into what is dropout, let’s try to answer the question of why should we use Dropout. Deep Neural Networks which are trained on smaller datasets can overfit the training data. Now, what is Overfitting? Overfitting happens when your model is too powerful to memorize the training data. But who cares about your training data, we want to take the model and put it into production for the use case. We want the model to perform well on the test/unseen dataset. Usually, Convolutional Neural Networks are less prone to Overfitting because of the idea of weight sharing i.e., you use the same filter/kernel and slide over the image, and therefore, you end up having fewer parameters in CNNs. The problem primarily occurs when you have a Deep Fully Connected Neural Network that tends to have a lot of parameters & they are likely to overfit. So, in short, Dropout is a regularization technique that prevents your Deep Neural Networks to overfit.\nDoes that mean you cannot apply to Convolutional Neural Networks? The answer is No, You can apply to CNNs and even to LSTMs and GRUs."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#what-is-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#what-is-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Let’s try to Understand the dropout in the context of Fully Connected Neural Networks since they are more prone to Overfit. Look at the below image: \nIn a Nutshell, Dropout is about killing/turning off the nodes in a network at a layer. Suppose you have an MLP like in figure(a), then during the training stage at layer l, you are essentially ignoring the neurons and that is done randomly with a certain probability, p. This probability, p, is the dropout rate that is a HyperParameter that you set before training the Network. Say you choose the dropout rate p=0.5 then it means you will delete each node with a 50% probability during each forward pass of your minibatch. In each forward pass of your minibatch, you will randomly drop the nodes and these nodes will be not going to update during backpropagation. \nIn the above Image, Consider in the first forward pass in the Middle layer dropout may disable the 2nd & 4th nodes but in the second forward pass maybe 1st & 4th nodes will get disabled. In each iteration, during backpropagation, the weights corresponding to these nodes will not be updated.\nHow do we drop nodes practically? Let’s compare the standard network and a dropout network. \nWe can observe the inputs of layer l, y^l, are multiplied with the weights, wi^(l+1) and bias bi^(l+1) is added to get net inputs for one neuron, and later a non-linearity (activation function) is applied on it to generate the value of the neuron in layer (l+1).\n\n - But in Dropout Network, first, a boolean vector r^(l) is generated which will contain the same number of elements as the number of neurons in layer l, then this vector is multiplied by the input vectors of layer l to get the net input. The net input is finally passed into an activation function. But the question is how do you arrive at this boolean vector? Let’s take an example to illustrate this point, in the above image we have 3 neurons in layer l, suppose you set the dropout rate to p=0.5 then one way to create the boolean vector is first to generate the random number vector containing numbers between 0 & 1. How can you do that? You can sample 3 numbers from a Uniform distribution with an interval [0,1]. Assume that you get a vector of random numbers as [0.1, 0.6, 0.8], then you will set the value of the vector to be zero when the random value &lt; dropout rate(0.5 in our case) else you set 1. So, our random vector becomes [0,1,1] which represents that neuron1 is dropped and the rest of the neurons and not. Now, This vector we are calling r^(l) & the rest of the things are the same as usual.\n\nSo far we talked about the Training stage, But how does dropout work in the Inference stage? Since dropout is dropping neurons randomly in every forward pass of the minibatch, the authors of the paper: A Simple Way to Prevent Neural Networks from Overfitting uses a trick to avoid this randomization, and your model should be deterministic during Inference otherwise your predictions and accuracy computed on the test set will not be deterministic. For instance, we are creating a credit scoring model then due to dropping neurons randomly a customer might get different scores on different dates because the neurons will be dropped randomly. That’s why Dropout is applied to training the model only. During Inference, before finalizing the network the weights are first scaled by the chosen dropout rate (p) i.e., Wtest^(l) = pW^(l). Now the network can be used as normal to make predictions on unseen data.\nNote: The rescaling of the weights can be performed at the training time instead, after each weight update at the end of the min-batch. This is sometimes called an “Inverse Dropout” and it does not require any modification of weights during training. Both Keras & PyTorch deep learning Libraries Dropout in this way."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#why-does-dropout-works",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#why-does-dropout-works",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "One interpretation is the Co-adaptation theory: It refers to the fact that now the network doesn’t learn to rely on particular connections too heavily. So, if we have a Fully Connected Neural Network with Dropout applied to a particular layer then the network will rely on neuron1 & neuron4 only if that layer has 4 neurons in total while training. Consider a real-life example where your teacher forms groups for projects and you are randomly assigned to a group. So that you don’t rely on your best friend (let’s say). This way your teacher is making you independent and this is called Co-adaptation & you want to prevent your network from it."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#tips-for-using-dropout",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#tips-for-using-dropout",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "Dropout is a general idea and it can be used with any Network either LSTMs, GRUs, MLPs, CNNs.\n\n\nA good dropout rate is between 0.5 to 0.8 in the hidden layers. But we can use different dropout rates in different hidden layers.\n\n\nMake your network complex by increasing its capacity until it overfits and then add dropout to the layers to prevent overfitting.\n\n\nTo find the optimal dropout rate we can use cross-validation to find it. Since, it is not always possible to do cross-validation because your dataset might be very big. Therefore, the dropout rate = 0.5 works very well for most problems."
  },
  {
    "objectID": "posts/Dropout_Explained/Dropout_Explained.html#references",
    "href": "posts/Dropout_Explained/Dropout_Explained.html#references",
    "title": "Dropout in Neural Networks Explained",
    "section": "",
    "text": "https://www.youtube.com/watch?v=FKTWKZU3l6k&list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4&index=5\nPart-1, Part-2, Part-3 : By Professor Sebastian Raschka"
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html",
    "href": "posts/Web_Scraping/Flipkart_.html",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#introduction",
    "href": "posts/Web_Scraping/Flipkart_.html#introduction",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "",
    "text": "Today, the Internet is flooded with an enormous amount of data relative to what we had a decade ago. According to Forbes, the amount of data we produce every day is truly mind-boggling. There are 2.5 quintillion bytes of data generated every day at our current pace, and the credit goes to the Internet of Things (IoT) devices. With access to this data, either in the form of audio, video, text, images, or any format, most businesses are relying heavily on data to beat their competitors & succeed in their business. Unfortunately, most of this data is not open. Most websites do not provide the option to save the data which they display on their websites. This is where Web Scraping tools/ Software comes to extract the data from the websites."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#what-is-web-scraping",
    "href": "posts/Web_Scraping/Flipkart_.html#what-is-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "What is Web Scraping? ",
    "text": "What is Web Scraping? \n\nWeb Scraping is the process of automatically downloading the data displayed on the website using some computer program. A web scraping tool can scrape multiple pages from a website & automate the tedious task of manually copying and pasting the data displayed. Web Scraping is important because, irrespective of the industry, the web contains information that can provide actionable insights for businesses to gain an advantage over competitors."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "href": "posts/Web_Scraping/Flipkart_.html#to-fetch-the-data-using-web-scraping-using-python-we-need-to-go-through-the-following-steps",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:",
    "text": "To Fetch the data using Web Scraping using Python, we need to go through the following steps:\n\nFind the URL that you want to scrape\nInspecting the Page\nFind the data you want to extract\nWrite the code\nRun the code & extract the data\nFinally, Store the data in the required format"
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#packages-used-for-web-scraping",
    "href": "posts/Web_Scraping/Flipkart_.html#packages-used-for-web-scraping",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Packages used for Web Scraping",
    "text": "Packages used for Web Scraping\n\nWe’ll use the following python packages:\n\nPandas: Pandas is a library used for data manipulation and analysis. It is used to store the data in the desired format.\nBeautifulSoup4: BeautifulSoup is the python web scraping library used for parsing HTML documents. It creates parse trees that are helpful in extracting tags from the HTML string.\nSelenium: Selenium is a tool designed to help you run automated tests in web applications. Although it’s not its main purpose, Selenium is also used in Python for web scraping, because it can access JavaScript-rendered content (which regular scraping tools like BeautifulSoup can’t do). We’ll use Selenium to download the HTML content from Flipkart and see in an interactive way what’s happening."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#project-demonstration",
    "href": "posts/Web_Scraping/Flipkart_.html#project-demonstration",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Project Demonstration",
    "text": "Project Demonstration\n\nImporting necessary Libraries\n\n\nCode\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\n\n\n\nStarting up the WebDriver\n\n\nCode\n# Creating an instance of webdriver for google chrome\ndriver = webdriver.Chrome()\n\n\n\n\nCode\n# Using webdriver we'll now open the flipkart website in chrome\nurl = 'https://flipkart.com'\n# We;ll use the get method of driver and pass in the URL\ndriver.get(url)\n\n\n\nNow there a few ways we can conduct a product search :\n\n\nFirst is to automate the browser by finding the input element and then insert a text and hit enter key on the keyboard. The image like below.\n\n\n\n\n\nHowever, this kind of automation is unnecessary and it creates a potential for program failure. The Rule of thumb for automation is to only automate what you absolutely need to when Web Scraping.\n\nLet’s search the input inside the search area and hit enter. You’ll notice that the search term has now embeded into the URL site. Now we can use this pattern to create a function that will build the necessary URL for our driver to retrieve. This will be much more efficient in the long term and less prone to proram failure. The image like below.\n\n\n\n\nLet’s copy this Pattern and create a function that will insert the search term using string formatting.\n\n\n\nCode\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    # We'are replacing every space with '+' to adhere with the pattern \n    search_item = search_item.replace(\" \",\"+\")\n    return template.format(search_item)\n\n\n\nNow we have a function that will generate a URL based on the search term we provide.\n\n\n\nCode\n# Checking whether the function is working properly or not\nurl = get_url('mobile phones')\nprint(url)\n\n\nhttps://www.flipkart.com/search?q=mobile+phones&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on\n\n\n\nThe fuction produces the same result as before.\n\n\n\nExtracting the collection\n\nNow we are going to extract the contents of the webpage from which we want to extract the information from.\nTo do that we need to create a BeautifulSoup object which will parse the HTML content from the page source.\n\n\n\nCode\n# Creating a soup object using driver.page_source to retreive the HTML text and then we'll use the default html parser to parse\n# the HTML.\nsoup = BeautifulSoup(driver.page_source, 'html.parser')\n\n\n\n\nNow that we have identified that the above card indicated by the box contains all the information what we need for a mobile phone. So let’s find out all the tags for these boxes/cards which contains information we want to extract.\nWe’ll be extracting Model , stars, number of ratings, number of reviews, RAM, Storage capacity, Exapandable option, display, camera information, battery, processor , warranty and Price information."
  },
  {
    "objectID": "posts/Web_Scraping/Flipkart_.html#inspecting-the-tags",
    "href": "posts/Web_Scraping/Flipkart_.html#inspecting-the-tags",
    "title": "Web Scraping using BeautifulSoup & Selenium in Python.",
    "section": "Inspecting the tags",
    "text": "Inspecting the tags\n\n\nWe can fetch the a tag & specificallyclass = _1fQZEK to get all the cards/boxes and then we can easily take out information of out these boxes for any mobile phone.\n\n\n\nCode\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nlen(results)\n\n\n24\n\n\n\nPrototyping for a single record\n\n\nCode\n# picking the 1st card from the complete list of cards\nitem = results[0]\n\n\n\n\nCode\n# Extracting the model of the phone from the 1st card\nmodel = item.find('div',{'class':\"_4rR01T\"}).text\nmodel\n\n\n'REDMI 9i (Nature Green, 64 GB)'\n\n\n\n\nCode\n# Extracting Stars from 1st card\nstar = item.find('div',{'class':\"_3LWZlK\"}).text\nstar\n\n\n'4.3'\n\n\n\n\nCode\n# Extracting Number of Ratings from 1st card\nnum_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\nnum_ratings\n\n\n'4,06,452 Ratings'\n\n\n\n\nCode\n# Extracting Number of Reviews from 1st card\nreviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\nreviews\n\n\n'23,336 Reviews'\n\n\n\n\nCode\n# Extracting RAM from the 1st card\nram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\nram\n\n\n'4 GB RAM '\n\n\n\n\nCode\n# Extracting Storage/ROM from 1st card\nstorage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\nstorage\n\n\n'64 GB ROM'\n\n\n\n\nCode\n# Extracting whether there is an option of expanding the storage or not\nexpandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\nexpandable\n\n\n'Expandable Upto 512 GB'\n\n\n\n\nCode\n# Extracting the display option from the 1st card\ndisplay = item.find_all('li')[1].text.strip()\ndisplay\n\n\n'16.59 cm (6.53 inch) HD+ Display'\n\n\n\n\nCode\n# Extracting camera options from the 1st card\ncamera = item.find_all('li')[2].text.strip()\ncamera\n\n\n'13MP Rear Camera | 5MP Front Camera'\n\n\n\n\nCode\n# Extracting the battery option from the 1st card\nbattery = item.find_all('li')[3].text\nbattery\n\n\n'5000 mAh Lithium Polymer Battery'\n\n\n\n\nCode\n# Extracting the processir option from the 1st card\nprocessor = item.find_all('li')[4].text.strip()\nprocessor\n\n\n'MediaTek Helio G25 Processor'\n\n\n\n\nCode\n# Extracting Warranty from the 1st card\nwarranty = item.find_all('li')[-1].text.strip()\nwarranty\n\n\n'Brand Warranty of 1 Year Available for Mobile and 6 Months for Accessories'\n\n\n\n\nCode\n# Extracting price of the model from the 1st card\nprice = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n\n\n\n\nGeneralizing the Pattern\n\nNow let create a function that will extract all the information at once from a single page.\n\n\n\nCode\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\n\n\n\nCode\n# Now putting all the information from all the cards/phone models and putting them into a list\nrecords_list = []\nresults = soup.find_all('a',{'class':\"_1fQZEK\"})\nfor item in results:\n    records_list.append(extract_phone_model_info(item))\n\n\n\nViewing how does our dataframe look like for the 1st page.\n\n\n\nCode\npd.DataFrame(records_list,columns=['model',\"star\",\"num_ratings\"\n   ,\"reviews\",'ram',\"storage\",\"expandable\",\"display\",\"camera\",\"battery\",\"processor\",\"warranty\",\"price\"])\n\n\n\n\n\n\n\n\n\nmodel\nstar\nnum_ratings\nreviews\nram\nstorage\nexpandable\ndisplay\ncamera\nbattery\nprocessor\nwarranty\nprice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n4\nrealme C21 (Cross Blue, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499\n\n\n5\nREDMI 9 Power (Mighty Black, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n6\nPOCO M3 (Cool Blue, 64 GB)\n4.3\n14,630 Ratings\n930 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹10,499\n\n\n7\nPOCO M2 Reloaded (Mostly Blue, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n8\nrealme C11 2021 (Cool Grey, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n9\nrealme C11 2021 (Cool Blue, 32 GB)\n4.3\n3,584 Ratings\n197 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nOcta-core Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹6,999\n\n\n10\nREDMI 9 Power (Fiery Red, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n11\nREDMI 9i (Midnight Black, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n12\nInfinix Smart 5A (Quetzal Cyan, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n13\nInfinix Smart 5A (Midnight Black, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n14\nInfinix Smart 5A (Ocean Wave, 32 GB)\n4.5\n3,546 Ratings\n244 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.56 cm (6.52 inch) HD+ Display\n8MP + Depth Sensor | 8MP Front Camera\n5000 mAh Li-ion Polymer Battery\nMediaTek Helio A20 Processor\n1 Year on Handset and 6 Months on Accessories\n₹6,699\n\n\n15\nPOCO M3 (Power Black, 64 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹11,499\n\n\n16\nPOCO M3 (Power Black, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n17\nPOCO M2 Reloaded (Greyish Black, 64 GB)\n4.3\n20,010 Ratings\n1,315 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n13MP + 8MP + 5MP + 2MP | 8MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G80 Processor\n1 Year for Handset, 6 Months for Accessories\n₹9,999\n\n\n18\nREDMI Note 9 (Aqua Green, 64 GB)\n4.3\n92,325 Ratings\n6,906 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 13MP Front Camera\n5020 mAh Battery\nMediaTek Helio G85 Processor\n1 Year Manufacturer Warranty for Device and 6 ...\n₹11,999\n\n\n19\nrealme Narzo 30 5G (Racing Silver, 128 GB)\n4.4\n27,618 Ratings\n2,243 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 1 TB\n16.51 cm (6.5 inch) Full HD+ Display\n48MP + 2MP + 2MP | 16MP Front Camera\n5000 mAh Battery\nMediaTek Dimensity 700 (MT6833) Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹15,999\n\n\n20\nREDMI 9 Power (Blazing Blue, 64 GB)\n4.3\n1,30,038 Ratings\n9,051 Reviews\n4 GB RAM\n64 GB ROM\n\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 8MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Battery\nQualcomm Snapdragon 662 Processor\n1 year manufacturer warranty for device and 6 ...\n₹10,999\n\n\n21\nPOCO M3 (Yellow, 128 GB)\n4.3\n3,10,045 Ratings\n22,471 Reviews\n6 GB RAM\n128 GB RO\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) Full HD+ Display\n48MP + 2MP + 2MP | 8MP Front Camera\n6000 mAh Lithium-ion Polymer Battery\nQualcomm Snapdragon 662 Processor\nOne Year Warranty for Handset, 6 Months for Ac...\n₹12,999\n\n\n22\nrealme C20 (Cool Grey, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n23\nrealme C20 (Cool Blue, 32 GB)\n4.4\n1,35,513 Ratings\n6,302 Reviews\n2 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n8MP Rear Camera | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹7,499\n\n\n\n\n\n\n\n\n\nNavigating to next page\n\nWriting a custom function that will help us getting information from multiple pages\n\n\n\nCode\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\n\n\n\nPutting it all together\n\nNow combining all thhe things that we have done so far.\n\n\n\nCode\n# Importing necessary Libraries\nimport csv\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nimport pandas as pd\n\ndef get_url(search_item):\n    '''\n    This function fetches the URL of the item that you want to search\n    '''\n    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n    search_item = search_item.replace(\" \",\"+\")\n    # Add term query to URL\n    url = template.format(search_item)\n    # Add term query placeholder\n    url += '&page{}'\n    return url\n\ndef extract_phone_model_info(item):\n    \"\"\"\n    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n    \"\"\"\n    # Extracting the model of the phone from the 1st card\n    model = item.find('div',{'class':\"_4rR01T\"}).text\n    # Extracting Stars from 1st card\n    star = item.find('div',{'class':\"_3LWZlK\"}).text\n    # Extracting Number of Ratings from 1st card\n    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n    # Extracting Number of Reviews from 1st card\n    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n    # Extracting RAM from the 1st card\n    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n    # Extracting Storage/ROM from 1st card\n    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n    # Extracting whether there is an option of expanding the storage or not\n    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n    # Extracting the display option from the 1st card\n    display = item.find_all('li')[1].text.strip()\n    # Extracting camera options from the 1st card\n    camera = item.find_all('li')[2].text.strip()\n    # Extracting the battery option from the 1st card\n    battery = item.find_all('li')[3].text\n    # Extracting the processir option from the 1st card\n    processor = item.find_all('li')[4].text.strip()\n    # Extracting Warranty from the 1st card\n    warranty = item.find_all('li')[-1].text.strip()\n    # Extracting price of the model from the 1st card\n    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n    return result\n\ndef main(search_item):\n    '''\n    This function will create a dataframe for all the details that we are fetching from all the multiple pages\n    '''\n    driver = webdriver.Chrome()\n    records = []\n    url = get_url(search_item)\n    for page in range(1,464):\n        driver.get(url.format(page))\n        soup = BeautifulSoup(driver.page_source,'html.parser')\n        results = soup.find_all('a',{'class':\"_1fQZEK\"})\n        for item in results:\n            records.append(extract_phone_model_info(item))\n    driver.close()\n    # Saving the data into a csv file\n    with open('Flipkart_results.csv','w',newline='',encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Model','Stars','Num_of_Ratings','Reviews','Ram','Storage','Expandable',\n                        'Display','Camera','Battery','Processor','Warranty','Price'])\n        writer.writerows(records)\n\n\n\n\nExtracting Informtion of all the Mobile phones present on multiple pages\n\n\nCode\n%%time\nmain('mobile phones')\n\n\nWall time: 40min 54s\n\n\n\n\nViewing the data\n\n\nCode\nscraped_df = pd.read_csv('C:\\\\Users\\\\DELL\\\\Desktop\\\\Jupyter Notebook\\\\Jovian Web Scraping\\\\Amazon Products Web Scrapper\\\\Flipkart_results.csv')\nscraped_df.head()\n\n\n\n\n\n\n\n\n\nModel\nStars\nNum_of_Ratings\nReviews\nRam\nStorage\nExpandable\nDisplay\nCamera\nBattery\nProcessor\nWarranty\nPrice\n\n\n\n\n0\nREDMI 9i (Nature Green, 64 GB)\n4.3\n4,06,452 Ratings\n23,336 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 512 GB\n16.59 cm (6.53 inch) HD+ Display\n13MP Rear Camera | 5MP Front Camera\n5000 mAh Lithium Polymer Battery\nMediaTek Helio G25 Processor\nBrand Warranty of 1 Year Available for Mobile ...\n₹8,499\n\n\n1\nrealme C21 (Cross Black, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n2\nrealme C21 (Cross Blue, 64 GB)\n4.4\n63,273 Ratings\n2,912 Reviews\n4 GB RAM\n64 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹9,499\n\n\n3\nSAMSUNG Galaxy S21 Ultra (Phantom Silver, 256 GB)\n4.5\n537 Ratings\n101 Reviews\n12 GB RAM\n256 GB RO\nNaN\n17.27 cm (6.8 inch) Quad HD+ Display\n108MP + 12MP + 10MP + 10MP | 40MP Front Camera\n5000 mAh Lithium-ion Battery\nExynos 2100 Processor\n1 Year Manufacturer Warranty for Handset and 6...\n₹1,05,999\n\n\n4\nrealme C21 (Cross Black, 32 GB)\n4.4\n51,035 Ratings\n2,564 Reviews\n3 GB RAM\n32 GB ROM\nExpandable Upto 256 GB\n16.51 cm (6.5 inch) HD+ Display\n13MP + 2MP + 2MP | 5MP Front Camera\n5000 mAh Battery\nMediaTek Helio G35 Processor\n1 Year Warranty for Mobile and 6 Months for Ac...\n₹8,499"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Building a Simple Chatbot to talk with YouTube Videos using Langchain & OpenAI API.\n\n\n\n\n\n\n\nGenerativeAI\n\n\nLangchain\n\n\nOpenAI\n\n\nLLMs\n\n\n\n\nIn this post, we’ll create a simple chatbot that can talk with YouTube videos using Langchain & OpenAI API.\n\n\n\n\n\n\nJan 2, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\nDropout in Neural Networks Explained\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nIn this post, We’ll understand What Dropout is? Why should we use it? Why does it work? Some tips for using it in practice.\n\n\n\n\n\n\nJan 9, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nWeb Scraping using BeautifulSoup & Selenium in Python.\n\n\n\n\n\n\n\nWeb Scrapping\n\n\nPython\n\n\nBeautifulSoup\n\n\n\n\nIn this post, we’ll demonstrate how to scape data from flipkart using BeautifulSoup & Selenium using Python.\n\n\n\n\n\n\nJan 9, 2023\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sql_portfolio.html",
    "href": "sql_portfolio.html",
    "title": "SQL Portfolio",
    "section": "",
    "text": "Case Study #7 - Balanced Tree Clothing Co.\n\n\n\n\n\n\n\n\n\n\n\n\n36 min\n\n\n\n\n\n\n\n\nCase Study #4 - Data Bank\n\n\n\n\n\n\n\n\n\n\n\n\n10 min\n\n\n\n\n\n\n\n\nCase Study #5 - Data Mart\n\n\n\n\n\n\n\n\n\n\n\n\n11 min\n\n\n\n\n\n\n\n\nCase Study #3 - Foodie-Fi\n\n\n\n\n\n\n\n\n\n\n\n\n22 min\n\n\n\n\n\n\n\n\nSolving Case Study #6 - Clique Bait using SQL\n\n\n\n\n\nIn this post, we will solve an end-to-end an analytical case study using SQL.\n\n\n\n\n\n\nJan 6, 2024\n\n\n36 min\n\n\n\n\n\n\n\n\nSolving Case Study #2 - Pizza Runner using SQL\n\n\n\n\n\nIn this post, I will solve an end-to-end an analytical case study using SQL.\n\n\n\n\n\n\nJan 5, 2024\n\n\n36 min\n\n\n\n\n\n\n\n\nCase Study #1 - Danny’s Diner\n\n\n\n\n\nIn this post, we’ll tackle the first case study using MySQL from 8WEEKSSQLCHALLENGE.com. We’ll help Danny’s Diner by figuring out when people visit, what they like, and how much they spend. We’ll use simple SQL queries to assist Danny in making decisions about loyalty programs and make data easy for the team to understand. Discover the secrets behind why this cozy Japanese restaurant is doing so well!\n\n\n\n\n\n\nJan 1, 2024\n\n\n23 min\n\n\n\n\n\n\nNo matching items"
  }
]